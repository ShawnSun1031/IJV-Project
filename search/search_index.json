{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome-to-ijv-project","title":"Welcome to IJV-Project","text":"<p>The primary objective of this study is to quantitatively measure changes in internal jugular vein oxygen saturation non-invasively using near-infrared spectroscopy. Initially, a surrogate model based on neural networks is employed to accelerate the Monte Carlo method which is traditionally used to simulate photon transport in tissue. Subsequently, another neural network is applied to establish a predictive model for oxygen saturation changes. The input to this model consists of spectral features extracted using formulas same as modified Beer-Lambert law, while the output represents oxygen saturation changes.  </p> <p>As for the measurement system, the study utilizes 20 wavelength points based on the absorption spectra of blood, within the wavelength range of 700 nm to 850 nm. A dual-channel system is set up, with the short channel having a distance of 10 mm between the light source and detector, and the long channel having a distance of 20 mm. This design effectively minimizes the impact of superficial tissues and enhances the signal from deeper tissues including the internal jugular vein area. During simulation, a three-dimensional numerical model is constructed based on ultrasound images of each subject\u2019s neck, ensuring that simulation results closely resemble reality, thus providing more accurate simulated data.  </p> <p>To evaluate the prediction model\u2019s performance, the study investigates the impacts of factors such as human respiration, changes in oxygen levels in surrounding tissues, and measurement noise on the predictive model. The results indicate that the effects of respiration may lead to a maximum increase of 3% to 4% in root-mean-square error (RMSE). Changes in oxygen levels in surrounding tissues have a less significant impact, with a maximum RMSE increase of only 1%. Measurement signal errors can cause an RMSE increase of 1% to 2%.  </p> <p>For model generalization, the study conducts simulated experiments using transfer learning. Through experimentation, it is observed that by using a thousandth of the original dataset and employing transfer learning, an RMSE of 3.5% can be achieved, while without transfer learning and using only a thousandth of the dataset, an RMSE of 7% is obtained.  </p> <p>Based on the simulation results, the prediction model established in this study predicts changes in internal jugular vein oxygen saturation with an RMSE of less than 1.5%. In vivo experiments involve measuring diffuse reflectance spectra from living subjects, extracting spectral features using the formulas designed in this study, and inputting them into the prediction model after appropriate normalization. The prediction results are consistent with expected physiological response and spectral features in the measured data.  </p> <p>\u2014 [Chin-Hsuan Sun][1] </p>"},{"location":"#publication","title":"Publication","text":"<p> Quantifying changes in oxygen saturation of the internal jugular vein in vivo using deep neural networks and subject-specific three-dimensional Monte Carlo models Published in Optics Letters Vol. 49, Issue 10, 2024</p> <p>Central venous oxygen saturation (ScvO2) is an important parameter for assessing global oxygen usage and guiding clinical interventions. However, measuring ScvO2 requires invasive catheterization. As an alternative, we aim to noninvasively and continuously measure changes in oxygen saturation of the internal jugular vein (SijvO2) by a multi-channel near-infrared spectroscopy system. The relation between the measured reflectance and changes in SijvO2 is modeled by Monte Carlo simulations and used to build a prediction model using deep neural networks (DNNs). The prediction model is tested with simulated data to show robustness to individual variations in tissue optical properties. The proposed technique is promising to provide a noninvasive tool for monitoring the stability of brain oxygenation in broad patient populations.</p> <p>Recommended citation: Chin-Hsuan Sun, Hao-Wei Lee, Ya-Hua Tsai, Jia-Rong Luo &amp; Kung-Bin Sung (2024). Quantifying changes in oxygen saturation of the internal jugular vein in vivo using deep neural networks and subject-specific three-dimensional Monte Carlo models. Optics Letters. 49(10). https://doi.org/10.1364/OL.517960</p>"},{"location":"MCX_simulation/","title":"How to execute MCX_sim","text":""},{"location":"MCX_simulation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Executing Experiments</li> <li>Beginners Guide</li> <li>Details for setting</li> <li>MCX forum</li> <li>MCX cloud</li> <li>Analyze the output file from MCX</li> <li>TODO</li> </ul>"},{"location":"MCX_simulation/#executing-experiments","title":"Executing Experiments","text":"<p>Make sure you already have /bin/MCX in this directory. If not, compiled from MD703_edit_MCX_src_v2023/src in advance.*</p> <p>You should use linux environment to do the following commands. If you want to execute in Windows, you need to fix two problems carefully: 1. Compile the MD703_edit_MCX_src_v2023/src in Windows, we use a user-define LED source pattern to make the simulation in consistent with the experiments. 2. Write your own shell script in Windows version. (bash can't work, use batch)</p> <pre><code>bash Run_MCX_Sim.sh\n</code></pre> <p>please see Run_MCX_Sim.sh for details and understand the work flows.</p>"},{"location":"MCX_simulation/#beginners-guide","title":"Beginners Guide","text":"<p>If you are a total beginner to this, start here!</p> <ol> <li>Learn some syntax about shell script https://blog.techbridge.cc/2019/11/15/linux-shell-script-tutorial/</li> <li>Read the document about MCX source code https://github.com/fangq/mcx</li> </ol>"},{"location":"MCX_simulation/#details-for-setting","title":"Details for setting","text":"<p>[LED source]: 1. in the folder \"input_template/share_files/model_input_related\" contains the CDF of LED source (multiplied by sin due to 3D). 2. we edited the source code, to view what we edited check this https://github.com/fangq/mcx/pull/194/files  3. more detail about LED source see here 4. Geometry model setting IJV | \u7d44\u7e54\u6a21\u578b\u8a2d\u5b9a July, 2021 - Revised </p> <p>[How we execute MCX]: 1. we use mcx_ultrasound_opsbased.py to access the executive file which is compiled from the source code</p>"},{"location":"MCX_simulation/#mcx-forum","title":"MCX forum","text":"<p>This is the google group of MCX. There are lots of conversation and discussion about utilization of MCX. https://groups.google.com/g/mcx-users</p>"},{"location":"MCX_simulation/#mcx-cloud","title":"MCX cloud","text":"<p>When you run S3_run_sim.py, it will generate a file called input_run_&lt;#&gt;_forpreview.json. Copy the content and paste it in the MCX cloud to view the structure you designed.</p> <ul> <li>Find the input_run_&lt;#&gt;_forpreview.json in run_&lt;#&gt;/json_output/ </li> <li>Copy the content</li> <li>Open the MCX cloud website: https://mcx.space/cloud/# and click the JSON bottom. </li> <li>Paste your content here </li> <li>Change to Preview and have fun! </li> </ul>"},{"location":"MCX_simulation/#analyze-the-output-file-from-mcx","title":"Analyze the output file from MCX","text":"<p>Please look at utils.py to get the comprehension about how to analyze the output file from MCX such as pathlength, scattering count, diffuse reflectance and so on.</p>"},{"location":"MCX_simulation/#todo","title":"TODO","text":"<ul> <li>[ ] 1. Write an example code for how to use utils.py </li> <li>[ ] 2. To apply different NA on different SDS. https://github.com/ShawnSun1031/IJV-Project/blob/main/mcx_sim/S3_run_sim.py#L146 https://github.com/ShawnSun1031/IJV-Project/blob/main/mcx_sim/mcx_ultrasound_opsbased.py#L419</li> <li>[x] 3. Create functions to access the MCX simulation results. Function list (reference as ma): https://github.com/fangq/mcx/blob/master/utils/mcxmeanpath.m https://github.com/fangq/mcx/blob/master/utils/mcxmeanscat.m</li> </ul>"},{"location":"MCX_simulation/#tags-mcx-documentation","title":"tags: <code>MCX</code> <code>Documentation</code>","text":""},{"location":"introduction/","title":"Internal-Jugular-Vein Project","text":"<ul> <li>Author: Chin-Hsuan Sun</li> <li>License: MIT License</li> <li>Update Date: 2023/12/13</li> <li>Download Size: 128 MB</li> <li>Github Link: https://github.com/ShawnSun1031/IJV-Project</li> <li>Contact : dicky10311111@gmail.com </li> <li>IJV Web Site Link: https://shawnsun1031.github.io/IJV-Project/</li> </ul> <p>  This is the description of the IJV project, which aims to ensure code behavior consistency. Please ensure that your environment aligns with the following specifications.</p> <p> </p>"},{"location":"introduction/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Introduction</li> <li>Installation</li> <li>Project Flows<ul> <li>Simulation</li> <li>In-vivo</li> </ul> </li> <li>Desciprtion of Each Folder</li> <li>Reference</li> </ul>"},{"location":"introduction/#introduction","title":"Introduction","text":"<p>\u00a0\u00a0\u00a0The primary objective of this study is to quantitatively measure changes in internal jugular vein oxygen saturation non-invasively using near-infrared spectroscopy. Initially, a surrogate model based on neural networks is employed to accelerate the Monte Carlo method which is traditionally used to simulate photon transport in tissue. Subsequently, another neural network is applied to establish a predictive model for oxygen saturation changes. The input to this model consists of spectral features extracted using formulas same as modified Beer-Lambert law, while the output represents oxygen saturation changes. \u00a0\u00a0\u00a0As for the measurement system, the study utilizes 20 wavelength points based on the absorption spectra of blood, within the wavelength range of 700 nm to 850 nm. A dual-channel system is set up, with the short channel having a distance of 10 mm between the light source and detector, and the long channel having a distance of 20 mm. This design effectively minimizes the impact of superficial tissues and enhances the signal from deeper tissues including the internal jugular vein area. During simulation, a three-dimensional numerical model is constructed based on ultrasound images of each subject\u2019s neck, ensuring that simulation results closely resemble reality, thus providing more accurate simulated data. \u00a0\u00a0\u00a0To evaluate the prediction model\u2019s performance, the study investigates the impacts of factors such as human respiration, changes in oxygen levels in surrounding tissues, and measurement noise on the predictive model. The results indicate that the effects of respiration may lead to a maximum increase of 3% to 4% in root-mean-square error (RMSE). Changes in oxygen levels in surrounding tissues have a less significant impact, with a maximum RMSE increase of only 1%. Measurement signal errors can cause an RMSE increase of 1% to 2%. \u00a0\u00a0\u00a0For model generalization, the study conducts simulated experiments using transfer learning. Through experimentation, it is observed that by using a thousandth of the original dataset and employing transfer learning, an RMSE of 3.5% can be achieved, while without transfer learning and using only a thousandth of the dataset, an RMSE of 7% is obtained. \u00a0\u00a0\u00a0Based on the simulation results, the prediction model established in this study predicts changes in internal jugular vein oxygen saturation with an RMSE of less than 1.5%. In vivo experiments involve measuring diffuse reflectance spectra from living subjects, extracting spectral features using the formulas designed in this study, and inputting them into the prediction model after appropriate normalization. The prediction results are consistent with expected physiological response and spectral features in the measured data.  \u2014 Chin-Hsuan Sun </p>"},{"location":"introduction/#installation","title":"Installation","text":"<p> Suggestion: create a \\({\\rm\\color{red}{virtual \\space environment}}\\) and activate it. How to creaete a virtual environment? For Anaconda user, Read this document</p> <p> 1. make sure your local computer has \\({\\rm\\color{red}{cuda \\space toolkit}}\\) 2. \\({\\rm\\color{red}{recompile}}\\) the MCX source code at MD703_edit_MCX_src_v2023/src 3. Install the dependencies: <code>pip install -r requirements.txt</code> 4. Install cupy package  </p>"},{"location":"introduction/#project-flows","title":"Project Flows","text":""},{"location":"introduction/#simulation","title":"Simulation","text":"<ol> <li>Building Numerical Model of IJV by Ultrasound Image</li> <li>MCX (Monte Carlo) simulation</li> <li>Surrogate Model (To accerlerate the MC simulation)     &gt; To understand the concept, read this paper: link</li> <li>Prediction Model </li> </ol>"},{"location":"introduction/#in-vivo","title":"In-vivo","text":"<ol> <li>In-vivo experiment to validate simulation (ex: hyperventilation, valsalva maneuver, etc.)</li> <li>Preprocess raw data</li> <li>Calibration (remove system response)</li> <li>Feed processed data into prediction data</li> </ol>"},{"location":"introduction/#desciprtion-of-each-folder","title":"Desciprtion of Each Folder","text":"<ul> <li>MCX_src_modified_by_MD703<ul> <li>We modified the source code of MCX https://github.com/fangq/mcx. Please see this file to check what we modified if you're intereseted in. (adjust the source pattern)  </li> </ul> </li> <li>absoprtion_spectrum_by_substance<ul> <li>The diffuse reflectance spectra is generated by the chromophore in the tissue.</li> </ul> </li> <li>find_OPs_boundary<ul> <li>Based on multiple literature, finding the possible boundary of each optical parameters.</li> </ul> </li> <li>mcx_sim<ul> <li>Run Monte Carlo simulation based on the open source MCX we modified</li> </ul> </li> <li>ultrasound_image_processing_parallel<ul> <li>Constuct the numerical model of IJV by ultrasound image.</li> </ul> </li> <li>surrogate_model<ul> <li>Build the surrogate model to replace traditional MC simulation.</li> </ul> </li> </ul>"},{"location":"introduction/#reference","title":"Reference","text":"<ul> <li>To understand more detail, basically this repository is followed by my master thesis. Please check NAS:Data/BOSI Lab/Thesis/R10 to access the full text version.</li> </ul>"},{"location":"ultrasound_image_processing_parallel/","title":"Ultrasound Image Processing","text":""},{"location":"ultrasound_image_processing_parallel/#1-get-ultrasound-image-by-screenshot-to-video","title":"1. get ultrasound image by screenshot to video","text":"<p>you need to find to geometry structure one for IJV systolic (IJV small), one for IJV diastolic (IJV large).</p> <p> </p>"},{"location":"ultrasound_image_processing_parallel/#2-s1_ijv_model_find_tissuepy","title":"2. S1_ijv_model_find_tissue.py","text":"<p>labeling each tissue type (skin, fat, muscle, ijv, cca)</p> <ol> <li>for skin and fat, we find a boundary(straight line) to decide skin and fat.</li> <li>for ijv, using GUI to get the region of IJV</li> <li>for cca, we use circle equation to determine the region of cca </li> </ol> <p>As the figure shown above, the from z=0 to green line is the skin area, from green line to blue line is fat area ... vice and versa.</p> <p>Basically, how to run S1_ijv_model_find_tissue.py is according to blood_vessel_segmentation_line_new.json as shown below.</p> <pre><code>{\n    \"HW\":{\n        \"20230903\":{\n            \"bound\": [3, 650],\n            \"length10mmEdge\": [215, 445],\n            \"skin\":{\n                \"__comment__\": \"axis-0 of imagee is x for line.\",\n                \"x\": 35\n            },\n            \"fat\":{\n                \"__comment__\": \"axis-0 of imagee is x for line.\",\n                \"x\":70\n            },\n            \"IJVLarge\":{\n                \"cca\":{\n                    \"__comment__\":\"[x, y, radius]\",\n                    \"v\": [470, 500, 100]\n                }\n            },\n            \"IJVSmall\":{\n                \"cca\":{\n                    \"__comment__\":\"[x, y, radius]\",\n                    \"v\": [470, 500, 100]\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>** json file is a dictionary structure, if you don't know you can see the link https://tw.alphacamp.co/blog/json</p> <p>** bound --&gt; clip the image to what we want.</p> <p>** length10mmEdge --&gt; scale bar</p>"},{"location":"ultrasound_image_processing_parallel/#s2_ijv_model_seg_constructpy","title":"S2_ijv_model_seg_construct.py","text":"<ol> <li>make sure <code>subject</code> and <code>date</code> is synchronous with S1_ijv_model_find_tissue.py</li> <li>run the program</li> </ol> <p>Then you will success to get the voxel file (*vol.npy) </p>"},{"location":"ultrasound_image_processing_parallel/#s3_plot_3d_ijv_modelpy","title":"S3_plot_3D_ijv_model.py","text":"<ol> <li>make sure <code>subject</code> and <code>date</code> is synchronous with S1_ijv_model_find_tissue.py</li> <li>run the program </li> </ol>"},{"location":"ultrasound_image_processing_parallel/#reference","title":"Reference","text":"<p>To analyze ultrasound image by using ImageJ, please view this document.</p> <p>TODO :  oblique IJV model --&gt; https://stackoverflow.com/questions/48818373/interpolate-between-two-images</p> <p>https://stackoverflow.com/questions/59690451/how-to-turn-ct-segmentation-into-3d-model-in-python</p>"},{"location":"handover/IJV_optical_properties/","title":"IJV | optical properties","text":""},{"location":"handover/IJV_optical_properties/#1-skin","title":"1. skin","text":""},{"location":"handover/IJV_optical_properties/#2-subcutaneous-adipose","title":"2. subcutaneous adipose","text":"<ul> <li>Optical Properties of skin, subcutaneous, and muscle tissues : a review; Tabel 1</li> </ul> wavelength (nm) \\(\\mu_a\\) (cm^-1^) \\(\\mu_s\\)' (cm^-1^) 600 1.89 27.0 700 1.27 23.0 800 1.08 20.2 <ul> <li>Optical properties of biological tissues: a review</li> </ul>"},{"location":"handover/IJV_optical_properties/#3-muscle","title":"3. muscle","text":"<ul> <li>Optical Properties of skin, subcutaneous, and muscle tissues : a review ; Table 1</li> </ul> wavelength (nm) \\(\\mu_a\\) (cm^-1^) \\(\\mu_s\\) (cm^-1^) g 650 0.56 79.0 0.930 700 0.52 73.56 0.930 750 0.52 71.30 0.931 800 0.54 66.70 0.930 <ul> <li> <p>Optical properties of biological tissues: a review</p> </li> <li> <p>Near-infrared optical properties of ex vivo human skin and subcutaneous tissues measured using the Monte Carlo inversion technique</p> </li> </ul> wavelength (nm) \\(\\mu_a\\) (mm^-1^) \\(\\mu_s\\)' (mm^-1^) 633 0.121 0.89 700 0.046 0.83 900 0.032 0.59"},{"location":"handover/IJV_optical_properties/#4-whole-blood","title":"4. whole blood","text":"<ul> <li>Optical Properties of Whole Blood; Table 3 </li> </ul> wavelength (nm) \\(\\mu_a\\) (cm^-1^) \\(\\mu_s\\) (cm^-1^) g 488 102.2 134.4 0.91 <ul> <li>Optical properties of blood in the near-infrared spectral range; Fig 2</li> </ul> wavelength (nm) \\(\\mu_a\\) (mm^-1^) \\(\\mu_s\\) (mm^-1^) g 705.6 0.397 72.889 0.986 724.9 0.402 73.010 0.990 745.1 0.454 72.761 0.990 765.1 0.528 69.956 0.991 784.8 0.578 69.169 0.989 804.4 0.648 68.516 0.989 824.6 0.729 73.709 0.991 <ul> <li>A literature review and novel theoretical approach on the optical properties of whole blood; Fig 1, Fig 2 <p>SO2 = 0%</p> </li> </ul> wavelength (nm) \\(\\mu_s\\) (mm^-1^) \\(\\mu_s\\)'(mm^-1^) g 650 83.074 1.122 0.986 660 81.367 1.118 0.986 669 79.088 1.109 0.985 680 76.912 1.096 0.985 690 75.283 1.087 0.985 699 73.740 1.087 0.985 710 72.989 1.091 0.985 720 71.833 1.096 0.984 730 70.766 1.096 0.984 740 68.852 1.091 0.984 750 67.284 1.074 0.983 760 65.881 1.068 0.983 769 64.177 1.062 0.983 780 63.324 1.066 0.983 790 62.863 1.082 0.982 800 62.119 1.096 0.982 810 60.778 1.079 0.982 <p>SO2 &gt; 98%</p> wavelength (nm) \\(\\mu_s\\) (mm^-1^) \\(\\mu_s\\)'(mm^-1^) g 650 94.111 1.363 0.985 660 92.125 1.347 0.985 670 89.803 1.333 0.985 680 87.433 1.318 0.984 690 85.871 1.307 0.984 700 83.911 1.295 0.984 710 82.766 1.307 0.984 720 81.606 1.319 0.984 730 80.270 1.318 0.983 740 78.520 1.302 0.983 750 76.786 1.296 0.983 760 75.055 1.285 0.983 770 73.318 1.277 0.982 780 72.399 1.276 0.982 790 71.652 1.293 0.981 800 70.482 1.303 0.981 810 69.338 1.296 0.981 <p> </p>"},{"location":"handover/ImageJ_tutorial/","title":"ImageJ \u6559\u5b78\uff1a\u5206\u6790\u8d85\u97f3\u6ce2\u5f71\u50cf","text":"<p>[name=dicky1031][time=Fri, Oct 6, 2023 5:57 PM][color=#1aba0e]</p>"},{"location":"handover/ImageJ_tutorial/#imagej_1","title":"\u4e0b\u8f09ImageJ","text":"<p>\u5230\u5b98\u7db2\u4e0b\u8f09\u76f8\u5c0d\u61c9\u7684\u61c9\u7528\u7a0b\u5f0f https://imagej.nih.gov/ij/download.html </p> <p>\u6253\u958bImageJ </p>"},{"location":"handover/ImageJ_tutorial/#_1","title":"\u64cd\u4f5c\u6b65\u9a5f","text":""},{"location":"handover/ImageJ_tutorial/#_2","title":"\u532f\u5165\u8d85\u97f3\u6ce2\u5f71\u50cf","text":"<p>File -&gt; Open </p>"},{"location":"handover/ImageJ_tutorial/#_3","title":"\u627e\u51fa\u6bd4\u4f8b\u5c3a","text":"<p>\u4f7f\u7528straight line\u628a\u5716\u4e0a\u768410mm\u90e8\u5206\u756b\u51fa\u4f86 </p> <p>Analyze --&gt; Set Scale  \u6b64\u6642\u5982\u5716\u4e0a\u6240\u793a\uff0c\u7a0b\u5f0f\u544a\u8a34\u4f60\u9019\u6bb5\u9577\u5ea6\u7b49\u65bc107.0047\u500bpixels\uff0c \u9019\u6642\u628aknown distance\u586b 10 unit of length \u586b mm \u63a5\u8457\u6309ok </p>"},{"location":"handover/ImageJ_tutorial/#ijv","title":"\u91cf\u6e2cIJV\u9577\u77ed\u8ef8","text":"<p>\u9078\u7528\u76f4\u7dda\uff0c\u5728\u9577\u8ef8\u77ed\u8ef8\u756b\u51fa\u4f86\u5f8c\uff0cAnalyze--&gt;Measure(\u5feb\u6377\u9375Ctrl+M)\u4f86\u91cf\u6e2c\u9577\u5ea6 </p>"},{"location":"handover/ImageJ_tutorial/#_4","title":"\u5ef6\u4f38\u95b1\u8b80","text":"<ol> <li>ImageJ \u6559\u5b78\uff1a\u5206\u6790\u5149\u8b5c\u7167\u7247</li> </ol>"},{"location":"handover/ImageJ_tutorial/#tags-imagej","title":"tags : <code>ImageJ</code>","text":""},{"location":"handover/MD703_Computer_Setting_Tutorial/","title":"MD703 Computer Setting Tutorial","text":""},{"location":"handover/MD703_Computer_Setting_Tutorial/#1-linux-ubuntu-1804-anaconda-follow","title":"1. [Linux] Ubuntu 18.04\u7cfb\u7d71\u4e0b\u5b89\u88dd Anaconda, follow \u9019\u500b\u7db2\u7ad9\u7684\u6b65\u9a5f\u5373\u53ef\u3002","text":"<p>https://ithelp.ithome.com.tw/m/articles/10290542</p>"},{"location":"handover/MD703_Computer_Setting_Tutorial/#2-conda","title":"2. \u4e00\u4e9bconda \u74b0\u5883\u914d\u7f6e\u6307\u4ee4:","text":"<p><code>conda create --name test python=3.8</code>  \u5275\u5efa\u4e00\u500b\u540d\u5b57\u70ba\u201dtest\u201d python\u7248\u672c\u70ba3.8\u7684\u865b\u64ec\u74b0\u5883  <code>conda list</code> \u8a72\u865b\u64ec\u74b0\u5883\u4e0b\u6240\u5df2\u7d93\u5b89\u88dd\u7684\u5957\u4ef6(package)  <code>conda install spyder</code> \u5b89\u88ddspyder\u9019\u500b\u5957\u4ef6\uff0c\u8f38\u5165<code>spyder</code> \u6253\u958bspyer IDE  <code>conda deactivate</code> \u9000\u51fa\u6b64\u865b\u64ec\u74b0\u5883 <code>conda env list</code> \u986f\u793a\u73fe\u5728\u6709\u7684\u865b\u64ec\u74b0\u5883(\u5982\u5716\u6709base\u3001test\u9019\u5169\u500b\u74b0\u5883)  conda remove -n test --all \u522a\u9664\"test\"\u9019\u500b\u865b\u64ec\u74b0\u5883  \u5982\u4e0a\u5716\u6b64\u6642\"test\"\u9019\u500b\u865b\u64ec\u74b0\u5883\u5df2\u7d93\u522a\u9664\u3002</p>"},{"location":"handover/MD703_Computer_Setting_Tutorial/#3-cudafollow-nvidiacuda117httpsdevelopernvidiacomcuda-11-7-0-download-archivetarget_oslinuxtarget_archx86_64distributionubuntutarget_version1804target_typedeb_local","title":"3. cuda\u5b89\u88dd\uff0cfollow nvidia\u5b98\u7db2\uff0c\u770b\u4f60\u662f\u8981\u88dd\u751a\u9ebc\u7248\u672c\u7684\uff0c\u6b64\u7bc4\u4f8b\u5b89\u88dd\u7248\u672c\u70bacuda11.7\uff0chttps://developer.nvidia.com/cuda-11-7-0-download-archive?target_os=Linux&amp;target_arch=x86_64&amp;Distribution=Ubuntu&amp;target_version=18.04&amp;target_type=deb_local","text":"<p> \u5728\u88dd\u7684\u6642\u5019\u4ed6\u6703\u554f\u4f60\u8a2d\u5b9amok\u91d1\u9470\uff0c\u8f38\u5165\u81ea\u5df1\u8a2d\u5b9a\u7684\u5bc6\u78bc\u5f8c\u91cd\u958b\u6a5f\uff0c\u6703\u9032\u5165enroll\u756b\u9762\uff0c\u9078\u64c7MOK\u8a18\u5f97\u8f38\u5165\u525b\u525b\u8a2d\u5b9a\u7684\u5bc6\u78bc\u3002  \u82e5\u662f\u4e0d\u5e78\u64cd\u4f5c\u932f\u8aa4\u6309\u5230continue boot\uff0c\u96fb\u8166\u76f4\u63a5\u958b\u6a5f\uff0c\u6c92\u6709enroll\u5230\u7684\u8a71\uff0c\u76f4\u63a5\u5728terminal\u8f38\u5165 <code>sudo mokutil --import /var/lib/shim-signed/mok/MOK.der</code> \u5373\u53ef\uff0c\u8a73\u7d30\u554f\u984c\u7684\u767c\u751f\u5167\u5bb9\u4ee5\u53ca\u89e3\u6cd5\u53c3\u7167https://askubuntu.com/questions/1122855/mok-manager-nvidia-driver-issue-after-cuda-install \u7576cuda\u88dd\u597d\u5f8c\uff0c\u4f7f\u7528 <code>nvidia-smi</code> \u53ef\u4ee5\u67e5\u770bgpu\u7684\u60c5\u6cc1\uff0c\u82e5\u662f\u6709\u51fa\u73fe\u6771\u897f\u5c31\u8868\u793a\u88dd\u6210\u529f\u4e86\u3002 </p>"},{"location":"handover/MD703_Computer_Setting_Tutorial/#4-disk","title":"4. \u5728\u4f7f\u7528\u5be6\u9a57\u5ba4\u96fb\u8166\u7684\u6642\u5019\uff0c\u9664\u4e86\u914d\u7f6e\u5c6c\u65bc\u81ea\u5df1\u7684\u865b\u64ec\u74b0\u5883\u5916\uff0c\u6700\u597d\u5728\u7a7a\u9593\u8db3\u5920\u7684\u786c\u789f(disk)\u5275\u5efa\u81ea\u5df1\u7684\u8cc7\u6599\u593e\uff0c\u628a\u81ea\u5df1\u7684\u6771\u897f\u56fa\u5b9a\u7576\u653e\u5728\u88e1\u9762\uff0c\u4e0d\u7136\u5230\u6642\u5019\u6771\u4e00\u500b\u897f\u4e00\u500b\u751a\u81f3\u8ddf\u5225\u4eba\u7684\u6771\u897f\u6df7\u96dc\u5728\u4e00\u8d77\u6703\u5f88\u4e0d\u65b9\u4fbf\u3002\u6240\u4ee5\u5148\u9078\u4e00\u500b\u7a7a\u9593\u8db3\u5920\u7684\u786c\u789f","text":""},{"location":"handover/MD703_Computer_Setting_Tutorial/#5-pytorch","title":"5. pytorch \u5b89\u88dd","text":"<p>\u5230pytorch\u5b98\u65b9\u7db2\u7ad9https://pytorch.org/\u3002 \u9078\u64c7\u76f8\u5c0d\u61c9\u7684\u9078\u9805(\u7bc4\u4f8b\u4f7f\u7528pip\u5b89\u88ddpytorch\uff0c\u4e26\u4e14\u9078\u64c7\u7684\u662fCUDA11.7\u7684\u7248\u672c) </p>"},{"location":"handover/eidt_mcx_source_code/","title":"\u5982\u4f55\u4fee\u6539MCX\u539f\u7248code","text":"<p>[time=Sat, Jan 29, 2022 11:11 PM] [name=dicky1031] [TOC]</p>"},{"location":"handover/eidt_mcx_source_code/#1","title":"1. \u521d\u59cb\u8a2d\u5b9a","text":"<p>\u539f\u59cb\u7a0b\u5f0f\u78bc\u9023\u7d50 \u6211\u4fee\u6539\u7684\u90e8\u5206\u5f9ebranch-master clone\u800c\u4f86 \u4fee\u6539\u8cc7\u6599\u593e MCX/src (\u539f\u7248MCX C code\u90fd\u6703\u5728\u9019\u88cf\u9762)</p>"},{"location":"handover/eidt_mcx_source_code/#2-code","title":"2. \u539f\u59cbcode\u529f\u80fd\u8aaa\u660e","text":"<p>\u5224\u65b7\u54ea\u500bdetector\u5075\u6e2c\u5230: mcx_core.cu : finddetector \u4f7f\u7528\u51fa\u5149\u5149\u5b50\u8207detector XYZ\u8ddd\u96e2\u7684\u5e73\u65b9\u5408\uff0c\u770b\u662f\u5426\u5728detector\u534a\u5f91\u4ee5\u5167</p> <p>mcx_core.cu : mcx_main_loop \u662fMC\u7684\u4e3b\u8981loop \u4ed6\u771f\u7684\u662f\u5728\u6bcf\u500bwhile loop\u88e1\u9762\u4e00\u500b\u4e00\u500bvoxel\u79fb\u52d5</p> <p>random\u51fa\u4e00\u500bunitless PL\u5f8c \u8d70\u4e00\u500bvoxel\u82b1\u7684\u8ddd\u96e2\u8981\u4e58\u4e0amus\u624d\u662f\u4ed6\u82b1\u6389\u7684PL</p> <p>\u4e0d\u904e\u4ed6\u88e1\u9762\u6709\u8a08\u7b97mua\u8870\u6e1b\u7684code mcx_core.cu : line 1255~1259</p> <p>finddetector\u6703\u88absavedetphoton\u547c\u53eb\u5230\uff0c\u7528\u4f86\u6c7a\u5b9a\u8981\u5132\u5b58\u5728\u54ea\u500bdetector \u800csavedetphoton\u6703\u5728\u6bcf\u6b21launchnewphoton\u6642\u88ab\u547c\u53eb\u5230\uff0c\u628a\u4e0a\u4e00\u9846\u5149\u5b50\u7684\u8cc7\u8a0a\u5b58\u8d77\u4f86 launchnewphoton\u5247\u6703\u5728mcx_main_loop\u88e1\u9762\u7684\u958b\u59cb\u65b0\u5149\u5b50\u3001\u5149\u5b50\u8dd1\u51fa\u7d44\u7e54\u5916\u3001\u8f2a\u76e4\u73a9\u8f38\u4e4b\u5f8c\u88ab\u547c\u53eb\u5230 \u770b\u8d77\u4f86\u9817\u5408\u7406 \u56e0\u6b64\u61c9\u8a72\u4e0d\u6703\u6709\u4e0a\u6b21\u61f7\u7591\u7684\uff0cdetector\u592a\u5927(\u8d85\u51fa\u4e00\u500bvoxel)\u5c0e\u81f4\u6536\u5230\u7684\u5149\u5b50\u4e0d\u5c0d\u7684\u72c0\u6cc1</p> <p>mcx_main_loop\u4e4b\u4e2d launchnewphoton\u5728\u4e00\u958b\u59cb\u3001\u73a9\u8f2a\u76e4\uff0c\u6216\u662ftime window\u8d85\u904e\u3001\u5149\u5b50\u5f9enon-zero voxel\u8dd1\u5230zero voxel\u6642\u6703\u88ab\u547c\u53eb</p> <p>launchnewphoton: \u7d50\u675f\u76ee\u524d\u7684\u5149\u5b50\uff0c\u4e26\u4e14\u958b\u59cb\u65b0\u7684\u5149\u5b50 \u5982\u679cmediaid(the medium index at the voxel at launch)==0\u4e14isdet(\u524d\u4e00\u9846\u5149\u5b50\u6253\u5230\u4e86detector)\u7684\u8a71\uff0c\u88e1\u9762\u6703\u547c\u53eb\u5230savedetphoton\uff0c\u624d\u6703\u628a\u5075\u6e2c\u5230\u500b\u5149\u5b50\u5b58\u8d77\u4f86</p> <p>transmit\u6703\u5728\u6aa2\u67e5\u5149\u5b50\u662f\u5426\u8dd1\u5230zero voxel\u524d\u88ab\u547c\u53eb\uff0c\u800ctransmit\u6703\u6539\u8b8a\u65b9\u5411\uff0c\u6240\u4ee5\u8f38\u51fa\u7684\u5149\u5b50\u65b9\u5411\u61c9\u8a72\u662f\u5df2\u7d93\u8dd1\u5230\u5916\u9762\u7684\u65b9\u5411</p>"},{"location":"handover/eidt_mcx_source_code/#3-make-mex","title":"3. Make mex","text":""},{"location":"handover/eidt_mcx_source_code/#31-mex","title":"3.1 \u7de8\u8b6fmex\u6a94\u6848","text":"<p>mex\u70bamatlab\u57f7\u884cC\u8a9e\u6cd5\u7684\u57f7\u884c\u6a94\u6848\uff0c\u6240\u4ee5\u7576\u4fee\u6539\u5b8c\u539f\u59cb\u7248C\u8a9e\u8a00\u7684\u7a0b\u5f0f\u78bc\u5f8c\uff0c\u9700\u8981\u518d\u5c07\u5176\u7de8\u8b6f\u6210matlab\u53ef\u4ee5\u57f7\u884c\u7684mex\u6a94\u6848 \u5177\u9ad4\u4f7f\u7528\u65b9\u5f0f: \u6253\u958b\u7d42\u7aef\u6a5f(terminal)</p> <pre><code>cd ~/mcx-master/src\ncmake ~/mcx-master/src\nmake\n</code></pre> <p>\u4e4b\u5f8c\u5982\u679c\u90fd\u6c92\u5831\u932f\uff0c\u7de8\u8b6f\u597d\u7684mex\u6a94\u6848\u5c07\u6703\u5728mcx-master/bin\u88e1\u9762</p>"},{"location":"handover/eidt_mcx_source_code/#32","title":"3.2 \u5982\u679c\u9047\u5230\u932f\u8aa4\u7684\u89e3\u6c7a\u65b9\u6848","text":""},{"location":"handover/eidt_mcx_source_code/#321-gpu","title":"3.2.1 GPU\u74b0\u5883\u4e0d\u76f8\u5bb9","text":"<pre><code>nvcc fatal   : Value 'sm_30' is not defined for option 'gpu-architecture'\n</code></pre> <p>\u5982\u679c\u770b\u5230\u6b64\u932f\u8aa4(\u6ce8\u610f\u932f\u8aa4\u53ef\u80fd\u70basm_\uff0c\u70ba\u4efb\u610f\u6578\u5b57\u4ee3\u8868\u4e0d\u540c\u7248\u672c)\uff0c\u4ee3\u8868gpu\u74b0\u5883\u7de8\u8b6f\u6642\u767c\u751f\u4e0d\u76f8\u5bb9\u7684\u60c5\u6cc1\uff0c\u6b64\u6642\u8acb\u9ede\u6b64\u9023\u7d50\u67e5\u770b\u672c\u6a5f\u7aef\u96fb\u8166\u7684gpu\u898f\u683c\u3002</p> <p>ex:\u5047\u5982GPU\u70baRTX2080\uff0c\u5247\u8868\u793a\u6b64\u67b6\u69cb\u70baTuring (CUDA 10 and later)\u9069\u7528sm_75\u7248\u672c\u3002 \u56e0\u6b64\u9700\u8981\u5c07~/mcx-master/src\u9019\u500b\u8def\u5f91\u4e0b\u7684cmakelist.txt\u505a\u4fee\u6539\u3002</p> <pre><code># NVCC Options\nset(\n    CUDA_NVCC_FLAGS\n    ${CUDA_NVCC_FLAGS};\n    -g -lineinfo -Xcompiler -Wall -Xcompiler -fopenmp -O3 -arch=sm_30\n    -DMCX_TARGET_NAME=\"fermi  MCX\" -DUSE_ATOMIC -use_fast_math\n    -DSAVE_DETECTORS -Xcompiler -fPIC\n    )\n</code></pre> <p>\u5c07\u4e0a\u8ff0\u7a0b\u5f0f\u78bc\u4e2d\u7684 sm_30 \u4fee\u6539\u70ba sm_75\uff0c -DMCX_TARGET_NAME=\"fermi  MCX\" --&gt; -DMCX_TARGET_NAME=\"Turing  MCX\"</p>"},{"location":"handover/eidt_mcx_source_code/#322-zlib","title":"3.2.2 ZLIB\u932f\u8aa4","text":"<pre><code>Could NOT find ZLIB (missing: ZLIB_LIBRARY ZLIB_INCLUDE_DIR)\n</code></pre> <p>\u5982\u679c\u770b\u5230\u4e0a\u8ff0\u932f\u8aa4\u5247\u4ee3\u8868\u9700\u5b89\u88ddZLIB\uff0c\u57f7\u884c\u4ee5\u4e0b\u7a0b\u5f0f\u78bc\u5373\u53ef</p> <pre><code>sudo apt-get install zlib1g-dev\n</code></pre> <p>Config structure \u5728 mcx_utils.h 259\u884c mcx_run_simulation \u5728 mcx_core.cu 2236\u884c mcxlab.cpp 257\u884c launchnewphoton \u5728 mcx_core.cu 1026\u884c</p>"},{"location":"handover/eidt_mcx_source_code/#ifndefdefineendif","title":"ifndef#define#endif\u7684\u7528\u6cd5(\u6574\u7406)","text":"<p>https://huenlil.pixnet.net/blog/post/24339151</p> <pre><code>__device__ inline uint finddetector(MCXpos *p0){\n      uint i;\n      for(i=gcfg-&gt;maxmedia+1;i&lt;gcfg-&gt;maxmedia+gcfg-&gt;detnum+1;i++){\n        if((gproperty[i].x-p0-&gt;x)*(gproperty[i].x-p0-&gt;x)+\n       (gproperty[i].y-p0-&gt;y)*(gproperty[i].y-p0-&gt;y)+\n       (gproperty[i].z-p0-&gt;z)*(gproperty[i].z-p0-&gt;z) &lt; gproperty[i].w*gproperty[i].w){\n            return i-gcfg-&gt;maxmedia;\n       }\n      }\n      return 0;\n}\n\n</code></pre> <p>\u5728 core.cu \u76842764\u884c /* Copy param to the constant memory variable gcfg / \u628a cfg\u7684data\u8907\u88fd\u5230 gcfg(\u5728GPU\u4e0a)</p> <pre><code>__host__ \u200bcudaError_t cudaMemcpyToSymbol ( const void* symbol, const void* src, size_t count, size_t offset = 0, cudaMemcpyKind kind = cudaMemcpyHostToDevice )\n\nCopies data to the given symbol on the device.\n\nParameters\nsymbol\n- Device symbol address\nsrc\n- Source memory address\ncount\n- Size in bytes to copy\noffset\n- Offset from start of symbol in bytes\nkind\n- Type of transfer\n\n</code></pre> <p>2348\u884c</p> <pre><code>MCXParam param={cfg-&gt;steps,minstep,0,0,cfg-&gt;tend,R_C0*cfg-&gt;unitinmm,\n                     (uint)cfg-&gt;issave2pt,(uint)cfg-&gt;isreflect,(uint)cfg-&gt;isrefint,(uint)cfg-&gt;issavedet,1.f/cfg-&gt;tstep,\n             p0,c0,s0,maxidx,uint4(0,0,0,0),cp0,cp1,uint2(0,0),cfg-&gt;minenergy,\n                     cfg-&gt;sradius*cfg-&gt;sradius,minstep*R_C0*cfg-&gt;unitinmm,cfg-&gt;srctype,\n             cfg-&gt;srcparam1,cfg-&gt;srcparam2,cfg-&gt;voidtime,cfg-&gt;maxdetphoton,\n             cfg-&gt;medianum-1,cfg-&gt;detnum,cfg-&gt;polmedianum,cfg-&gt;maxgate,0,0,ABS(cfg-&gt;sradius+2.f)&lt;EPS /*isatomic*/,\n             (uint)cfg-&gt;maxvoidstep,cfg-&gt;issaveseed&gt;0,(uint)cfg-&gt;issaveref,cfg-&gt;isspecular&gt;0,\n             cfg-&gt;maxdetphoton*hostdetreclen,cfg-&gt;seed,(uint)cfg-&gt;outputtype,0,0,cfg-&gt;faststep,\n             cfg-&gt;debuglevel,cfg-&gt;savedetflag,hostdetreclen,partialdata,w0offset,cfg-&gt;mediabyte,\n             (uint)cfg-&gt;maxjumpdebug,cfg-&gt;gscatter,is2d,cfg-&gt;replaydet,cfg-&gt;srcnum,cfg-&gt;nphase,cfg-&gt;omega};\n</code></pre> <p>param initial\u6642\u6c92\u6709\u5ba3\u544abc \u4e4b\u5f8c\u624d\u5ba3\u544abc\u5982\u4e0b</p> <p>2677\u884c</p> <pre><code>memcpy(&amp;(param.bc),cfg-&gt;bc,12);\n</code></pre> <p>\u958b\u59cb\u4fee\u6539 \u5728MCX_utils.h\u7684259\u884c Config{}\u589e\u52a0 float detreflect\u4e4b\u5c6c\u6027;</p> <p>matlab structure to cJson https://github.com/fangq/mcx/tree/master/src/cjson</p> <p>mcx_loadjson</p>"},{"location":"handover/handover/","title":"IJV\u4ea4\u63a5\u6587\u4ef6","text":""},{"location":"handover/handover/#eric","title":"Eric \u4ea4\u63a5\u6587\u4ef6","text":"<ul> <li> <p>IJV Notebook</p> </li> <li> <p>\u7d44\u7e54\u6a21\u578b\u8a2d\u5b9a July, 2021 - Revised</p> </li> <li> <p>Useful mcx-user group threads \u8207 MCX Code \u6574\u7406</p> </li> <li> <p>QEPro \u74b0\u5883\u5b89\u88dd</p> </li> <li> <p>\u76ee\u524dholder\u8a2d\u8a08\u5716\u3001\u6d3b\u9ad4\u91cf\u6e2c\u6ce8\u610f\u4e8b\u9805</p> </li> <li> <p>3D\u7e6a\u5716\u82073D\u5217\u5370</p> </li> </ul>"},{"location":"handover/handover/#scs","title":"SCS \u4ea4\u63a5\u6587\u4ef6","text":"<ul> <li> <p>Internal-Jugular-Vein Project (description)</p> </li> <li> <p>IJV Optical Properties</p> </li> <li> <p>\u5be6\u9a57\u5ba4\u96fb\u8166\u74b0\u5883\u914d\u7f6e\u6559\u5b78</p> </li> <li> <p>\u5982\u4f55\u4fee\u6539MCX\u539f\u7248code</p> </li> <li> <p>ImageJ \u6559\u5b78\uff1a\u5206\u6790\u8d85\u97f3\u6ce2\u5f71\u50cf</p> </li> <li> <p>Ultrasound Image Processing</p> </li> <li> <p>Windows 10 \u5b89\u88dd \u986f\u5361\u9a45\u52d5\u3001CUDA Toolkit\u3001cuDNN</p> </li> </ul>"},{"location":"handover/ijv_numeric_model_constructed_by_ultrasound/","title":"Ultrasound Image Processing","text":""},{"location":"handover/ijv_numeric_model_constructed_by_ultrasound/#1-get-ultrasound-image-by-screenshot-to-video","title":"1. get ultrasound image by screenshot to video","text":"<p>you need to find to geometry structure one for IJV systolic (IJV small), one for IJV diastolic (IJV large).</p> <p> </p>"},{"location":"handover/ijv_numeric_model_constructed_by_ultrasound/#2-s1_ijv_model_find_tissuepy","title":"2. S1_ijv_model_find_tissue.py","text":"<p>labeling each tissue type (skin, fat, muscle, ijv, cca)</p> <ol> <li>for skin and fat, we find a boundary(straight line) to decide skin and fat.</li> <li>for ijv, using GUI to get the region of IJV</li> <li>for cca, we use circle equation to determine the region of cca </li> </ol> <p>As the figure shown above, the from z=0 to green line is the skin area, from green line to blue line is fat area ... vice and versa.</p> <p>Basically, how to run S1_ijv_model_find_tissue.py is according to blood_vessel_segmentation_line_new.json as shown below.</p> <pre><code>{\n    \"HW\":{\n        \"20230903\":{\n            \"bound\": [3, 650],\n            \"length10mmEdge\": [215, 445],\n            \"skin\":{\n                \"__comment__\": \"axis-0 of imagee is x for line.\",\n                \"x\": 35\n            },\n            \"fat\":{\n                \"__comment__\": \"axis-0 of imagee is x for line.\",\n                \"x\":70\n            },\n            \"IJVLarge\":{\n                \"cca\":{\n                    \"__comment__\":\"[x, y, radius]\",\n                    \"v\": [470, 500, 100]\n                }\n            },\n            \"IJVSmall\":{\n                \"cca\":{\n                    \"__comment__\":\"[x, y, radius]\",\n                    \"v\": [470, 500, 100]\n                }\n            }\n        }\n    }\n}\n</code></pre> <p>** json file is a dictionary structure, if you don't know you can see the link https://tw.alphacamp.co/blog/json</p> <p>** bound --&gt; clip the image to what we want.</p> <p>** length10mmEdge --&gt; scale bar</p>"},{"location":"handover/ijv_numeric_model_constructed_by_ultrasound/#s2_ijv_model_seg_constructpy","title":"S2_ijv_model_seg_construct.py","text":"<ol> <li>make sure <code>subject</code> and <code>date</code> is synchronous with S1_ijv_model_find_tissue.py</li> <li>run the program</li> </ol> <p>Then you will success to get the voxel file (*vol.npy) </p>"},{"location":"handover/ijv_numeric_model_constructed_by_ultrasound/#s3_plot_3d_ijv_modelpy","title":"S3_plot_3D_ijv_model.py","text":"<ol> <li>make sure <code>subject</code> and <code>date</code> is synchronous with S1_ijv_model_find_tissue.py</li> <li>run the program </li> </ol> <p>TODO : oblique IJV model --&gt; https://stackoverflow.com/questions/48818373/interpolate-between-two-images</p> <p>https://stackoverflow.com/questions/59690451/how-to-turn-ct-segmentation-into-3d-model-in-python</p>"},{"location":"handover/eric/IJV_Notebook/","title":"IJV | Notebook","text":""},{"location":"handover/eric/IJV_Notebook/#20211103","title":"2021/11/03","text":"<p>==Debug== 1. \u4e0d\u540c x size \u4e0b\u7684 detected photons     - \u8f03\u5927\u53ef\u80fd\u4e0a\u5347\u4e5f\u53ef\u80fd\u4e0b\u964d     - \u4e00\u6a23\u7684\u8a71\u61c9\u8a72\u8981\u4e0d\u8b8a     - \u7e2e\u5c0f\u7684\u8a71\u61c9\u8a72\u8981\u8b8a\u5c11 2. \u7c21\u55ae\u6a21\u578b\u4e0b\uff0cGrid \u8207 jdata \u7684\u5f62\u5f0f\u53bb\u6a21\u64ec\uff0c\u5f97\u5230\u7684 \"detected photons\" \u8207 \"absorbed\" \u6703\u4e00\u6a23 3. \u4ee5 jdata \u53bb\u6a21\u64ec\uff0cmodel size \u7e2e\u6e1b\u7684\u8da8\u52e2\u7b26\u5408\u9810\u671f\u3002     - \u4f46 mus \u904e\u5927\u6642\uff0creflectance \u6703\u904e\u5c0f</p> <p>==\u518d\u78ba\u8a8d\u6a21\u578b\u662f\u5426\u6709\u8aa4== 1. \u5149\u6e90\u5206\u4f48 - \u6c92\u554f\u984c 2. \u6a21\u64ec\u64fa\u8a2d\u4f4d\u7f6e(\u5c0d\u7a31\u6027\uff0c\u5149\u6e90\u9700\u7f6e\u65bc\u4e2d\u592e) -- debug: unitmm \u6700\u5927\u53ea\u80fd 0.5mm</p> <p>==determine== 1. Eric large - mus_ub</p> <p>==final size test== 1. Eric large - mus_ub (big model) 2. Eric large - mus_ub (small model)</p> <p>==\u95e1\u91cb\u6563\u5c04\u5f71\u97ff== 1. Eric large - mus_ub v.s. Eric large - mus_lb</p> <p>==for\u6615\u539f== 1. Eric small - mus_lb</p>"},{"location":"handover/eric/IJV_Notebook/#20211024-","title":"2021/10/24 - \u8d85\u97f3\u6ce2\u6a21\u578b\u78ba\u8a8d","text":"<p>\u6a21\u578b 1. ~~scaling \u662f\u5426\u6b63\u78ba~~ 2. ~~z \u65b9\u5411 \u2192 \u6bcf\u4e00\u5c64\u7684\u683c\u6578\u662f\u5426\u7b26\u5408\u9810\u671f~~     ~~- \u9084\u884c~~ 4. ~~x, y \u2192 holder \u5bec\u5ea6\u662f\u5426\u7b26\u5408\u9810\u671f~~     ~~- source holder \u7b26\u5408~~     ~~- detector holder \u7b26\u5408~~ 5. ~~source, detector \u662f\u5426\u6709\u64fa\u5728\u6b63\u78ba\u4f4d\u7f6e~~     ~~- fiber \u4f4d\u7f6e\u6b63\u78ba~~\u3001detector \u9700\u518d\u78ba\u8a8d     ~~- source window \u4f4d\u7f6e\u8207\u5927\u5c0f\u6b63\u78ba~~\uff0csource \u4f4d\u7f6e\u672c\u8eab\u9700\u518d\u78ba\u8a8d 6. python \u8207 matlab \u7684 xyz \u65b9\u5411</p> <p>\u6a21\u64ec 1. interior boundary \u2192 \u53cd\u5c04 2. exterior boundary \u2192 \u4e0d\u53cd\u5c04</p> <p>\u5f71\u50cf 1. skin, fat \u5340\u5206 2. \u6c7a\u5b9a\u8981\u7528\u6211\u7684\u9084\u662f\u6615\u539f\u7684\u5f71\u50cf</p>"},{"location":"handover/eric/IJV_Notebook/#20211013","title":"2021/10/13","text":"<ol> <li>copy \u4e00\u4efd mcx.py \u53bb\u6539</li> <li>\u6539 postprocess.py \u4e2d\u7684 detectorNum \u2192 \u76f4\u63a5 x 3 x 2\uff0c\u4f46\u53ef\u80fd\u9700\u8981\u6539\uff0c\u8207 reflectance.shape[1]</li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20211012","title":"2021/10/12","text":"<ol> <li>Boundary condition check<ul> <li>\u6539\u8b8a musp \u904b\u7b97\u7684\u57fa\u5e95\u6ce2\u9577 \u2192 \u76ee\u524d\u66ab\u5b9a 800nm (mcx.py function\u66f4\u52d5)</li> <li>\u5c07 skin \u7684 g \u6539\u70ba 0.9</li> <li>specular \u4e0d\u7528\u53bb\u6539\uff0c\u539f\u672c\u9810\u8a2d\u5c31\u6703\u8003\u616e\u9032\u53bb(\u8003\u616e specular reflection) ---&gt; \u53ef\u662f\u5982\u679c\u6709\u8003\u616e\u9032\u53bb\uff0cinitial weight \u61c9\u8a72\u4e0d\u8981\u662f 1 ??</li> <li>\u9700\u66f4\u65b0 postprocess \u4e2d\u7684 pathlength \u53d6\u5f97</li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20211004-training","title":"2021/10/04 - Training","text":"<p>\u5148\u524d\u5b78\u9577\u8a13\u7df4\u8cc7\u6599\u96c6 1. \u5b50\u4f73     - \u505a\u8ff4\u6b78\u4ee5\u6e1b\u5c11\u6a21\u64ec\u6642\u9593     - \u76ee\u6a19\u6ce2\u9577\uff1a700-900 nm     - \u8cc7\u6599\u7b46\u6578\uff1a5808(\u6563\u5c04) * 3000(\u5438\u6536) = 17424000 (\u4e00\u5343\u4e03\u767e\u591a\u842c\u7b46)     - \u8cc7\u6599\u5283\u5206\uff1atraining \u2192 75%\u3001validation \u2192 10%\u3001testing \u2192 15% 2. \u662f\u6f82     - \u653e\u5927 detector radius \u4ee5\u6e1b\u5c11\u6a21\u64ec\u6642\u9593     - \u8cc7\u6599\u7b46\u6578\uff1a48700 (\u56db\u842c\u591a\u7b46)     - \u8cc7\u6599\u5283\u5206\uff1atraining \u2192 98%(47700)\u3001testing \u2192 2%(1000) 3. \u6211     - \u76ee\u524d\u7121\u653e\u5927 detector</p>"},{"location":"handover/eric/IJV_Notebook/#20210901-","title":"2021/09/01 - \u9a57\u8b49\u6a21\u64ec\u6b63\u78ba\u6027(\u8207\u662f\u6f82\u6bd4\u8f03)","text":"<p>\u76ee\u6a19\uff1a\u61c9\u8a72\u662f\u9a57\u8b49 code \u662f\u5426\u6b63\u78ba\u5c31\u884c(\u7a1c\u93e1\u2026\u7b49\u7b49\u7684\u5f71\u97ff\u56b4\u683c\u4e0a\u4e0d\u5305\u542b\u5728\u9019\u88e1) \u524d\u8a00\uff1a\u662f\u6f82\u7684 wmc \u6709\u62ff\u53bb\u8207\u771f\u6b63\u7684 mc \u505a\u9a57\u8b49\uff0c\u7d50\u679c\u662f\u7a69\u5408\u7684\u3002(\u6211\u60f3\u95dc\u9375\u662f pathlength \u7684\u63d0\u53d6\uff0c\u53ea\u8981\u5c64\u6578\u8207\u55ae\u4f4d\u6b63\u78ba\uff0c\u61c9\u8a72\u5c31\u6c92\u554f\u984c) 1. \u5148\u78ba\u8a8d mcx_fanqq.mch \u7684 pathlength \u4f4d\u65bc\u54ea\u5e7e\u500b column (\u8207 mcx_toast.mch \u505a\u6bd4\u8f03)\uff0c\u627e\u51fa\u4f86\u5f8c\u9032\u884c\u6a19\u8a18 2. \u4f7f\u7528 mcxlab \u4e2d\u7684 loadmch.m \u9032\u884c mch file \u7684\u8b80\u53d6\uff0c\u5167\u90e8\u6703\u5c07 pathlength \u7684\u55ae\u4f4d\u5f9e [grid] \u8f49\u6210 [mm] (\u662f\u6f82\u7684 load_mch \u4e5f\u6709\u505a\u6b64\u6b65\u9a5f)\u3002     - \u88dc\u5145\uff1aload mcx_fanqq.mch \u6642\u6709\u8f49\u63db\u55ae\u4f4d\uff0c\u4f46 load mcx_toast.mch \u6642\u6c92\u6709\u3002(\u731c\u6e2c\u539f\u56e0\u662f mcx \u7248\u672c\u4e0d\u540c) 4. \u5047\u5b9a mcx_fanqq.mch \u6a21\u64ec\u51fa\u7684\u7d50\u679c(pathlength\u8207dz)\u662f\u6b63\u78ba\u7684\u3002 5. \u4f7f\u7528 mcx_fanqq \u6a21\u64ec\uff0coutput \u51fa\u5169\u7a2e\u8cc7\u6599\u578b\u614b\uff0c\u4e26\u6bd4\u8f03\u5169\u8005\u7684\u4e0b\u65b9\u8cc7\u8a0a(\u76f8\u540c seed)\uff1a\u2713     - \u4ee5 loadmch.m \u8b80\u53d6 mcx_fanqq.mch \u6240\u5f97\u51fa\u7684 pathlength \u8207 dz     - \u4ee5 jd.load \u8b80\u53d6 mcx_fanqq.jdat \u6240\u5f97\u51fa\u7684 pathlength \u8207 dz 6. \u5982\u679c\u4e0a\u8ff0\u6bd4\u8f03\u76f8\u540c\uff0c\u5247\u6bd4\u8f03\u4ee5\u4e0b\u5169\u8005(\u76f8\u540c seed)\uff1a\u2713     - \u4ee5 mcx_fanqq \u6a21\u64ec\u5f97\u7684 mch (pathlength \u8207 dz)     - \u4ee5 mcx_syu \u6a21\u64ec\u5f97\u7684 jdata (pathlength \u8207 dz) 7. \u5be6\u969b\u53bb\u8a08\u7b97 reflectance\uff0c\u770b\u662f\u6f82\u7684\u7b97\u6cd5\u8207\u6211\u7684\u7b97\u6cd5\u662f\u5426\u4e00\u6a23\uff1a     \u6bd4\u8f03\u5c0d\u8c61     - \u4ee5 mcx_fanqq \u6a21\u64ec\u5f97\u7684 mch     - \u4ee5 mcx_syu \u6a21\u64ec\u5f97\u7684 jdata</p> <p>==\u5f85\u8fa6== \u8981\u628a\u9019\u6bb5\u6539\u597d\uff1a </p> <p>==\u5148\u9a57\u8b49 jdata \u7684 reflectance \u8a08\u7b97\u53ef\u884c\u518d\u7e7c\u7e8c== 1. \u6bd4\u8f03\u662f\u6f82 loadmch \u8207\u6211 load jdata \u4e4b\u5f8c\u7684 code 2. \u6bd4\u8f03 loadmch \u5167\u7684\u5167\u5bb9(\u662f\u6f82-oldmcx\u3001mcxlab\u3001\u6211-newmcx) 3. mcxlab loadmch.m 4. \u4e0d\u4e00\u5b9a\u8981\u6bd4\u8f03\u771f\u6b63\u7684 reflectance \u548c\u7d93 wmc \u7684 reflectance\u3002\u4f46\u53ef\u4ee5\u6bd4\u8f03 loadmch \u5f8c\u548c jdata \u7684\u5167\u5bb9\uff0c\u4ee5\u53ca\u5f8c\u7e8c\u8a08\u7b97 reflectance \u7684 code\u3002(\u4e5f\u53ef\u4ee5\u6bd4\u8f03,\u6ce8\u610f total diffuse reflectance \u2192 per voxel ??) 5. \u6bd4\u8f03 wmc \u8207 \u771f\u6b63 mc \u7684 weight 6. normalize\u3001specular\u3001skipradius\u3001initial weight 7. </p> <p>\u6a21\u64ec\u74b0\u5883\u5dee\u7570\u6574\u7406\uff1a 1. 2021 \u8207 2018 \u7528\u76f8\u540c seed \u8dd1\uff0c\u7d50\u679c\u6703\u4e0d\u540c 2. 2021 \u8dd1\u7684 mch \u6c92\u8fa6\u6cd5\u7528\u662f\u6f82\u5beb\u7684 load_mch \u8b80\u53d6</p> <p>\u6e2c\u8a66\u76ee\u7684\uff1a \u70ba\u4e86\u6bd4\u8f03\uff0c\u9700\u5118\u91cf\u9084\u539f\u7576\u6642\u662f\u6f82\u6a21\u64ec\u7684\u74b0\u5883\uff0c\u4e5f\u65bc\u6b64\u6703\u72a7\u7272 seed\u3002</p> <p>\u6e2c\u8a66\u65b9\u5f0f\uff1a \u9084\u539f\u7576\u6642\u662f\u6f82\u6a21\u64ec\u7684\u74b0\u5883\uff0c\u518d\u8207\u73fe\u884c\u7684\u7248\u672c\u6bd4\u8f03(\u6539\u904e\u7684 cuda code\uff0c\u4e0d\u540c\u7684\u786c\u9ad4\uff0c\u4e0d\u540c\u7684\u64fa\u8a2d\uff0c\u4e0d\u540c\u7684 output \u6a94\u6848\u683c\u5f0f)</p> <p>\u6e2c\u8a66\u6d41\u7a0b \u539f\u59cb\u6a21\u578b\uff1a\u50c5\u542b cone source, detector 1. \u6e2c\u73fe\u884c\u4f7f\u7528\u7248\u672c mcx \u7684\u6b63\u78ba\u6027(reflectance \u6578\u91cf\u7d1a\u4e5f\u61c9\u8a72 1e-10 \u5de6\u53f3)     \u4f7f\u7528\u76f8\u540c seed     - \u662f\u6f82\uff1a\u539f\u59cb\u6a21\u578b     - \u65b0\u7248\uff1a\u539f\u59cb\u6a21\u578b 2. \u6e2c\u5149\u6e90\u5f71\u97ff     - \u662f\u6f82\uff1a\u539f\u59cb\u6a21\u578b     - \u65b0\u7248\uff1a\u539f\u59cb\u6a21\u578b but \u5149\u6e90\u6539\u70ba LED 3. \u6e2c holder \u5f71\u97ff     - \u662f\u6f82\uff1a\u539f\u59cb\u6a21\u578b     - \u65b0\u7248\uff1a\u539f\u59cb\u6a21\u578b but \u5149\u6e90\u6539\u70ba LED\u3001probe \u52a0\u4e0a holder(detector \u4e00\u6a23\u8cbc\u65bc\u8868\u9762\uff0c\u4e5f\u56e0\u6b64 detector \u7684\u4f4d\u7f6e\u9700\u6709\u4e00\u5c64\u7a7a\u6c23\u67f1) 4. \u6e2c\u7a1c\u93e1\u5f71\u97ff     - \u662f\u6f82\uff1a\u539f\u59cb\u6a21\u578b     - \u65b0\u7248\uff1a\u539f\u59cb\u6a21\u578b but \u5149\u6e90\u6539\u70ba LED\u3001probe \u52a0\u4e0a holder(detector \u4e0d\u8cbc\u65bc\u8868\u9762)\u3001\u52a0\u4e0a\u7a1c\u93e1 **\u5099\u4f4f\uff1a\u8a18\u5f97\u662f\u6f82\u7684 na \u8f03\u5c0f (0.12)</p>"},{"location":"handover/eric/IJV_Notebook/#20210827","title":"2021/08/27","text":"<ol> <li>\u6574\u7406\u76ee\u524d\u7d50\u679c <ul> <li>\u6700\u65b0 reflectance mean &amp; cv</li> <li>cv variation</li> <li>reflection coefficient analysis</li> <li>\u56de\u5831\u5b50\u4f73\u3001\u662f\u6f82 reflectance \u7684\u6578\u91cf\u7d1a\u8207\u6240\u9700\u5149\u5b50\u6578</li> </ul> </li> <li>\u5f8c\u7e8c\u898f\u5283<ul> <li>\u6a21\u64ec\u6ce2\u9577</li> <li>\u627e\u6700\u4f73\u6a21\u578b\u5927\u5c0f</li> <li>\u5c0d\u7a31\u6a21\u64ec\u3001\u591a\u500b sds \u540c\u6642\u6a21\u64ec\u3001detector fiber \u76f4\u5f91\u653e\u5927</li> <li>regression \u4fee\u6b63\u7684\u4e8b\u7269<ol> <li>\u5927 NA to \u5c0f NA</li> <li>\u534a\u5f91</li> <li>\u7a1c\u93e1</li> <li>\u5ef6\u9577\u7684\u7a1c\u93e1\u9020\u6210\u7684\u53cd\u5c04\u7387\u5dee\u7570</li> </ol> </li> <li>\u6821\u6b63\u4eff\u9ad4\u4fee\u6b63\u7684\u4e8b\u7269(\u6a21\u64ec\u5149\u8b5c\u8207\u5be6\u9a57\u5149\u8b5c\u4e4b\u9593\u7684\u95dc\u4fc2\u5f0f)<ol> <li>sensor \u97ff\u61c9</li> </ol> </li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210819-","title":"2021/08/19 - \u6e1b\u5c11\u6a21\u64ec\u6642\u9593\u65b9\u6cd5/","text":"<ul> <li>\u6e1b\u5c11\u6240\u9700\u5149\u5b50\u6578<ol> <li>\u5927\u6578\u503c\u5b54\u5f91 ==(\u5b50\u4f73)==</li> <li>\u5c0d\u7a31 detector holder</li> <li>\u589e\u52a0 detector \u9762\u7a4d</li> </ol> </li> <li>\u63d0\u5347\u6a21\u64ec\u901f\u5ea6<ol> <li>\u7e2e\u5c0f\u6a21\u578b\u5c3a\u540b(\u672c\u7814\u7a76\u70ba\u958b\u653e\u6a21\u578b\uff0c\u8207\u5927\u8166\u7684\u5c01\u9589\u6a21\u578b\u4e0d\u540c)</li> </ol> </li> </ul>"},{"location":"handover/eric/IJV_Notebook/#20210807-","title":"2021/08/07 - \u6a21\u64ec\u5f85\u8fa6","text":"<ul> <li>For sds \u78ba\u8a8d<ol> <li>~~\u78ba\u8a8d source \u70ba\u4f55\u5728\u76ae\u819a\u4e0a\u4e00\u5c64\u4e5f\u6709\u5438\u6536(ignore first)~~</li> <li>~~\u5b8c\u6210 pattern \u5206\u4f48\u78ba\u8a8d function~~</li> <li>~~\u5b8c\u6210 reflectance \u8a08\u7b97 function~~ ==(previous) \u2191==</li> <li>~~improve the function for making mcxInputForPreview.~~</li> <li>~~\u6aa2\u67e5\u6a21\u64ec\u6578\u91cf\u662f\u5426\u70ba 10 \u7684\u57fa\u6578~~</li> <li>~~cv.max()\u3001show reflectanceMean~~ ==(8/11) \u2191==</li> <li>run mcx in windows</li> <li>~~++github pull++~~</li> <li>\u4f7f\u7528\u65b0\u7684 ssd\uff0c\u88dd linux\uff0c\u65bc 3070 \u4e2d\u8dd1</li> <li>~~\u8a08\u7b97 cv \u4e0b\u964d\u8da8\u52e2 (\u61c9\u8a72\u5149\u5b50\u6578\u6709\u95dc)~~</li> <li>\u8a08\u7b97 reflectance \u6642\uff0c\u8981\u4ee5id\u6392\u5e8foutput\uff0c\u4e00\u500b\u662f\u522a\u6e1b\u6642\u6392\uff0c\u4e00\u500b\u662f\u5408\u4f75\u524d\u6392</li> <li>\u5982\u679c\u5df2\u6709\u8a72\u6ce2\u9577\u7684 mcxInput\uff0c\u5c31\u4e0d\u7528\u518d\u505a</li> <li>check jdata information in MCX google group (mcx cloud paper, jdata specification in github)</li> <li>\u770b\u5b50\u4f73\u662f\u6f82\u4e4b\u524d\u6a21\u64ec\u7684\u6578\u91cf\u7d1a\u3001\u4ee5\u53ca\u539f\u56e0<ul> <li>\u5b50\u4f73<ol> <li>\u5728\u505a sds \u9748\u654f\u5ea6\u5206\u6790\u6642\u662f\u4ee5 1e11 \u7684\u5149\u5b50\u6578\u6a21\u64ec</li> <li></li> </ol> </li> <li>\u662f\u6f82</li> </ul> </li> <li>make sure \"air\" is outside the \"Grid\"<ul> <li>validate by comparing two cases based on same seed.</li> </ul> </li> <li>\u756b\u76ee\u524d\u7a0b\u5f0f\u7684\u67b6\u69cb\u5716</li> <li>reading of mcxInput in two functions of postprocess.py (for wavelength)</li> <li>add wl id to mcxoutput</li> <li>make wavelength, detector_na be an input (maybe in config)</li> </ol> </li> <li>For sds sensitivity analysis &amp; training<ol> <li>\u6574\u7406\u6615\u539f\u6563\u5c04\u3001\u5438\u6536\uff0c\u6a21\u64ec\u53c3\u6578\u78ba\u8a8d</li> <li>\u6c7a\u5b9a\u6a21\u64ec\u7684\u6ce2\u9577</li> <li>n, g \u8a62\u554f\u8001\u5e2b</li> <li>\u8d85\u97f3\u6ce2\u5e7e\u4f55\u7d50\u69cb\u64f7\u53d6 (image \u2192 grid)</li> <li>==training \u524d\u53ef\u5148\u627e\u4e00\u689d\u5e73\u6ed1\u7684 curve ?==</li> </ol> </li> <li>For fitting<ol> <li>add tissue composition</li> <li>\u8840\u7d05\u7d20\u6fc3\u5ea6\u4e5f\u9700\u8981 fit ?</li> <li>\u6574\u7406\u8981 fit \u7684\u751f\u7406\u7269\u8cea(\u53c3\u6578\u7a2e\u985e)</li> </ol> </li> <li>Keep doing<ul> <li>do modularization, add design pattern</li> <li>plot structure of the code</li> </ul> </li> </ul>"},{"location":"handover/eric/IJV_Notebook/#20210709-","title":"2021/07/09 - \u6a21\u64ec\u67b6\u69cb\u898f\u5283","text":"<p>\u9023\u7d50</p>"},{"location":"handover/eric/IJV_Notebook/#20210707-source-simulation-6","title":"2021/07/07 - source simulation \u9a57\u8b49 6","text":"<p>\u5169\u7a2e\u6bd4\u8f03\u65b9\u5f0f 1. \u6bd4\u8f03\u4e0d\u540c\u89d2\u5ea6\u7684\u76f4\u7dda\uff0c\u770b\u5be6\u9a57\u8207mcx\u7684\u8da8\u52e2\u662f\u5426\u76f8\u540c     - \u5be6\u4f5c         - \u9078\u64c7\u89d2\u5ea6\u3001\u534a\u5f91\u9577\u5ea6 \u2192 r         - \u5c07 -r ~ r \u5206\u6210 n \u500b\u7b49\u5206         - \u53d6\u5f97\u9019 n \u500b\u7b49\u5206\u6240\u5c0d\u61c9\u5230\u7684 (x,y)         - \u53d6\u5f97\u6bcf\u4e00\u500b (x,y) \u6240\u5c0d\u61c9\u7684 pixel \u7684 gray value         - \u756b\u8b8a\u5316\u8da8\u52e2     - \u4f8b\u5b50          </p> <ol> <li>\u6bd4\u8f03\u76f8\u540c radial distance \u7684\u5e73\u5747 gray value<ul> <li>\u524d\u63d0\uff1f<ul> <li>\u4e2d\u5fc3\u8981\u6293\u597d\uff1f</li> <li>\u5f71\u50cf\u4e00\u5b9a\u8981\u662f\u8f3b\u5c04\u5c0d\u7a31\uff1f==\u597d\u50cf\u4e5f\u4e0d\u7528!==(\u96d6\u7136\u5149\u6e90\u662f\u77e9\u5f62\uff0c\u4f46\u4f3c\u4e4e\u662f\u8ddd\u96e2\u7b97\u9060\uff0c\u6240\u4ee5\u7167\u5c04\u4e0b\u4f86\u7684\u5f37\u5ea6\u5206\u4f48\u50cf\u5713\u5f62)</li> </ul> </li> <li>\u5be6\u4f5c<ul> <li>\u9078\u64c7\u5f91\u5411\u8ddd\u96e2(radial distance)</li> <li>\u5c07 0~360^o^ \u5206\u6210 n \u7b49\u5206</li> <li>\u53d6\u5f97\u9019 n \u500b\u89d2\u5ea6\u6240\u5c0d\u61c9\u5230\u7684 (x,y)</li> <li>\u53d6\u5f97\u6bcf\u4e00\u500b (x,y) \u6240\u5c0d\u61c9\u7684 pixel \u7684 gray value</li> <li>\u8a08\u7b97\u9019 n \u500b gray value \u7684\u5e73\u5747 \u2192 \u5f97\u6b64\u5f91\u5411\u8ddd\u96e2\u7684 gray value</li> </ul> </li> <li>\u4f8b\u5b50      </li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210705-source-simulation-5","title":"2021/07/05 - source simulation \u9a57\u8b49 5","text":"<p>\u5be6\u9a57\u62cd\u651d\u7d50\u679c\u6bd4\u8f03(use pillow to read image) 1. 20210607\uff1a\u6700\u5f31\u5f37\u5ea6\u5f71\u50cf \u2192 blue channel \u904e\u66dd\uff0c\u6574\u9ad4\u7070\u968e\u503c\u6c92\u6709 2. 20210624\uff1a\u6700\u5f31\u5f37\u5ea6\u5f71\u50cf \u2192 blue channel \u904e\u66dd\uff0c\u6574\u9ad4\u7070\u968e\u503c\u6c92\u6709 3. 20210630\uff1aled\u7167\u5149\u7684\u5340\u57df\u5747\u6c92\u6709\u904e\u66dd\u7684\u73fe\u8c61     - EX:      - \u627e\u51fa\u4e2d\u5fc3\u9ede     - \u756b\u8da8\u52e2\u7dda</p>"},{"location":"handover/eric/IJV_Notebook/#20210526-source-simulation-4","title":"2021/05/26 - source simulation \u9a57\u8b49 4","text":"<p>LED pattern \u672c\u8eab\u4f3c\u4e4e\u6709\u4e00\u9ede\u9ede\u5fae\u53f3\u504f\uff0c\u4f46\u770b\u4e0d\u51fa\u4f86\uff0c\u61c9\u8a72 data sheet \u63d0\u4f9b\u7684\u8cc7\u6599\u672c\u8eab\u9020\u6210\uff0c\u56e0\u6b64\u9019\u88e1\u5ffd\u7565\u3002 1. \u671f\u671b\u5206\u4f48\u8a08\u7b97     - \u8aa4\u5dee\u4f86\u6e90         - \u5be6\u969b\u4e0a\u6bcf\u4e00\u500b grid \u7684 angle_interval \u4e0d\u4e00\u6a23 (\u8d8a\u5916\u570d\u7684 angle_interval \u61c9\u8a72\u6703\u8d8a\u5c0f\uff0c\u4f46\u76ee\u524d\u5047\u8a2d\u4e00\u6a23) ++\u2192 \u505a\u4fee\u6b63++         - \u5be6\u969b\u4e0a\u6bcf\u4e00\u500b grid \u6240\u9700\u7684\u5713\u74b0\u5927\u5c0f\u4e0d\u4e00\u6a23 (\u8d8a\u63a5\u8fd1\u5c0d\u89d2\u7dda\u7684 grid \u6240\u9700\u7684\u5713\u74b0\u61c9\u8a72\u8981\u8d8a\u5927\uff0c\u4f46\u76ee\u524d\u5047\u8a2d\u4e00\u6a23) 3. mcx \u6a21\u64ec     - \u6700\u5916\u74b0\u7279\u5225\u5c0f 5. \u5be6\u9a57</p>"},{"location":"handover/eric/IJV_Notebook/#20210505-source-simulation-3","title":"2021/05/05 - source simulation \u9a57\u8b49 3","text":"<p>==\u91cd\u65b0\u6aa2\u67e5\u4e00\u6b21 mcx_core.cu, mcx_utils.c== - mcx_core.cu         - \u65b0\u589e radiated window \u53ef\u4ee5\u662f\u5713\u5f62\u7684\u72c0\u6cc1         - angle pattern array \u9700\u70ba radians         - \u9700\u6aa2\u67e5 code \u6709\u7121\u932f\u8aa4\u4ee5\u53ca ifelse \u5224\u65b7\u662f\u5426\u6709\u6548\u7387\u3002\u4ee5\u53ca if \u5167\u80fd\u4e0d\u80fd\u76f4\u63a5\u653e float\u3002\u78ba\u8a8d srcparam1, srcparam2 \u7684 float \u7684\u9806\u5e8f\u662f x, y, z, w.\u3002angle array \u8981\u6709 10001 \u500b\u55ce\uff1f\u78ba\u8a8d\u5149\u5b50 p \u7684\u8d77\u59cb\u4f4d\u7f6e\u8207x, y, z, w         - \u4e4b\u524d\u8dd1\u932f\u4e86!!!!!!! srcparam1.x \u61c9\u8a72\u8981\u653e\u6578\u5b57\uff0c\u4e0d\u80fd\u653e array !!!         - \u61c9\u8a72\u8981\u65b0\u589e\u4e00\u500b src.pattern !!!         - origin type \u8207 source \u4f4d\u7f6e\u7684\u95dc\u4fc2         - \u4e0d\u77e5\u70ba\u4f55 p \u9700\u8981\u6bd4 0 \u5c0f\u4e00\u9ede\u9ede\u624d\u80fd\u8b93\u5149\u5b50\u5728\u4e0d\u540c\u7684\u4f4d\u7f6e\u767c\u5c04\uff0c\u4f8b\u5982 -0.0001\u3002         - \u4e0d\u77e5\u9053\u70ba\u4ec0\u9ebc sizeof(srcpattern[0]) \u4e00\u76f4\u90fd\u662f 0.00000 ...QQ - mcx_utils.c     - \u78ba\u8a8d\u6574\u9ad4 code \u7684\u4fee\u6539\u662f\u5426\u80fd match \u5b50\u4f73\u7684 mcxlab.cpp     - \u78ba\u8a8d code \u6709\u7121\u932f\u8aa4</p>"},{"location":"handover/eric/IJV_Notebook/#20210503-source-simulation-2","title":"2021/05/03 - source simulation \u9a57\u8b49 2","text":"<p>==\u78ba\u8a8d\u9a57\u8b49\u9700\u8981\u7528\u7684 mua==</p> <p>\u6e2c\u8a66 mua\u3002.py \u6a94\u4f4d\u65bc <code>Desktop/ijv_2</code></p> <ol> <li>\u64b0\u5beb <code>determine_validationMUA.py</code>\uff0c\u5167\u542b step size \u7684 pdf, cdf, quantile, get_properCoeff ... \u7b49 function\u3002</li> <li>MCX \u672c\u8eab\u61c9\u8a72\u662f\u8dd1 WMC\uff0c\u56e0\u6b64\u9700\u8981\u7528 WMC \u7684\u89d2\u5ea6\u53bb\u601d\u8003\u3002(\u5148\u6839\u64da mus sampling step size\uff0c\u518d\u7528 mua \u53bb\u8a08\u7b97\u5149\u5b50\u6cbf\u8457 trajectory \u884c\u8d70\u6642\u88ab\u5438\u6536\u7684\u80fd\u91cf)</li> <li>\u7d50\u679c\uff1a     \u8a08\u7b97\u51fa\u4e0d\u540c\u7684 traveling pathlength \u4e0b, \u9700\u8981\u591a\u5c11\u7684 mua \u624d\u80fd\u8b93 99.9% \u7684 photon weight \u88ab\u5438\u6536\u3002(99.9% \u70ba\u53ef\u8abf\uff0ctraveling pathlength \u4e5f\u662f\u53ef\u8abf)</li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210426-source-simulation-1","title":"2021/04/26 - source simulation \u9a57\u8b49 1","text":"<p>==\u78ba\u8a8d\u53ef\u8dd1== - <code>0426</code>      1. \u65b0\u589e inverse_cdf \u7684 script\uff0c\u4e26\u88fd\u4f5c\u65b0\u7684 input_addLED.json\uff0c\u5167\u653e sourcepattern_array \u8207\u76f8\u95dc\u7684 srcparam\u3002     2. \u8aa4\u4ee5\u70ba\u53ef\u8dd1\uff0c\u4e0d\u904e\u7d93\u6e2c\u8a66\u5f8c\u5176\u5be6\u61c9\u8a72\u662f\u8dd1\u6210 default \u2192 src_type=\"pencil\"\u3002 - <code>0427</code>     1. \u521d\u6b65\u78ba\u8a8d mcx_utils.c \u8207 mcx_core.cu \u7684\u95dc\u9023\uff0c\u4e26\u65bc mcx_utils.c \u65b0\u589e new source type \u2192 \"anglepattern\"\u3002     2. \u76ee\u524d\u61c9\u8a72\u662f\u53ef\u8dd1\uff0c\u81f3\u5c11\u8dd1\u8d77\u4f86\u8ddf src_type = \"pencil\" \u6642\u4e0d\u4e00\u6a23 (detected photons \u7684\u6578\u76ee\u4e0d\u540c)\u3002 - <code>0428</code>     1. \u9806 mcx code\u3002     2. \u78ba\u8a8d mua \u8a2d\u6210 10000\uff0cmus \u8a2d\u6210 0 \u7684\u60c5\u6cc1\u4e0b\u662f\u53ef\u8dd1\u7684\uff0c\u4e0d\u904e\u9084\u4e0d\u6703 output .mc2 file\u3002 - <code>0429</code>     1. \u767c\u73fe .jdata \u8207 .mch \u61c9\u8a72\u53ef\u4ee5\u7b97\u662f equivalent \u7684\uff0c\u4e4b\u5f8c\u53ef\u4ee5\u8a66\u8457\u6ce8\u610f .jdata \u7684\u4f7f\u7528\uff0c\u56e0\u5176\u5bb9\u91cf\u5c0f(\u770b\u8d77\u4f86\u6709\u58d3\u7e2e)\uff0c\u4e14\u4f7f\u7528\u8d77\u4f86\u5341\u5206\u65b9\u4fbf\uff0c\u5c31\u50cf json data \u4e00\u6a23! (\u4e0d\u6703\u50cf\u4ee5\u5f80\u4e00\u6a23\u518d\u600e\u9ebc\u8a66\u90fd\u53ea\u6703\u8f38\u51fa Detected photon data !)     2. \u6210\u529f\u8a66\u8457\u6253\u958b \"\u8f38\u51fa Volumetric data --save2pt\" \u7684\u958b\u95dc\uff0c\u9019\u500b\u958b\u95dc\u6253\u958b\u4e4b\u5f8c\uff0coutputtype \u8a2d\u6210 energy density \u6216\u662f outputformat \u8a2d\u6210 .mc2 \u5c31\u6703\u6709\u4f5c\u7528\u4e86!     3. \u53e6\u5916 mcx \u6307\u4ee4\u7684 --savedet, --savedetflag, --saveexit\uff0c\u6709\u4e00\u4e9b\u8a2d\u5b9a\u6703\u4e92\u76f8\u95dc\u9023\u5f71\u97ff\uff0c\u8981\u6ce8\u610f! \u9019\u4e9b\u6307\u4ee4\u4e26\u6c92\u6709\u90a3\u9ebc\u7368\u7acb!     4. \u6210\u529f\u5927\u81f4\u5730\uff0c\u628a fluence_rate \u7684 jdata output \u8b80\u9032 spyder\uff0c\u4e26\u5229\u7528 plt.imshow() \u756b\u5716\uff0c\u4e0d\u904e\u50c5\u662f\u5927\u6982\uff0c\u756b\u7684\u5230\u5e95\u5c0d\u4e0d\u5c0d\uff0c\u9084\u8981\u518d\u78ba\u8a8d!     5. \u4f46\u4eca\u5929\u662f\u6709\u8a66\u8457\u628a mcx_input_file \u7684\u4e00\u4e9b\u6307\u4ee4\u95dc\u6389\uff0c\u4e4b\u5f8c\u53ef\u4ee5\u8a66\u8457\u53bb\u78ba\u8a8d mcx_command \u8207 mcx_inputFile\u5167\u7684\u8a2d\u5b9a \u4e4b\u9593\uff0c\u54ea\u4e00\u500b\u512a\u5148\u6b0a\u8f03\u9ad8!     6. \u767c\u73fe\u6a21\u64ec\u9084\u662f\u9700\u8981 mus\uff0c\u81f3\u5c11\u8981\u662f 1\uff0c\u4e0d\u7136 mcx \u4e0d\u80fd\u6b63\u5e38\u505a\u8f38\u51fa\u3002(\u770b\u8d77\u4f86 mcx \u662f\u6703\u5148\u8dd1 wmc\uff0c\u518d\u505a mua \u7684\u5438\u6536\u8a08\u7b97\u3002\u56e0\u70ba\u5be6\u969b\u4e0a\u5728\u6a21\u64ec\u7684\u6e2c\u8a66\u904e\u7a0b\u4e2d\u767c\u73fe\uff0c\u4e0d\u7ba1 mua \u8abf\u591a\u5c11\uff0c\u5728 random seed \u8a2d\u4e00\u6a23\u7684\u60c5\u6cc1\u4e0b\uff0cdetected photons \u90fd\u6703\u662f\u4e00\u6a23\u591a)</p>"},{"location":"handover/eric/IJV_Notebook/#20210426-source-simulation","title":"2021/04/26 - source simulation \u9a57\u8b49\u898f\u5283","text":""},{"location":"handover/eric/IJV_Notebook/#_1","title":"\u65b9\u6cd5","text":"<p>\u5229\u7528\u6a21\u64ec\u5438\u6536\u77e9\u9663\u7684\u65b9\u5f0f\uff0c\u5f37\u8feb\u5149\u5b50\u5747\u6703\u5728\u7b2c\u4e00\u500b grid \u4ee5\u5167\u5c31\u88ab\u5438\u6536\u3002\u7136\u5f8c\u8207\u4e4b\u524d ++\u91cf\u6e2c\u5f97\u5230\u7684++ &amp; ++\u6a21\u64ec\u5f97\u5230\u7684++ power distribution \u505a\u6bd4\u5c0d(\u53c3\u8003 0309, 0312, 0319 \u7684 slide)\u3002</p>"},{"location":"handover/eric/IJV_Notebook/#_2","title":"\u6d41\u7a0b","text":"<ol> <li>==\u78ba\u8a8d\u53ef\u8dd1==</li> <li>==\u78ba\u8a8d\u9a57\u8b49\u9700\u8981\u7528\u7684 mua==<ul> <li>\u5438\u6536 \u2192 \u975e\u5e38\u5927\u3001\u6563\u5c04 \u2192 ~~0~~ 1\u3002\u5f37\u8feb\u5149\u5b50\u5e7e\u4e4e\u65bc\u7b2c\u4e00\u500b\u683c\u9ede\u5c31\u88ab\u5438\u6536\u3002\u56e0\u6b64\u9700\u8a08\u7b97\u8def\u5f91\u9577\u7684 pdf\uff0c\u770b\u770b\u5149\u5b50\u7684\u8def\u5f91\u9577\u5206\u4f48\u70ba\u4f55\u3002\u53ef\u4ee5\u8a2d\u4e00\u500b threshold\uff0c\u4f8b\u5982 99.9% \u4ee5\u4e0a\u7684\u5149\u5b50\u7684\u7b2c\u4e00\u500b random walk \u90fd\u4e0d\u6703\u8d85\u51fa\u7b2c 1 \u500b grid\u3002(\u770b\u8d77\u4f86\u9700\u8003\u91cf\u5230\u5149\u5b50\u7684\u5165\u5c04\u89d2\u4ee5\u53ca grid \u7684\u908a\u9577)</li> </ul> </li> <li>==\u91cd\u65b0\u6aa2\u67e5\u4e00\u6b21 mcx_core.cu, mcx_utils.c==<ul> <li>mcx_core.cu<ul> <li>\u65b0\u589e radiated window \u53ef\u4ee5\u662f\u5713\u5f62\u7684\u72c0\u6cc1</li> <li>\u78ba\u8a8d code \u6709\u7121\u932f\u8aa4</li> </ul> </li> <li>mcx_utils.c<ul> <li>\u78ba\u8a8d\u6574\u9ad4 code \u7684\u4fee\u6539\u662f\u5426\u80fd match \u5b50\u4f73\u7684 mcxlab.cpp</li> <li>\u78ba\u8a8d code \u6709\u7121\u932f\u8aa4</li> </ul> </li> </ul> </li> <li>==\u91cd\u65b0\u6aa2\u67e5\u4e00\u6b21 find LED cdf \u7684 code==<ul> <li>check inverse \u7684\u6d41\u7a0b\u662f\u5426\u6b63\u78ba(\u5167\u63d2\u6c42\u51fa function ... etc)\u3002</li> <li>check code \u6709\u7121\u932f\u8aa4\u3002</li> </ul> </li> <li>==\u91cd\u65b0\u6aa2\u67e5\u4e00\u6b21 find_power_distribution \u7684 code==<ul> <li>check \u6703\u9664\u4ee5 0 \u7684\u72c0\u6cc1\uff0c\u4e26\u6392\u9664\u3002</li> <li>check code \u6709\u7121\u932f\u8aa4\u3002</li> <li>\u659c\u5411\u5165\u5c04\u7684 \\(\\cos{\\theta}\\) \uff1f\uff1f</li> </ul> </li> <li>==\u6700\u5f8c mcx core \u9a57\u8b49==<ul> <li>\u76f4\u63a5\u6a21\u64ec\u5149\u5b50\u65bc\u7d44\u7e54\u8868\u9762\u7684\u5438\u6536\u77e9\u9663\u3002<ul> <li>\u6b64\u65b9\u6cd5\u7b49\u540c\u65bc\u7528\u4e00\u7a2e\u8fd1\u4f3c\u7684\u65b9\u6cd5\u89c0\u770b\u5149\u5b50\u65bc\u7d44\u7e54\u8868\u9762\u7684\u5206\u4f48(\u659c\u5411\u5165\u5c04\u7684 \\(\\cos{\\theta}\\) \uff1f\uff1f)</li> </ul> </li> <li>\u5c07++\u5438\u6536\u77e9\u9663++\u8207\u5148\u524d++\u5be6\u9a57\u91cf\u6e2c\u5230\u7684++\u8207++\u7c21\u55ae\u96fb\u8166\u8a08\u7b97\u5f97\u5230\u7684++ power distribution on radiated window \u4f5c\u6bd4\u8f03\u3002</li> <li>\u9700\u78ba\u8a8d source parameter \u7684\u55ae\u4f4d\uff0cmm ?</li> <li>\u78ba\u8a8d source \u7684\u5ea7\u6a19\u7a76\u7adf\u662f\u5426\u80fd\u662f\u6d6e\u9ede\u6578\uff0c\u800c\u4e0d\u9700\u8981\u4ee5 grid \u70ba\u55ae\u4f4d\u3002</li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210423-mcx_corecu","title":"2021/04/23 - mcx_core.cu \u4fee\u6539\uff08\u7e8c\uff09","text":"<p>20210730 \u88dc\u5145\uff1a\u5b50\u4f73\u7576\u521d\u7684\u4fee\u6539\u8a18\u9304\u3002 1. \u8f09\u4e86\u4e00\u500b \u984d\u5916\u7684 package \u5f8c(\u8a18\u5f97\u662f zlib)\uff0c\u6210\u529f compile \u6210 mcx bin\u3002 2. kb's comment on meeting     - fitting \u7684\u53c3\u6578 \u2192 \u8840\u7d05\u7d20\u6fc3\u5ea6\u3001epsilon \u8981\u53ef\u4ee5 fit\uff0c\u56e0\u70ba\u4e0d\u540c\u4eba\u7684\u8840\u6db2\u672c\u8cea\u53ef\u80fd\u4e0d\u4e00\u6a23     - \u8981\u8003\u616e\u5230\u7d44\u7e54\u8868\u9762\u7684\u6536\u5149\u9762\u7a4d\uff0c\u5982\u679c\u6536\u5149\u9762\u7a4d\u8d8a\u5927\uff0c\u6a21\u64ec\u9700\u8981\u7684\u5149\u5b50\u6578\u53ef\u80fd\u5c31\u8d8a\u591a\uff0c\u4e14\u7a7a\u9593\u7684\u8cc7\u8a0a\u6703\u6d41\u5931\u3002     - \u786c\u9ad4\u7684\u6750\u6599\u53ef\u8003\u616e\u8981\u7528\u54ea\u4e00\u7a2e\u3002     - \u659c\u5411\u5165\u5c04\uff1a\\(\\cos{\\theta}\\) \uff1f\uff1f</p>"},{"location":"handover/eric/IJV_Notebook/#20210421-mcx_corecu","title":"2021/04/21 - mcx_core.cu \u4fee\u6539\uff08\u7e8c\uff09","text":"<ol> <li>\u70ba\u4ec0\u9ebc\u9700\u8981     <code>c++     CUDA_ASSERT(cudaMalloc((void **) &amp;gsrcpattern, sizeof(float)*(int)(cfg-&gt;srcparam1.x+1)));     CUDA_ASSERT(cudaMemcpy(gsrcpattern,cfg-&gt;srcpattern,sizeof(float)*(int)(cfg-&gt;srcparam1.x+1), cudaMemcpyHostToDevice));</code><ul> <li>gsrcpattern \u662f\uff1f</li> <li>cfg-&gt;srcparam1.x+1 \u662f\uff1f</li> </ul> </li> <li>Main Flow for launching new photon<ul> <li>zenith angle \u2192 \\(\\theta\\) (\u5149\u5b50\u5f9e LED \u5c04\u51fa\u5f8c\u8207 Z \u7684\u593e\u89d2, distributed according to the pdf of radiation pattern)</li> <li>azimuth \u2192 \\(\\phi\\) (\u65b9\u4f4d\u89d2\uff0c\u5e73\u9762\u89d2\uff0cuniform distributed)</li> <li>Purpose: Determine whether photon is launched into a legal window. If yes, then update photon position onto the surface being the initial position to start mcx simulation. (\u4f46\u4e3b\u8981\u7684\u5149\u5b50\u65b9\u5411\u5411\u91cf\u6c92\u6709\u65cb\u8f49\uff0c\u9019\u662f\u4ea4\u7d66 mcx \u5167\u90e8\u7684 rotatevector() \u81ea\u884c\u53bb\u65cb\u8f49\u7684\uff0c\u6211\u5011\u505a\u7684\u53ea\u662f\u66f4\u65b0\u5149\u5b50\u7684\u4f4d\u7f6e\u5230\u7d44\u7e54\u8868\u9762)</li> <li>The following flow chart is translated to code in launchnewphoton() function in mcx_core.cu.</li> </ul> </li> </ol> <pre><code>st=&gt;start: Start\ne=&gt;end: End\nop1=&gt;operation: Sampling azimuth, zenith angle\nop2=&gt;operation: Sampling launch position\nuniformly in rectangle area\nop3=&gt;operation: Sampling azimuth, zenith angle\nop4=&gt;operation: Update photon position\nop5=&gt;operation: Sampling launch position\nuniformly in circular area\nop6=&gt;operation: Sampling azimuth, zenith angle\nop7=&gt;operation: Use the legal launching angle\nto update stheta, ctheta. And\nthen formally rotate the \ndirection vector of the photon\ncond1=&gt;condition: LED panel is\nrectangular ?|future\ncond2=&gt;condition: Will be launched to\nradiated window ?\ncond3=&gt;condition: Will be launched to\nradiated window ?\n\nst-&gt;op1-&gt;cond1\ncond1(yes)-&gt;op2\ncond1(no)-&gt;op5-&gt;cond3\ncond3(yes)-&gt;op4\ncond3(no)-&gt;op6-&gt;op5\nop2-&gt;cond2\ncond2(yes)-&gt;op4-&gt;op7-&gt;e\ncond2(no)-&gt;op3-&gt;op2\n</code></pre> <ol> <li>\u5f85\u8fa6<ul> <li>\u4fee\u6539\u597d code</li> <li>\u9a57\u8b49</li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210409-mcx_corecu","title":"2021/04/09 - mcx_core.cu \u4fee\u6539","text":"<ul> <li>tab <code>else if(gcfg-&gt;srctype==MCX_SRC_ANGLEPATTERN)</code></li> <li><code>float4 srcparam1</code>\u3001<code>float4 srcparam2</code><ul> <li>\u78ba\u8a8d\u5b50\u4f73\u8a2d\u5b9a\u7684 <code>srcparam1.x</code> \u662f\u4ec0\u9ebc (length of angle array ?)</li> <li>\u8207\u5b50\u4f73\u8a0e\u8ad6\u5f8c\u5c0d source \u53c3\u6578\u7684\u60f3\u6cd5, \u53ef\u4ee5==\u8996\u6574\u500b holder \u70ba source==, \u65b0\u589e \u2193<ol> <li>led_x, led_y, led_r</li> <li>win_x, win_y, win_r, led2win</li> </ol> </li> <li>\u73fe\u884c source \u7684\u9810\u5b9a\u8a2d\u5b9a\u53c3\u6578     <code>python     srctype: MCX_SRC_ANGLEPATTERN     srcpattern: sampled angle array     srcparam1: {length of angle array (default=10000),                  led_x,                  led_y,                  led_r}     srcparam2: {win_x,                  win_y,                  win_r,                  led2win}</code></li> </ul> </li> </ul>"},{"location":"handover/eric/IJV_Notebook/#20210408-mcx_corecu","title":"2021/04/08 - mcx_core.cu \u95b1\u8b80","text":"<p>\u95b1\u8b80\u8a18\u9304\u8acb\u53c3\u8003 \u2192 \u5167\u90e8\u6a21\u64ec / \u57fa\u672c\u6d41\u7a0b / mcx_core.cu</p>"},{"location":"handover/eric/IJV_Notebook/#20210326-mcx_corecu","title":"2021/03/26 - mcx_core.cu \u4fee\u6539\u65b9\u6cd5\u898f\u5283","text":"<p>03/26 meeting slide</p> <ol> <li>sampling \u5149\u5b50\u767c\u5c04\u4f4d\u7f6e (coordinate on LED panel)</li> <li>sampling \u5149\u5b50\u767c\u5c04\u89d2\u5ea6</li> <li>\u904b\u7b97\u3001\u5224\u65b7\u5149\u5b50\u662f\u5426\u843d\u65bc radiated window \u5167</li> <li>\u53c3\u6578\uff1a++\u5149\u6e90\u81f3\u7d44\u7e54\u8868\u9762\u7684\u8ddd\u96e2++\u3001++LED panel\u5927\u5c0f++\u3001++radiated window\u5927\u5c0f++</li> <li> <p>\u5149\u6e90\u8a2d\u7f6e\u793a\u610f\u5716\uff1a</p> <ul> <li> <p>3D </p> </li> <li> <p>2D </p> </li> <li>==Kb's comment:== </li> <li>\u9700\u8981\u6ce8\u610f radiated window \u7684\u53c3\u6578\uff0c\u8981\u53ef\u4ee5\u8b93\u5176\u4ed6 project \u4f7f\u7528\u7684\u8a71\uff0c\u9700\u8981\u53ef\u4ee5\u8a2d\u5b9a\u6210\u5713\u5f62\u3001\u65b9\u5f62\u2026\u7b49\u7b49\uff0c\u6240\u4ee5\u9700\u8981\u9577\u3001\u5bec\u3001\u534a\u5f91\u2026\u7b49\u7b49\u7684\u53c3\u6578</li> <li>\u53ef\u4ee5\u770b\u770b\u5176\u4ed6\u7684 source type \u6709\u6c92\u6709 sampling \u767c\u5c04\u4f4d\u7f6e\u76f8\u95dc\u7684 code\uff0c\u7136\u5f8c\u518d\u770b\u5982\u4f55 copy\u3001\u501f\u7528\u3001\u4fee\u6539\u3002</li> <li>filter \u5be6\u969b\u4e0a\u61c9\u8a72\u662f\u6bd4 window \u5927\uff0c\u6240\u4ee5 raidated window \u4ee5\u5be6\u969b\u4e0a tissue surface \u6703\u6536\u5230\u5149\u7684\u9762\u7a4d\u70ba\u4e3b\u5c31\u597d\u3002</li> <li>\u5149\u5728 filter \u5167\u7684\u6298\u5c04\u548c\u53cd\u5c04\u5c0d\u65bc detector reflectance \u7684\u5f71\u97ff\u4f4e\uff0c\u6240\u4ee5\u53ef\u4ee5\u5ffd\u7565\u3002</li> <li>\u4e4b\u5f8c\u53ef\u4ee5\u60f3\u60f3 detector \u7684\u500b\u6578(\u56e0\u61c9\u9700\u8981\u7684 resolution), \u56e0\u70ba\u5149\u7e96\u7684\u6578\u91cf\u8207\u6536\u5149\u534a\u5f91\u5927\u5c0f\u662f\u53ef\u4ee5\u6539\u8b8a\u7684\u3002</li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210321-source","title":"2021/03/21 - source \u91cf\u6e2c\u8207\u78ba\u8a8d","text":"<p>02/26 meeting slide \u2192 \u7b2c\u4e00\u6b21\u767c\u73fe\u6703\u6f0f\u5149 03/09 meeting slide \u2192 \u7b2c\u4e00\u6b21\u8acb\u7950\u7965\u7528\u76f8\u6a5f\u7167\uff0c\u4e26\u770b gray value \u8b8a\u5316\u8da8\u52e2\u3002\u53e6\u5916\u4e5f\u8a66\u8457\u7528\u986f\u5fae\u93e1\u89c0\u5bdf LED \u767c\u5149\u60c5\u5f62\u3002\u53e6\u5916\u4e5f\u8a66\u8457\u7528\u96fb\u8166\u6a21\u64ec LED \u7167\u5230 radiated plane \u61c9\u6709\u7684 power \u5206\u4f48\u3002 03/12 meeting slide \u2192 source \u5747\u52fb\u5ea6\u91cf\u6e2c\u3002 03/19 meeting slide \u2192 source \u5747\u52fb\u5ea6\u91cf\u6e2c(\u7e8c)\uff0c\u4e26\u5be6\u969b\u5617\u8a66\u91cf\u6e2c LED \u767c\u5149\u9762\u7a4d\u3002</p> <ol> <li>\u5f9e\u986f\u5fae\u93e1\u4e0b\u770b LED plate \u767c\u5149\u7684\u5340\u57df\u9762\u7a4d</li> <li>\u76f4\u63a5\u4ee5 LED \u5c0d\u76f8\u6a5f\u7684 sensor \u62cd\u651d\uff0c\u4e0a\u6b21\u662f\u6709\u52a0\u900f\u93e1(\u4f46\u61c9\u4e0d\u80fd\u52a0)</li> <li>\u5c07 sensor \u62cd\u651d\u5f97\u7684 profile \u8207\u6a21\u64ec\u5c0d\u7167</li> <li>\u770b\u5b50\u4f73\u7684 c++ \u7684 source code of new source case</li> </ol>"},{"location":"handover/eric/IJV_Notebook/#20210201-source","title":"2021/02/01 - source \u8a2d\u5b9a","text":"<ul> <li>\u8001\u5e2b\u6587\u4ef6\u95b1\u8b80<ul> <li>\u9700\u8981\u77e5\u9053 die \u7684 size \u55ce\uff1f\u9084\u662f\u5c31\u8996\u70ba\u9ede\u5149\u6e90\uff1f</li> <li>\u4e4b\u524d\u6240\u5b78\u7fd2\u5230\u7684 source type: uniform, gaussian\uff0c\u2026etc\uff0c\u90fd\u662f\u6307 radiated surface \u4e0a power density \u7684 pattern \u55ce\uff1f</li> </ul> </li> <li>C++ code \u95b1\u8b80</li> <li>MCX readme</li> </ul>"},{"location":"handover/eric/IJV_Notebook/#20210120-training-ann","title":"2021/01/20 - \u91cd\u65b0 training ann \u898f\u5283\uff08\u7e8c\uff09","text":"<p>\u66ab\u6642\u518d\u6574\u7406\u4e00\u4e0b\u6d41\u7a0b\uff0c\u4e4b\u5f8c\u505a\u6642\u82e5\u767c\u73fe\u4e00\u4e9b\u7d30\u7bc0\u518d\u88dc\u4e0a\u4f86\u3002(\u8a73\u7d30\u53ef\u53c3\u8003 2020/01/08\uff0c\u7d9c\u5408 kb's comment)</p> <p>==\u5df2\u5b8c\u6210== 1. transformed (a,b) \u7684 output\u300201/22 meeting slide\u3002     - ann \u7684\u8a13\u7df4\u8cc7\u6599\u7522\u751f\u9810\u8a08\u662f\u7531\u5728 range \u5167\u7684\u96a8\u6a5f (a,b) \u6578\u503c\uff0c\u800c\u4e0d\u662f\u662f\u6f82\u539f\u672c\u4f7f\u7528\u7684\"\u76f4\u63a5 sampling mus\"     - \u554f\u984c\uff1a\u6587\u737b\u4e0a\u6709 whole blood \u7684 musp ? \u9084\u6709\u4e00\u4e9b\u6587\u737b\u7684\u6574\u7406\uff0c\u76ee\u524d\u4ea4\u7d66\u6615\u539f\u3002(\u6574\u7406\u5404\u7d44\u7e54 mus, mua) 2. source \u8a2d\u5b9a\u300201/29 meeting slide\u3002     - LED \u51fa\u5149\u89d2         - Typical FWHM Beam Angle (Table from datasheet)         - Radiation Pattern Characteristics (Figure from datasheet)         - Datasheet     - LED \u5167\u5149\u6e90\u8207\u7d44\u7e54\u8868\u9762\u7684\u5be6\u969b\u8ddd\u96e2     - mcx code \u8a2d\u5b9a     - omlc gaussian beam example</p> <p>==\u5f85\u505a== 1. \u9700\u78ba\u8a8d\u4f7f\u7528\u7684 code template 2. \u786c\u9ad4: \u5b89\u88dd gpu 3060</p> <p>==\u7c21\u6613\u6d41\u7a0b== 1. wmc     - code template: xx     - \u9700\u6c42: \u96a8\u6a5f\u6311\u51fa\u4e0d\u540c\u7684 (a,b) \u9032\u884c\u6a21\u64ec 3. calculate reflectance     - code template: xx 5. ann training     - code template: xx</p>"},{"location":"handover/eric/IJV_Notebook/#20210108-training-ann","title":"2021/01/08 - \u91cd\u65b0 training ann \u898f\u5283","text":"<p>01/08 meeting slide</p> <p>\u7d93\u904e\u4e00\u7cfb\u5217\u7684\u6e2c\u8a66\u5f8c\uff0c\u767c\u73fe ann \u6975\u6709\u53ef\u80fd\u9700\u8981\u91cd\u65b0 training\u3002(\u4e0d\u904e\u5c1a\u4e0d\u77e5\u70ba\u4f55\u662f\u6f82\u7684\u78a9\u8ad6 error \u6703\u5982\u6b64\u4f4e)</p> <p>==\u898f\u5283== - wmc     - \u5149\u5b78\u53c3\u6578         - \u4f7f\u7528\u7684 (a,b) \u9700\u65bc\u6b63\u5e38\u7bc4\u570d     - \u5e7e\u4f55\u53c3\u6578         - \u5e7e\u4f55\u7d50\u8b1b\u8a2d\u5b9a - 2\u4eba (kb\u8207\u6615\u539f or kb\u8207\u6211)     - \u7cfb\u7d71(\u786c\u9ad4)\u53c3\u6578 - \u78ba\u8a8d\u786c\u9ad4\u7684\u8a2d\u8a08\u7b26\u5408wmc\u7684\u8a2d\u5b9a         - sds \u53ef\u4ee5\u591a\u500b             - size \u7b26\u5408\u5be6\u9a57             - \u53ef\u80fd\u4e0d\u80fd\u91cd\u8907(wmc default \u7684 code \u8a18\u5f97\u662f\u4e0d\u80fd\u91cd\u758a\u7684)         - source \u8207\u7d44\u7e54\u8868\u9762\u7684\u8ddd\u96e2             - led \u7684 location \u8207 orientation (led \u767c\u5149\u5f8c\u662f\u5426\u6709 collimator \u6216\u662f filter\uff0c\u5c0e\u81f4\u5c31\u7b97 led \u8207\u8868\u9762\u7dca\u8cbc\uff0c\u5149\u9084\u662f\u6703\u7d93\u904e\u4e00\u6bb5\u8ddd\u96e2\u624d\u6703\u5230\u7d44\u7e54\u8868\u9762\uff0c\u53e6\u5916\u51fa\u5149\u89d2\u8207\u65b9\u5411\u4e5f\u8981\u8003\u616e\uff0c\u8981\u53bb\u67e5\u786c\u9ad4\u898f\u683c!!)         - prism \u8207\u7d44\u7e54\u8868\u9762\u53ef\u4ee5\u5857\u4e00\u5c64\u81a0(\u5927\u8166\u8a08\u5283\u767c\u73fe\u9019\u6a23\u6703\u6bd4\u8f03\u7a69\u5b9a)         - detector \u8207\u7d44\u7e54\u8868\u9762\u7684\u8ddd\u96e2             - \u8981\u67e5 prism \u7684\u898f\u683c\uff0c\u770b\u662f\u5426\u53cd\u5c04\u9762\u6709 coating (\u770b\u53cd\u5c04\u7387\u6703\u4e0d\u6703\u53d7\u5149\u7684\u6ce2\u9577\u6216\u5165\u5c04\u89d2\u7684\u5f71\u97ff)         - \u78ba\u8a8d wmc \u7684\u7b2c 0 \u5c64\u8981\u8a2d\u70ba\u7a7a\u6c23\u9084\u662f detector\u3001prism             - \u5982\u679c\u91cf\u6e2c\u7684\u74b0\u5883\u662f\u6709\u5149\uff0c\u90a3\u7b2c 0 \u5c64\u4e5f\u8981\u662f\u8cbc\u7247(\u5982\u679c\u6709\u8cbc\u7247\u7684\u8a71\uff0c\u4e5f\u6703\u9020\u6210\u6536\u5230\u7684\u5149\u7522\u751f\u8870\u6e1b) - calculate reflectance     - \u4f7f\u7528\u7684 mua \u9700\u65bc\u6b63\u5e38\u7bc4\u570d - ann     - input         - 5 \u5c64\u7d44\u7e54\u7684 mua, mus,          - \u5404\u5c64\u7684 n, g(\u8b8a\u70ba\u56fa\u5b9a)             - g \u9700\u53c3\u8003\u6587\u737b\uff0c\u770b\u808c\u8089\u3001ijv \u7684 g \u503c\u8981\u8a2d\u5b9a\u6210\u4ec0\u9ebc             - \u6216\u662f g \u4e5f\u80fd\u7576\u4f5c\u672a\u77e5\u6578\u53bb fitting         - \u5e7e\u4f55\u5f62\u72c0             - \u5168\u90e8\u56fa\u5b9a             - \u90e8\u5206\u4e0d\u56fa\u5b9a                 - ijv, cca \u6df1\u5ea6(\u4e5f\u53ef\u4ee5\u7576\u4f5c\u672a\u77e5\u6578)                 - ijv, cca \u7ba1\u5f91(\u4e5f\u53ef\u4ee5\u7576\u4f5c\u672a\u77e5\u6578)     - output         - reflectance</p> <p>==\u76ee\u7684== - \u5148\u8dd1\u4e00\u500b\u5c0f\u898f\u6a21\u7684\u6a21\u64ec\uff0c\u78ba\u8a8d ann \u7684\u53ef\u7528\u6027 - \u6b78\u7d0d\u4e0d\u540c\u5149\u5b78\u53c3\u6578\u6240\u5c0d\u61c9\u7684\u7279\u5fb5\uff0c\u89c0\u5bdf\u54ea\u4e9b\u5149\u5b78\u53c3\u6578\u9700\u8981\u8f03\u591a\u7684\u5149\u5b50\u6578 (\u9084\u662f\u9700\u8981\u77e5\u9053 cv \u2192 \u8f03\u53ef\u9760\uff0c\u56e0\u70ba indicator \u6bd4\u8f03\u96e3\u627e)</p>"},{"location":"handover/eric/IJV_Notebook/#20201225-ann","title":"2020/12/25 - \u5927\u898f\u6a21\u6aa2\u6e2c ann \u7a69\u5b9a\u6027\uff08\u7e8c\uff09","text":"<p>12/25 meeting slide</p> <p>\u7531\u65bc\u4e0a\u9031\u50c5\u5728\u8dd1\u5b8c wmc \u4e4b\u5f8c\u6e2c\u8a66\u4e00\u7d44 mua\uff0c\u6240\u4ee5\u53ef\u4ee5\u518d\u66f4\u9032\u4e00\u6b65\u5730\u53bb\u6e2c\u8a66\u4e0d\u540c\u7d44\u7684 mua\uff0c\u4f8b\u5982\u8abf\u6574\u4e0d\u540c\u7684 ijv SO2\uff0c\u7136\u5f8c\u518d\u53bb\u770b\u9019\u4e9b\u53c3\u6578\u4e4b\u4e0b\u7684\u5149\u8b5c\uff0c\u5230\u5e95\u662f\u5426\u80fd\u8ddf ann \u6240\u8f38\u51fa\u7684\u5149\u8b5c\u7a69\u5408\u3002</p> <p>\u8a73\u7d30\u7d50\u679c\u5217\u65bc 12/25 meeting slide\u3002</p> <p>==\u5f85\u78ba\u8a8d\uff1a== - \u6aa2\u67e5 code\uff0c\u78ba\u8a8d mc \u8207 ann \u7684\u5dee\u7570\u662f\u5426\u6839\u672c\u662f code \u5beb\u932f\u5c0e\u81f4\u7684\u3002 - \u5982\u679c\u9700\u8981\u91cd train\uff0c\u9700\u8981\u518d\u5217\u4e00\u4efd\u6e05\u55ae\uff0c\u770b\u6709\u54ea\u4e9b\u7d30\u7bc0\u8981\u78ba\u8a8d\u3002</p>"},{"location":"handover/eric/IJV_Notebook/#20201218-ann","title":"2020/12/18 - \u5927\u898f\u6a21\u6aa2\u6e2c ann \u7a69\u5b9a\u6027\uff08\u7e8c\uff09","text":"<p>12/18 meeting slide</p> <p>\u6574\u7406 12/14 \u6a21\u64ec\u7d50\u679c\u3002\u540c\u6642\u5728\u8abf\u6574\u8eab\u9ad4\u7d44\u6210\u6bd4\u4f8b\u6642\uff0c\u767c\u73fe\u63db\u7b97\u5f8c\u7684 mua \u96e3\u4ee5\u7b26\u5408 ann \u7684 training range\u3002</p> <p>\u89e3\u6c7a\u65b9\u6cd5 \u2193 \u518d\u6b21\u8abf\u6574\u8840\u7d05\u7d20\u6fc3\u5ea6\uff0c\u7522\u751f hemoglobin \u6fc3\u5ea6\u70ba 20 g/L \u6240\u76f8\u61c9\u7684 mua\uff0c\u540c\u6642\u8abf\u6574\u808c\u8089\u7684\u542b\u8840\u91cf\uff0c\u4f7f\u808c\u8089\u7684\u5438\u6536\u4fc2\u6578\u4e5f\u843d\u65bc ann training range\u3002(\u6ce8\u610f\uff1a\u5728\u6b64\u6bd4\u8f03\u4e0d\u9867\u616e\u7d44\u7e54\u7269\u8cea\u7684\u7d44\u6210\u6bd4\u4f8b\u662f\u5426\u843d\u65bc\u6b63\u5e38\u7684\u751f\u7406\u7bc4\u570d\uff0c\u56e0\u70ba\u76ee\u6a19\u50c5\u662f\u8981\u770b mc \u8207 ann \u5728\u76f8\u540c\u7684\u5149\u5b78\u53c3\u6578\u4e4b\u4e0b\u662f\u5426\u80fd\u5920 output \u51fa\u4e00\u6a23\u6216\u5e7e\u4e4e\u8fd1\u4f3c\u7684\u5149\u8b5c - ++\u6578\u503c\u4e0a++\u8207++\u5f62\u72c0\u4e0a++)\u3002</p> <p>\u8a73\u7d30\u7d50\u679c\u5217\u65bc 12/18 meeting slide\u3002</p>"},{"location":"handover/eric/IJV_Notebook/#20201214-ann","title":"2020/12/14 - \u5927\u898f\u6a21\u6aa2\u6e2c ann \u7a69\u5b9a\u6027\uff08\u7e8c\uff09","text":"<p>\u9032\u884c mc \u6a21\u64ec\u3002\u8dd1 5 \u500b mus \u985e\u5225 (\u5206\u5225\u642d\u914d\u4e00\u7d44\u96a8\u6a5f\u7522\u751f\u7684 geo)\u3002 - wl: 680, 690, ..., 810 nm\u3002 - g \u9700\u78ba\u8a8d\u4e00\u4e0b(\u5df2\u78ba\u8a8d) - parameters.json \u4fee\u6539 (mus, geo)\uff1a     - mus \u985e\u5225 \u2193         1. \u5747\u5728\u7bc4\u570d\u5167         2. \u5747\u5728\u7bc4\u570d\u5167         3. \u50c5 fat \u8d85\u51fa         4. \u50c5 muscle \u8d85\u51fa         5. fat, muscle \u5747\u8d85\u51fa     - geo \u985e\u5225     \u8207 mus \u7368\u7acb\uff0c\u96a8\u6a5f\u7522\u751f\u3002(\u4e0d\u904e\u53ef\u80fd\u5148\u9650\u5236\u4e00\u4e0b cca \u4e0d\u61c9\u6bd4 ijv \u6dfa) - config.json \u4fee\u6539 - \u5149\u5b50\u6578 1e9 \u91cd\u8907 10 \u6b21\uff0c\u4ee5\u5f97\u5230\u7a69\u5b9a reflectance\u3002(\u76f8\u7576\u65bc\u5149\u5b50\u6578\u70ba 1e10 \u4e4b\u4e0b\u7684 reflectance\u3001spectrum)</p> <p>==\u5f85\u78ba\u8a8d\uff1a== - a, b \u4e0a\u4e0b\u754c\u7684\u6578\u503c\u662f\u5426\u6b63\u78ba - geo \u7684\u6df1\u6dfa\u662f\u5426\u6709\u4e00\u4e9b\u76f8\u4f9d\u6027\uff08\u4f8b\u5982 cca \u6703\u6bd4 ijv \u6dfa\uff1f\uff09</p>"},{"location":"handover/eric/IJV_Notebook/#20201203-ann","title":"2020/12/03 - \u5927\u898f\u6a21\u6aa2\u6e2c ann \u7a69\u5b9a\u6027","text":"<p>12/04 meeting slide</p> <p>~~\u7406\u60f3\u6aa2\u6e2c: \u53d6 2 \u7a2e\u5e7e\u4f55\u7d44\u7e54(\u96a8\u6a5f\uff1f)\uff0c\u5404\u81ea\u53d6 3 \u7a2e\u6563\u5c04\u7d44\u5408\u3002~~ \u7406\u60f3\u6aa2\u6e2c\uff1a\u5047\u8a2d\u4eba\u9ad4\u7684(a, b)\u662f\u7368\u7acb\u7684\uff0c\u7136\u5f8c\u5169\u8005\u5b58\u5728 continuous uniform joint distribution\uff0c\u610f\u5373\u4e0d\u540c\u4eba\u9ad4\u7684\u7d44\u7e54\uff0c\u843d\u5728\u4e0d\u540c\u7684 (a, b) \u6578\u503c\u7684\u6a5f\u7387\u662f\u76f8\u540c\u7684(\u96a8\u6a5f\u7684)\u3002\u90a3\u9ebc\u4eca\u5929\u5c31\u53ef\u4ee5\u96a8\u6a5f\u53d6\u5e7e\u500b\u4e0d\u540c\u7684(a, b)\u7d44\u5408\uff0c\u4e26\u642d\u914d\u96a8\u6a5f\u53d6\u51fa\u7684\u5e7e\u4f55\u7d44\u7e54\uff0c\u4f86\u8aaa\u4ed6\u5011\u662f\u96a8\u6a5f\u62bd\u51fa\u7684\u4e0d\u540c\u7684\u4eba\u7684\u7d44\u7e54\uff0c\u63a5\u8457\u9032\u884c mc \u6a21\u64ec\uff0c\u770b\u770b mc \u6a21\u64ec\u7684\u7d50\u679c\uff0c\u662f\u5426\u8207 ann output \u7684\u7d50\u679c\u4e00\u6a23\u3002</p> <p>\u5be6\u4f5c\uff1a - \u7522\u751f 5 \u500b\u7d44\u7e54\u7684 mus \u8207 1 \u500b tissue_geo\uff0c\u770b\u770b\u6578\u503c\u662f\u5426\u843d\u5728\u7bc4\u570d(\u662f\u6216\u5426\u90fd\u884c\uff0c\u56e0\u70ba\u90fd\u662f\u4ee3\u8868\u7d44\u7e54\u7684\u4e00\u7a2e\u53ef\u80fd\u6027\uff0c\u53ea\u662f \"\u5426\" \u7684\u8a71\u5c31\u4ee3\u8868 ann \u6c92\u6709 cover \u5230) - output \u9019 5 \u500b mus \u80cc\u5f8c\u7684 (a,b) \u8207 tissue_geo \u80cc\u5f8c\u7684 7 \u500b\u53c3\u6578\u6578\u503c(skin_thickness, ijv_depth ...etc) - \u505a\u70ba\u5f8c\u7e8c mcx \u904b\u884c\u6240\u6839\u64da\u7684\u6a94\u6848</p>"},{"location":"handover/eric/IJV_Notebook/#20201127-ann","title":"2020/11/27 - \u6aa2\u67e5 ann \u662f\u5426\u6709\u6548\uff08\u7e8c\uff09","text":"<p>11/27 meeting slide - \u78ba\u8a8d 11/23 \u7d50\u679c\u3002(\u6700\u7d42\u8a73\u7d30\u7d50\u679c\u5217\u65bc meeting slide)     - (1) ann no cover \u5be6\u9a57 \u2192 \u6539\u63a1 11/20 \u7684\u4f4e\u5149\u5b50\u6578\u6a21\u64ec\u3002     - (2) ann cover \u5be6\u9a57 \u2192 11/27 \u91cd\u8dd1\u4e00\u6b21\u6a21\u64ec\uff1a\u8abf\u6574 fat\u3001muscle \u7684 a, b\uff0c\u4f7f\u8dd1 mc \u6642\u7684\u7d44\u7e54 mus \u7b26\u5408 ann \u7684 training range\uff1b\u4e26\u907f\u958b header[\"photon\"] \u7684\u554f\u984c\uff0c\u4f7f\u7528\u4f4e\u5149\u5b50\u6578\u9032\u884c\u6a21\u64ec\u3002(10 \u5104\u9846\u91cd\u8907 100 \u6b21\uff0c\u7b49\u6548\u65bc 100 \u5104\u91cd\u8907 10 \u6b21)</p>"},{"location":"handover/eric/IJV_Notebook/#20201126-code","title":"2020/11/26 - \u6aa2\u67e5 code","text":"<ul> <li>na \u8907\u7fd2\uff0c\u8207\u7a1c\u93e1\u7684\u95dc\u4fc2\uff1f<ul> <li>\u770b\u8d77\u4f86 light source \u8ddd\u96e2 tissue surface 15mm \u662f\u56e0\u70ba\u7a1c\u93e1\u7684\u95dc\u4fc2\u3002(++\u54f2\u7693\u78a9\u8ad6\uff1a\u5149\u6e90\u8207\u5075\u6e2c\u5149\u7e96\u675f\u7684\u63a2\u982d\u7aef\u5747\u642d\u914d\u908a\u9577 5 mm \u7684\u7b49\u8170\u76f4\u89d2\u4e09\u7a1c\u93e1++)</li> <li>\u884d\u751f\u554f\u984c\uff1a++\u5982\u679c detector \u7aef\u4e5f\u6709\u7a1c\u93e1++\uff0c\u9019\u6a23\u8a08\u7b97 reflectance \u662f\u5426\u4e5f\u8981\u6539\u8b8a\uff1f(\u8207\u5e73\u5e38\u4e0d\u540c)<ul> <li>\u2192++\u770b\u8d77\u4f86\u662f\u6c92\u6709QQ++\uff0c\u800c\u4e14 detector \u5728\u8dd1 wmc \u6642\u9084\u662f\u88ab\u8a2d\u5b9a\u6210\u7dca\u8cbc\u7d44\u7e54\u8868\u9762(z=0)\u3002</li> <li>\u800c\u4e14\u5728\u8dd1 reflectance \u7684\u8a08\u7b97\u6642\uff0c\\(\\theta_{max}\\) \u7684\u8a08\u7b97\u9084\u662f detector \u7684 n \u4f86\u7576\u5206\u6bcd(\u7406\u8ad6\u4e0a\u61c9\u8a72\u662f\u8981\u7528\u7a7a\u6c23\u6216\u7d44\u7e54\u7684 n)\u3002</li> </ul> </li> </ul> </li> <li>\u54f2\u7693\u3001\u80e4\u752b\u3001\u662f\u6f82\u78a9\u8ad6\u53c3\u8003 \u2192 ++\u54f2\u7693\u78a9\u8ad6\uff1asds = 20mm++</li> <li>ann \u53c3\u6578\u8a2d\u5b9a\u662f\u5426\u7b26\u5408 mcx \u7684\u53c3\u6578\u8a2d\u5b9a\uff1f \u2192 \u78ba\u8a8d\uff1a\u6709\u3002<ul> <li>mua</li> <li>mus (\u4e0d\u904e\u4ec0\u9ebc\u6642\u5019\u9700\u8981\u53bb\u8a13\u7df4 high mus)</li> <li>n</li> <li>g</li> </ul> </li> <li>header \u7684 photon \u5b58\u53d6\u602a\u602a\u7684</li> </ul>"},{"location":"handover/eric/IJV_Notebook/#20201125-ann","title":"2020/11/25 - \u6aa2\u67e5 ann \u662f\u5426\u6709\u6548\uff08\u7e8c\uff09","text":"<p>\u6574\u7406\u554f\u984c\u6e05\u55ae(for\u662f\u6f82)\uff0c\u8a08\u7b97 11/23 \u6a21\u64ec\u4e4b reflectance \u7d50\u679c\u3002 - \u554f\u984c\u6e05\u55ae - 11/23 \u6a21\u64ec\u7e3d\u7d50     - (1) header[\"total_photon\"] \u6578\u5b57\u932f\u8aa4\uff0c\u4f8b\u5982\u61c9\u70ba 100 \u5104\uff0c\u7d50\u679c\u53ea\u986f\u793a 14 \u5104\uff0c\u5982\u6b64\u6703\u5c0e\u81f4 reflectance \u88ab\u9ad8\u4f30\u3002\u56e0\u6b64\u6b64\u6b21\u6a21\u64ec\u4e0d\u53ef\u7528\u3002     - (2) mus \u7bc4\u570d\u6c92\u6aa2\u67e5\u5230\uff0cfat &amp; muscle \u7684 mus \u8dd1\u5230 training range \u5916\u4e86\uff0c\u56e0\u6b64\u9700\u91cd\u65b0 run mc\u3002\u4e0d\u904e\u4e5f\u56e0\u6b64\u767c\u73fe mus \u7684 training range \u597d\u50cf\u4e5f\u6709\u9ede\u5947\u602a\u3002</p>"},{"location":"handover/eric/IJV_Notebook/#20201123-ann","title":"2020/11/23 - \u6aa2\u67e5 ann \u662f\u5426\u6709\u6548\uff08\u7e8c\uff09","text":"<p>11/20 \u7684\u5149\u5b50\u6578\u504f\u4f4e\uff0ccv \u662f12.5%\uff0c\u56e0\u6b64\u4ecd\u9700\u63d0\u9ad8\u5149\u5b50\u6578\u8a66\u8a66\u3002 \u505a 2 \u500b\u6e2c\u8a66 \u2192 (1) ann \u6c92\u6709 cover \u5230\u7684 mua \u662f\u5426\u8207 MC \u6a21\u64ec\u76f8\u540c\u3002(2) ann \u6709 cover \u5230\u7684 mua \u662f\u5426\u8207 MC \u6a21\u64ec\u76f8\u540c\u3002 - \u8dd1 (1)\uff0c100 \u5104\u9846\u91cd\u8907 9 \u6b21 (\u6709 1 \u6b21\u5df2\u7b49\u6548\u65bc 11/20 \u8dd1\u7684)\uff0c\u7b97 cv\uff0c wavelength \u2192 750nm\u3002 - \u8dd1 (2)\uff0c100 \u5104\u9846\u91cd\u8907 10 \u6b21\uff0c\u7b97 cv\uff0cwavelength  \u2192 680nm\u3002</p>"},{"location":"handover/eric/IJV_Notebook/#20201120-ann","title":"2020/11/20 - \u6aa2\u67e5 ann \u662f\u5426\u6709\u6548","text":"<p>\u8a66\u8dd1 ijv \u6a21\u64ec\u3002 - \u78ba\u8a8d\u6a21\u64ec\u524d\u7d30\u7bc0     1. validateANN.json     2. session_id     3. type     4. voxel_size     5. medium_n     6. detector_na     7. coefficients.csv     8. parameters (mua, mus, n, g checked, geometry checked, boundary checked which is for voxel = 0.1mm)     9. mcx_input, mcx.py: not sure whether to uncomment set seed) need to quick test     10. fiber     11. wavelength (++only 750nm++) - \u5148\u7528\u5c11\u91cf\u5149\u5b50\u8dd1\u8dd1\u770b     1. \u770b\u662f\u5426\u80fd\u8dd1 (\u6e2c\u8a66\u5f8c\uff1a\u53ef\uff01)     2. \u770b\u662f\u5426\u96a8\u6a5f (\u6e2c\u8a66\u5f8c\uff1a\u53ef\uff01)     3. \u770b\u770b 1 \u5104\u9846\u7684\u6642\u9593 - \u8dd1 10 \u5104\u9846\uff0c\u4e26\u91cd\u8907 10 \u6b21 ==(ann \u6c92\u6709 cover \u5230\u7684 mua)==     1. \u7b97 CV \u503c</p>"},{"location":"handover/eric/IJV_QEPro_installation/","title":"IJV | QEPro \u74b0\u5883\u5b89\u88dd","text":"<p>Editor: \u8a31\u9038\u7fd4 (R08)</p>"},{"location":"handover/eric/IJV_QEPro_installation/#1","title":"1. \u5b89\u88dd\u8edf\u9ad4","text":""},{"location":"handover/eric/IJV_QEPro_installation/#11-labview-ni-driver","title":"1.1 Labview \u8207 NI Driver","text":"<p>\u7c21\u6613\u6d41\u7a0b \u2193 - Copy IJV \u5916\u63a5\u786c\u789f(5TB) \u7684 LabView \u8cc7\u6599\u593e\u81f3\u4f60\u7684 Windows \u7b46\u96fb\u4e2d\u3002     - LabView \u8cc7\u6599\u593e\u8def\u5f91\uff1a<code>/media/md703/Expansion/LabView</code> - \u78ba\u8a8d\u5df2\u6709\u4e0b\u5217 4 \u500b\u6620\u50cf\u6a94     - <code>NI_Academic_Site_License_Fall_2014_Disk01.iso</code>     - <code>NI_Academic_Site_License_Fall_2014_Disk02.iso</code>     - <code>NI_Academic_Site_License_Fall_2014_Disk03.iso</code>     - <code>NI_Device_Driver.iso</code> - \u5f9e \u5b89\u88dd\u8aaa\u660e \u7684\u7b2c 2 \u6b65\u958b\u59cb\u63a5\u8457\u505a (\u5f9eDisk01.iso \u958b\u59cb\u6309)\uff0c\u540c\u6642\u8a18\u4e0b\u7b2c\u4e00\u6b65\u5716\u7247\u4e2d\u7684\u6388\u6b0a\u78bc (serial number)\u3002</p>"},{"location":"handover/eric/IJV_QEPro_installation/#12-vision-aquisition-software","title":"1.2 \u5b89\u88dd vision aquisition software","text":"<p>\u7c21\u6613\u6d41\u7a0b \u2193 - \u9032\u5165\u7db2\u7ad9 (\u9023\u7d50) - \u9078\u64c7 Version 17.5 - \u4e0b\u8f09     </p>"},{"location":"handover/eric/IJV_QEPro_installation/#13-qepro-driver-omnidriver-and-spam","title":"1.3 \u5b89\u88dd QEPro driver - OmniDriver and SPAM","text":"<p>\u7c21\u6613\u6d41\u7a0b \u2193 - \u9032\u5165\u7db2\u7ad9 (\u9023\u7d50) - \u4e0b\u8f09 OmniDriver and SPAM (Windows Version (32-bit)\uff0c\u56e0 Labview \u4e5f\u662f 32-bit\u3002 </p>"},{"location":"handover/eric/IJV_QEPro_installation/#2","title":"2. \u4f7f\u7528\u8edf\u9ad4","text":""},{"location":"handover/eric/IJV_QEPro_installation/#21-labview-qepro","title":"2.1 \u555f\u52d5 labview \u63a7\u5236 QEPro","text":"<p>\u65b9\u6cd5\uff1a \u4f7f\u7528 <code>QEPROcontrol_version3.vi</code> \u7684\u6a94\u6848, \u529f\u80fd\u53ef\u4ee5\u9304\u5f71\uff0c\u53ef\u4ee5\u63a7\u5236\u591a\u4e45\u62cd\u4e00\u5f35\uff0c\u53ef\u4ee5\u8f38\u51fa\u5149\u8b5c\u6a94\u6848\u3002</p> <p>\u53ef\u80fd\u5b58\u5728\u554f\u984c\uff1a\u6253\u4e0d\u958b v3 \u6a94\u6848\uff0c\u539f\u56e0 \u2192 \u627e\u4e0d\u5230 net40.dll\u3002 \u89e3\u6cd5\uff1a\u6307\u5b9a\u8def\u5f91\uff1a C:\\Program Files (x86)\\Ocean Optics\\OmniDriver\\OOI_HOME\\net40.dll \u7d66\u8edf\u9ad4\u3002Solver: \u6797\u570b\u8056(BOSI R08)</p>"},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/","title":"IJV | Useful mcx-user group threads \u8207 MCX Code \u6574\u7406","text":""},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/#mcx-user-group-threads","title":"mcx-user group threads","text":"<p>mcx-user link: here.</p> <ol> <li>pymcx \u958b\u767c\u7684\u59cb\u672b     \u8a18\u9304\u539f\u672c\u662f\u6f82\u958b\u767c\u7684\u6162\u6162\u7684 outdated\uff0c\u7136\u5f8c\u6709\u4eba\u627f\u63a5\u3001\u512a\u5316\uff0c\u4e26\u5f62\u6210\u4e00\u500b repo (\u4f46\u73fe\u5728\u597d\u50cf\u53c8 outdated \u4e86)     https://groups.google.com/g/mcx-users/c/Gypz-YvuYZY/m/Ih7m07qAAQAJ</li> <li>Info about issaveref     https://groups.google.com/g/mcx-users/c/nMZzRXeHmFQ/m/iHtjO5RfBQAJ</li> <li>2019-08-21 [d9d5238] allow one to overwrite SaveDataMask from command line</li> </ol>"},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/#mcx-code","title":"MCX code \u8aaa\u660e","text":"<ol> <li> <p>\u8981\u5229\u7528 MCX \u6a21\u64ec\u5149\u5b50\u7684 random walk\uff0c\u53ef\u5206\u70ba\u5169\u5927\u6b65\u9a5f\u3002==1. \u5916\u90e8\u547c\u53eb:== \u5229\u7528 python \u6216 matlab \u88fd\u4f5c\u9069\u7576\u7684 <code>simulation input file</code> (\u672c project \u70ba python)\uff0c\u4e26\u5408\u6210 command\uff0c\u65bc cmd \u4e0a\u9001\u5165 mcx kernel\u3002==2. \u5167\u90e8\u6a21\u64ec:== \u53ef\u5c07 <code>mcx_utils.c</code> \u8207 <code>mcx_core.cu</code> \u8996\u70ba\u6838\u5fc3\u7684\u6a21\u64ec\u7a0b\u5f0f\uff0c\u5169\u652f\u7a0b\u5f0f\u5404 include \u4e86\u6578\u500b .h file \u505a\u4f7f\u7528\uff0c\u4f8b\u5982 <code>mcx_utils.h</code>\u3001<code>mcx_const.h</code>\u3001<code>mcx_core.h</code> ... \u7b49\u3002</p> </li> <li> <p>mcx \u57f7\u884c\u6a94 compile \u65b9\u5f0f\uff1a\u65bc <code>/mcx/src</code> \u4e2d\u958b\u555f command line\uff0c\u4e26 type \u4e0a <code>make</code>\u3002</p> </li> </ol>"},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/#from-source-code","title":"\u5167\u90e8\u6a21\u64ec (from source code)","text":""},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/#_1","title":"\u57fa\u672c\u6d41\u7a0b","text":""},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/#mcx_utilsc","title":"<code>mcx_utils.c</code>","text":"<ul> <li>parse <code>command</code> from cmd.</li> <li>\u8b8a\u6578\u540d\u7a31\u8f49\u63db \u2192 \u4f8b\u5982 src_type \u7684 pencil \u8f49\u63db\u6210 1\uff0csrc_type \u7684 anglepattern \u8f49\u63db\u6210 16\u3002</li> <li>...etc\u3002</li> </ul>"},{"location":"handover/eric/IJV_Useful_mcx-user_group_threads_MCX_code_tutorial/#mcx_corecu","title":"<code>mcx_core.cu</code>","text":"<ul> <li>Do main simulation.</li> <li>Important functions (might be related to our project)<ul> <li>==Ln 693==: @brief Terminate a photon and launch a new photon according to specified source form<ul> <li><code>__device__ inline int launchnewphoton()</code></li> <li>==Ln 794==: Attempt to launch a new photon until success</li> <li>==Ln 1051==: Now a photon is successfully launched, perform necssary initialization for a new trajectory</li> </ul> </li> <li>==Ln 1107==:  @brief The core Monte Carlo photon simulation kernel (!!!Important!!!)<ul> <li>This is the core Monte Carlo simulation kernel, please see Fig. 1 in Fang2009. Everything in the GPU kernels is in grid-unit. To convert back to length, use cfg-&gt;unitinmm (scattering/absorption coeff, T, speed etc)</li> <li><code>kernel void mcx_main_loop()</code></li> </ul> </li> <li>==Ln 1675==: @brief Master host code for the MCX simulation kernel (!!!Important!!!)<ul> <li>This function is the master host code for the MCX kernel. It is responsible for initializing all GPU variables, copy data from host to GPU, launch the kernel for simulation, wait for competion, and retrieve the results.</li> <li><code>void mcx_run_simulation()</code></li> </ul> </li> </ul> </li> </ul>"},{"location":"handover/eric/IJV_holder_design_in_vivo_precautions/","title":"IJV | \u76ee\u524dholder\u8a2d\u8a08\u5716\u3001\u6d3b\u9ad4\u91cf\u6e2c\u6ce8\u610f\u4e8b\u9805","text":""},{"location":"handover/eric/IJV_holder_design_in_vivo_precautions/#current-holders-and-molds-tinkercad","title":"Current holders and molds (tinkercad)","text":"<ul> <li>\u9023\u7d50</li> </ul>"},{"location":"handover/eric/IJV_holder_design_in_vivo_precautions/#measurement-memo","title":"Measurement Memo","text":"<ul> <li> <p>Procedure</p> <ul> <li>\u958b\u555f LED \u5149\u6e90 (\u81f3\u5c11 10 \u5206\u9418\u5f8c\u518d\u958b\u59cb\u91cf\u6e2c\uff0c\u78ba\u4fdd\u5149\u5f37\u662f\u7a69\u5b9a\u7684)</li> <li>\u8d85\u97f3\u6ce2\u64cd\u4f5c<ol> <li>\u53d7\u6e2c\u8005\u5e73\u8eba\uff0c\u982d\u90e8\u671d\u4e0a\uff0ctarget \u70ba\u53f3\u9838 ijv</li> <li>\u8d85\u97f3\u6ce2\u63a2\u982d\u4f86\u56de\u79fb\u52d5\uff0c\u627e\u51fa\u6b63\u78ba ijv \u4f4d\u7f6e\u8207 ijv \u8d70\u52e2\u5e73\u7de9\u7684\u5340\u57df</li> <li>\u8cbc\u4e0a ijv \u5e73\u7de9\u8655\u7684\u5169\u7aef\u6a19\u8a18</li> <li>\u770b\u9700\u6c42\uff0c\u53ef\u4ee5\u8a18\u9304\u4e00\u4e0b\u6b64\u53d7\u8a66\u8005\u7684 ijv \u5f71\u50cf (\u9304\u5f71\u3001\u4e8b\u5f8c\u7528 imageJ \u5206\u6790\u5e7e\u4f55\u53c3\u6578)</li> </ol> </li> <li>\u8cbc\u4e0a\u63a2\u982d<ol> <li>\u6e05\u9664\u8d85\u97f3\u6ce2\u5c0e\u81a0\uff0c\u4f7f\u7528\u9152\u7cbe\u6e05\u6f54\u76ae\u819a\u8868\u9762(\u6cb9\u8102)</li> <li>\u6495\u9664 ok \u7e43\u4e0a\u96d9\u9762\u81a0\u5e03\u4e4b\u80cc\u81a0</li> <li>\u6293\u597d source, detector \u4f4d\u7f6e\uff0c\u8cbc\u4e0a ok \u7e43</li> <li>\u5fae\u8abf ok \u7e43\uff0c\u78ba\u8a8d source, detector \u7f6e\u65bc ijv \u6b63\u4e0a\u65b9</li> <li>\u78ba\u4fdd prism \u548c\u76ae\u819a\u4e4b\u9593\u6709\u8d85\u97f3\u6ce2 gel (for index-match)</li> </ol> </li> <li>\u78ba\u8a8d labview \u66dd\u5149\u6642\u9593\u662f\u5426\u70ba 0.1 s</li> <li>\u78ba\u8a8d\u8a0a\u865f\u662f\u5426\u5e73\u7a69<ol> <li>\u4e0a\u65b9 window \u7684\u6574\u9ad4\u5149\u8b5c\u8a0a\u865f\u662f\u5426\u5e73\u7a69</li> <li>\u5de6\u4e0b\u89d2\u7684 window \u662f\u5426\u6709\u770b\u898b CVP \u6ce2\u5f62 (\u5168\u6ce2\u9577\u5e73\u5747\u7684\u5012\u6578)</li> <li>\u53f3\u4e0b\u89d2\u7684 window \u5448\u73fe\u7684\u662f delta_R / R (800nm \u8655\uff0c\u4e5f\u662f\u8a0a\u865f\u6700\u5927\u503c\u8655)</li> </ol> </li> <li>\u958b\u59cb\u4f9d\u64da\u611f\u8208\u8da3\u4e4b sds \u9032\u884c\u91cf\u6e2c</li> <li>\u82e5\u662f\u8840\u6c27\u8abf\u8b8a\u5be6\u9a57\uff0c\u53ef\u89c0\u5bdf 760 nm \u8655\u7684\u6ce2\u5f62(\u5982\u679c\u7531\u5e73\u6ed1\u8b8a\u51f9\u9677\uff0c\u53ef\u521d\u6b65\u8996\u70ba\u8840\u6c27\u4e0b\u964d\u7684\u8b49\u64da)</li> </ul> </li> <li> <p>Note</p> <ul> <li>\u5be6\u9a57\u7aef<ol> <li>for index-match \u7684 gel \u4e0d\u8981\u592a\u591a\u4e5f\u4e0d\u8981\u592a\u5c11\uff0c\u4e26\u6ce8\u610f\u53ef\u80fd\u6703\u4e7e\u6389\u7684\u554f\u984c\uff0c\u91cf\u6e2c\u904e\u7a0b\u9700\u6ce8\u610f\uff0c\u82e5\u4e7e\u6389\u9700\u6dfb\u52a0 gel</li> <li>\u6ce8\u610f\u91cf\u6e2c\u904e\u7a0b\u4e2d\uff0c\u8d85\u97f3\u6ce2\u5c0e\u81a0\u4e0d\u8981\u6cbe\u5230 ok \u7e43\u4e0a\u4e4b\u96d9\u9762\u81a0\u5e03</li> <li>ok \u7e43\u9700\u76e1\u91cf\u8207\u76ae\u819a\u9ecf\u8cbc\u7dca\u5bc6\u3001\u5e73\u6574</li> <li>ok \u7e43\u4e0a\u96d9\u9762\u81a0\u5e03\u4e4b\u80cc\u81a0\u53ef\u5728 ok \u7e43\u9ecf\u8cbc\u81f3\u76ae\u819a\u524d\u9810\u5148\u6495\u9664\uff0cok \u7e43\u9ecf\u5f8c\u6703\u8f03\u96e3\u6495\u9664</li> </ol> </li> <li>3D \u5217\u5370\u7aef<ol> <li>support (\u652f\u6490) \u7684 infill pattern \u53ef\u9078\u64c7 line\uff0cinfill density \u53ef\u9078\u64c7 100%\u3002\u5927\u9762\u7a4d\u7684\u61f8\u7a7a\u8868\u9762\u6703\u8f03\u5e73\u6574\u3002</li> <li>detector holder \u5370\u5b8c\u5f8c fiber \u7aef\u6703\u8f03\u7dca\uff0c\u53ef\u5148\u78ba\u8a8d fiber \u662f\u5426\u80fd\u63a5\u81f3 holder \u5e95\u7aef</li> </ol> </li> </ul> </li> </ul>"},{"location":"handover/eric/IJV_tissue_model_setting/","title":"IJV | \u7d44\u7e54\u6a21\u578b\u8a2d\u5b9a July, 2021 - Revised","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#_1","title":"\u786c\u9ad4\u8a2d\u5b9a","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#i","title":"i. \u5149\u6e90\u57fa\u672c\u53c3\u6578\u8207\u6a21\u64ec","text":"<ul> <li>\u5be6\u9ad4\u5716     </li> <li>\u898f\u683c<ol> <li>\u5c3a\u540b<ul> <li>Holder<ul> <li>\u908a\u9577: 28 mm</li> <li>\u4e2d\u592e\u51fa\u5149\u5b54\u534a\u5f91: 5 mm</li> </ul> </li> </ul> </li> <li>\u6298\u5c04\u7387<ul> <li>Holder     \u6750\u8cea\u70ba PLA\uff0c680-810nm \u4e0b\u7684\u6298\u5c04\u7387\u5982\u4e0b\u5716\u3002\u8cc7\u6599\u4f86\u6e90: Optical Properties of Polylactides\u3002     </li> </ul> </li> </ol> </li> <li>MCX \u4e4b Illumination pattern \u6a21\u64ec\u65b9\u5f0f     \u4ee5\u4e0b\u65b9\u4e4b pattern \u9032\u884c\u6a21\u64ec(\u4e0a\u5b78\u671f\u9593\u4fee\u6539 mcx source code \u4e4b\u7d50\u679c) \u7c21\u55ae\u9a57\u8b49\u6d41\u7a0b\u56de\u9867(6\u6708\u81f37\u6708\u521d\u4e4b\u9032\u5ea6)<ol> <li>\u76ee\u6a19\uff1amcx \u6a21\u64ec\u4e4b pattern \u9700\u8207\u5be6\u9a57\u62cd\u651d\u4e4b pattern \u4e00\u81f4 (\u7b26\u5408\u771f\u5be6\u60c5\u6cc1)</li> <li>Pattern \u6bd4\u5c0d\u4e4b\u7d50\u679c<ul> <li>\u65b9\u6cd5\u4e00\uff1a\u756b\u4e00\u689d\u7a7f\u904e\u5f71\u50cf\u4e2d\u5fc3\u9ede\u7684\u76f4\u7dda\uff0c\u89c0\u5bdf\u6a21\u64ec\u7d50\u679c\u8207\u5be6\u9a57\u7d50\u679c\u5728\u6b64\u4e00\u76f4\u7dda\u4e0a\u7684\u8b8a\u5316\u8da8\u52e2\u662f\u5426\u4e00\u81f4\u3002     <code>One of previous results \u2193</code> </li> <li>\u65b9\u6cd5\u4e8c\uff1a\u89c0\u5bdf\u6a21\u64ec\u7d50\u679c\u8207\u5be6\u9a57\u7d50\u679c\u7684 average of gray value over the same radial distance \u662f\u5426\u4e00\u81f4\u3002     <code>One of previous results \u2193</code> </li> </ul> </li> <li>\u7d50\u8ad6     \u5728\u4ee5\u4e0a\u7684\u5169\u500b\u6bd4\u5c0d\u65b9\u6cd5\u4e2d\uff0cmcx \u6a21\u64ec\u4e4b pattern \u8207\u5be6\u9a57\u62cd\u651d\u4e4b pattern \u7686\u5177\u6709\u4e00\u5b9a\u7a0b\u5ea6\u4e4b\u4e00\u81f4\u6027\uff0c\u56e0\u6b64\u5f80\u5f8c\u5c07\u4ee5\u6b64 pattern \u4ee3\u8868\u771f\u5be6\u60c5\u6cc1\u9032\u884c\u6a21\u64ec\u3002</li> </ol> </li> </ul>"},{"location":"handover/eric/IJV_tissue_model_setting/#ii","title":"ii. \u5075\u6e2c\u5668\u57fa\u672c\u53c3\u6578\u8207\u6a21\u64ec","text":"<ul> <li>\u5be6\u9ad4\u5716    </li> <li>\u898f\u683c<ol> <li>\u5c3a\u540b<ul> <li>Holder<ul> <li>\u9577\u908a: 14.06 mm</li> <li>\u77ed\u908a: 12.13 mm</li> </ul> </li> <li>Prism<ul> <li>\u908a\u9577 5 mm \u4e4b\u7b49\u8170\u76f4\u89d2\u4e09\u89d2\u5f62</li> </ul> </li> </ul> </li> <li>\u6298\u5c04\u7387<ul> <li>Holder     \u6750\u8cea\u4ea6\u70ba PLA\uff0c\u56e0\u6b64\u6298\u5c04\u7387\u8207\u5149\u6e90\u4e4b Holder \u76f8\u540c\u3002</li> <li>Prism     \u7a1c\u93e1\u578b\u865f\u70ba \u2192 PS909, THORLAB\uff0c\u8207\u80e4\u752b\u3001\u54f2\u7693\u4f7f\u7528\u7684\u578b\u865f\u76f8\u540c\u3002\u6750\u8cea\u70ba N-BK7\u3002\u6298\u5c04\u7387\u8a08\u7b97\u516c\u5f0f(Sellmeier Equation)\u5982\u4e0b\u5716\u3002\u8cc7\u6599\u4f86\u81ea\u65bc\u4f86\u6e90\u3002  \u4e0a\u8ff0\u4e4b\u516c\u5f0f\u7684 unit of \u03bb \u70ba \u03bcm\u3002\u800c\u53e6\u4e00\u500b\u53ef\u4f9b\u53c3\u8003\u4e4b\u6298\u5c04\u7387\u8cc7\u6599\u4f4d\u65bc\u6b64\u7db2\u7ad9 (given by THORLAB Tech Support)\u3002\u56e0\u6b64\uff0c\u900f\u904e Sellmeier Equation \u7684\u8a08\u7b97\uff0c\u6211\u5011\u53ef\u5f97 680-810nm \u4e0b\u7684\u6298\u5c04\u7387\u5982\u4e0b\u5716 </li> </ul> </li> </ol> </li> <li>\u5149\u7e96 full acceptance angle \u4e4b\u8a08\u7b97<ul> <li>\u6578\u503c\u5b54\u5f91, NA     \u6839\u64da\u8001\u5e2b\u63d0\u4f9b\u7684\u8cc7\u6599\uff0c\u6211\u5011\u6240\u4f7f\u7528\u7684\u5149\u7e96NA\u70ba==0.22==\u3002     </li> <li>\u8a08\u7b97\u6d41\u7a0b     \u5149\u7e96\u7684 full acceptance angle \u8a08\u7b97\u516c\u5f0f\u70ba 2 x sin^-1^(NA/n_prism)\uff0c\u4ee3\u5165\u4e0d\u540c\u6ce2\u9577\u4e0b\u7684 n_prism\uff0c\u53ef\u898b\u4e0b\u5716\u7684\u6db5\u84cb\u7bc4\u570d\u3002     </li> </ul> </li> <li>\u7a1c\u93e1\u5c3a\u540b\u7684\u9069\u5408\u5ea6\u9a57\u7b97     </li> <li>MCX \u4e4b\u6a21\u64ec\u65b9\u5f0f\uff1a     \u65bc MCX \u5167\uff0c\u6211\u5011\u4ee5 <code>\u5c07 Prism \u8207 PLA \u7686\u8996\u70ba\u7d44\u7e54\u7684\u65b9\u5f0f\u9032\u884c\u6a21\u64ec</code>\uff0c\u907f\u958b\u65bc source code \u4e2d\u8abf\u6574 detector \u9ad8\u5ea6\u7684\u554f\u984c\u3002MCX \u4e4b\u6a21\u64ec\u793a\u610f\u5716\u5982\u4e0b\u3002          \u5c0d\u7167\u4e4b\u771f\u5be6\u60c5\u6cc1\u793a\u610f\u5716\u5982\u4e0b\u3002(\u4e26\u672a follow \u771f\u6b63\u64fa\u8a2d\u4e4b\u65b9\u5411)             </li> </ul>"},{"location":"handover/eric/IJV_tissue_model_setting/#iii","title":"iii. \u63a2\u982d\u91cf\u6e2c\u4e4b\u5b8c\u6574\u64fa\u8a2d","text":"<ul> <li>\u63a2\u982d\u64fa\u8a2d\u65b9\u5411\uff1a\u4f4d\u65bc ijv \u6b63\u4e0a\u65b9\u3001\u8207 ijv \u5e73\u884c</li> <li>\u5c40\u90e8\u793a\u610f\u5716\uff0c\u805a\u7126\u65bc\u786c\u9ad4\u7684\u64fa\u8a2d\uff0c\u63a2\u982d\u4e0b\u65b9\u50c5\u542b\u4e00\u5c64\u7d44\u7e54\u3002(\u5229\u7528 MCX \u4f5c\u8005\u8fd1\u671f\u958b\u767c\u7684 MCX Cloud \u4e2d\u7684 Preview \u529f\u80fd\u7e6a\u88fd)      \u7c21\u6613\u4e09\u8996\u5716 \u2193<ol> <li>\u4fef\u8996\u5716     </li> <li>\u524d\u8996\u5716     </li> <li>\u5074\u8996\u5716     </li> </ol> </li> <li>\u6574\u9ad4\u793a\u610f\u5716\uff0c\u63a2\u982d\u4e0b\u65b9\u542b\u5b8c\u6574\u7d44\u7e54\u3002     </li> </ul>"},{"location":"handover/eric/IJV_tissue_model_setting/#_2","title":"\u5e7e\u4f55\u53c3\u6578\u8a2d\u5b9a","text":"<ol> <li> <p>\u6615\u539f</p> <ul> <li>Upper neck     </li> <li>Middle neck     </li> <li>Lower neck     </li> </ul> </li> <li> <p>\u9038\u7fd4</p> <ul> <li>Upper neck     </li> <li>Middle neck     </li> <li>Lower neck     </li> </ul> </li> </ol>"},{"location":"handover/eric/IJV_tissue_model_setting/#_3","title":"\u5149\u5b78\u53c3\u6578\u8a2d\u5b9a","text":"<ol> <li>Background layer</li> <li>1st layer</li> <li>2nd layer</li> </ol>"},{"location":"handover/eric/IJV_tissue_model_setting/#_4","title":"\u76f8\u95dc\u554f\u984c","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#1-mcx","title":"1. MCX \u7d30\u7bc0\u53c3\u6578\u8a2d\u5b9a","text":"<ul> <li>Boundary reflect \u2192 No</li> </ul>"},{"location":"handover/eric/IJV_tissue_model_setting/#2","title":"2. \u6a21\u578b\u5927\u5c0f (\u9577\u3001\u5bec\u3001\u9ad8)","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#3-detector-2","title":"3. detector \u53ef\u4ee5\u6709 2 \u500b\uff1f","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#4","title":"4. \u5be6\u9a57\u767c\u73fe\u7a1c\u93e1\u5f71\u97ff\u7a69\u5b9a\u5ea6","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#5-2","title":"5. \u975c\u8108\u534a\u5f91\u8a08\u7b97 \u2192 (\u9577\u908a+\u77ed\u908a)/2","text":""},{"location":"handover/eric/IJV_tissue_model_setting/#_5","title":"\u5f8c\u7e8c","text":"<ol> <li>\u6e2c\u8a66\u6a21\u578b\u591a\u5927\u624d\u4e0d\u6703\u5f71\u97ff reflectance</li> <li>\u6e2c\u8a66\u591a\u500b sds \u6703\u4e0d\u6703\u5f71\u97ff reflectance\u3001\u8a08\u7b97\u53cd\u5c04\u7387\u5dee\u7570</li> </ol>"},{"location":"handover/eric/MD703_3D_printing_tutorial/","title":"MD703 3D\u7e6a\u5716\u82073D\u5217\u5370\u6559\u5b78","text":""},{"location":"handover/eric/MD703_3D_printing_tutorial/#tags","title":"tags: <code>\u5be6\u9a57\u5ba4\u7ba1\u7406</code>","text":"<p>Editor: \u8a31\u9038\u7fd4(R08)</p> <p>\u4e00\u822c\u5370\u51fa\u5be6\u9a57\u5ba4\u8981\u7528\u5230\u7684\u63a2\u982d(probe holder)\u9700\u8981\u7d93\u904e 2 \u500b\u6b65\u9a5f \u2192 3D \u7e6a\u5716\u8207 3D \u5217\u5370\u30023D \u7e6a\u5716\u7684\u76ee\u7684\u662f\u8a2d\u8a08\u63a2\u982d\u7684\u898f\u683c\uff0c\u5305\u542b\u5927\u5c0f\u3001\u5b54\u6d1e\u2026\u7b49\u7b49\u30023D \u5217\u5370\u7684\u76ee\u7684\u662f\u5c07\u7e6a\u88fd\u597d\u7684\u8a2d\u8a08\u5716\u5370\u51fa\u6210\u578b\uff0c\u5370\u51fa\u7684\u7269\u54c1\u80fd\u76f4\u63a5\u7528\u65bc\u76f8\u95dc\u5be6\u9a57\u3002</p>"},{"location":"handover/eric/MD703_3D_printing_tutorial/#3d","title":"3D \u7e6a\u5716","text":"<ol> <li>\u5de5\u5177\uff1aTinkercad</li> <li>\u4e3b\u8981\u4f7f\u7528\u529f\u80fd\uff1a\u7fa4\u7d44\u3001\u6316\u5b54\u3001\u5927\u5c0f\u8abf\u6574\u89e3\u6790\u5ea6\u8a2d\u5b9a</li> <li>\u7e6a\u5716\u5f8c\u7684\u6a94\u6848\u8f38\u51fa\u683c\u5f0f\uff1a.stl</li> <li>\u76f8\u95dc\u6559\u5b78\u7db2\u7ad9\uff1a<ul> <li>\u57fa\u672c\u529f\u80fd\u4ecb\u7d39</li> <li>\u5be6\u7528\u5c0f\u6280\u5de7</li> </ul> </li> </ol>"},{"location":"handover/eric/MD703_3D_printing_tutorial/#3d_1","title":"3D \u5217\u5370","text":"<ol> <li>\u8f38\u5165\u6a94\u6848\uff1a\u5403\u524d\u8ff0\u63d0\u5230\u7684 .stl \u6a94\u3002</li> <li>\u8f38\u51fa\u6a94\u6848\uff1a\u78ba\u8a8d\u5b8c 3D \u5217\u5370\u7684\u53c3\u6578\u6c92\u554f\u984c\u5f8c\u8f38\u51fa\u70ba .gcode \u6a94\u3002</li> <li>3D \u5217\u5370\u91cd\u8981\u53c3\u6578 (for black PLA - spyder)<ul> <li>\u5674\u982d\u6eab\u5ea6\uff1a220\u00b0C</li> <li>\u5e73\u53f0\u6eab\u5ea6\uff1a50\u00b0C</li> <li>\u5217\u5370\u54c1\u8cea\uff1asuper quality</li> <li>\u586b\u5145\u5bc6\u5ea6\uff1a\u5c0f holder \u53ef\u4ee5\u8a2d\u70ba 100%</li> <li>\u652f\u6490\u5bc6\u5ea6\uff1a\u5c0f holder \u53ef\u4ee5\u8a2d\u70ba 100%</li> <li>\u652f\u6490\u586b\u5145\u5f62\u614b\uff1aline</li> </ul> </li> <li>\u8edf\u9ad4\u8207 3D \u5217\u5370\u6a5f\u4f7f\u7528\u65b9\u6cd5\uff1a\u5b50\u4f73\u6587\u4ef6</li> </ol>"},{"location":"in_vivo_experiments/S1_preprocess_long/","title":"Preprocess Long Channel","text":"Analyze the long channel data collected from QEpro  <p>Below is the process structure of this jupyter notebook. The data you get containing three informations such as time, wavelength and intensity, represented as: $ I(t,\\lambda) $</p> <ul> <li><p>Format Setting</p> </li> <li><p>Initialize Processer Instance</p> </li> <li><p>Analyze in-vivo Data</p> <ul> <li>Get Background Data $$ \\bar{I}_{bg}(\\lambda) = \\frac{\\sum_{t=0}^{N} I_{bg}(t,\\lambda)}{N} $$</li> <li>Process Raw in-vivo Data<ul> <li>Remove spike</li> <li>Subtract background $$ \\hat{I}(t, \\lambda) = I_{NoSpike}(t,\\lambda) - \\bar{I}_{bg}(\\lambda)$$</li> <li>Moving average on spectrum $$ \\hat{I}_{MA}(t, \\lambda) = \\frac{\\sum_{\\hat{\\lambda}=\\lambda-0.5*WindowSize}^{\\lambda+0.5*WindowSize}\\hat{I}(t, \\hat{\\lambda})}{WindowSize}$$</li> <li>EMD</li> <li>Get diastolic and systolic peaks</li> <li>Plot Raw Data</li> <li>Plot all results</li> </ul> </li> </ul> </li> <li><p>Analyze Phantom Data for Calibration</p> <ul> <li>Load measured phantom data</li> <li>Load simulated phantom data</li> <li>fit measured phantom and simulated phantom</li> <li>Save fitting result as csv file</li> <li>Plot all measured phantom together</li> </ul> </li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom PyEMD import EMD \nfrom scipy.signal import convolve\nfrom scipy.signal import butter, lfilter, freqz\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib\nfrom utils import process_raw_data, process_phantom\nimport json\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\n</pre> import numpy as np import pandas as pd from PyEMD import EMD  from scipy.signal import convolve from scipy.signal import butter, lfilter, freqz import os import matplotlib.pyplot as plt import matplotlib as mpl import matplotlib from utils import process_raw_data, process_phantom import json # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_38016\\1177801975.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[2]: Copied! <pre># experimetn time setting\nbaseline_start = 0 # [sec]\nbaseline_end = 60 # [sec]\nexp_start = 60 # [sec]\nexp_end = 75 # [sec]\nrecovery_start = 75 # [sec]\nrecovery_end = 675 # [sec] \n\n# analyze setting\nmoving_window = 3 # [nm]\ntime_interval = 30 # [sec], show each n seconds window results\n\n# SDS2\ntime_resolution = 0.1 # [sec/point]\nusing_SDS = 20 # [mm]\ndate = '20230812'\nsubject = 'HW'\nphantom_measured_ID = ['2', '3', '4', '5']\nSDS_idx = 4  # SDS=20 mm, phantom simulated idx\n\nexp = 'VM'\nmother_folder_name = os.path.join(subject, \"SDS2\", date, exp)\nbackground_filenpath = os.path.join(\"dataset\", subject, \"SDS2\", date,'background.csv')\ndata_filepath = os.path.join(\"dataset\", subject, \"SDS2\", date, f'{subject}_{exp}.csv')\n</pre> # experimetn time setting baseline_start = 0 # [sec] baseline_end = 60 # [sec] exp_start = 60 # [sec] exp_end = 75 # [sec] recovery_start = 75 # [sec] recovery_end = 675 # [sec]   # analyze setting moving_window = 3 # [nm] time_interval = 30 # [sec], show each n seconds window results  # SDS2 time_resolution = 0.1 # [sec/point] using_SDS = 20 # [mm] date = '20230812' subject = 'HW' phantom_measured_ID = ['2', '3', '4', '5'] SDS_idx = 4  # SDS=20 mm, phantom simulated idx  exp = 'VM' mother_folder_name = os.path.join(subject, \"SDS2\", date, exp) background_filenpath = os.path.join(\"dataset\", subject, \"SDS2\", date,'background.csv') data_filepath = os.path.join(\"dataset\", subject, \"SDS2\", date, f'{subject}_{exp}.csv') In\u00a0[3]: Copied! <pre>process_raw_data.create_folder(mother_folder_name)\n\nProcesser = process_raw_data(baseline_start=baseline_start,\n                             baseline_end=baseline_end,\n                             exp_start=exp_start,\n                             exp_end=exp_end,\n                             recovery_start=recovery_start,\n                             recovery_end=recovery_end,\n                             time_resolution=time_resolution,\n                             time_interval=time_interval,\n                             mother_folder_name=mother_folder_name,\n                             using_SDS=using_SDS)\n</pre> process_raw_data.create_folder(mother_folder_name)  Processer = process_raw_data(baseline_start=baseline_start,                              baseline_end=baseline_end,                              exp_start=exp_start,                              exp_end=exp_end,                              recovery_start=recovery_start,                              recovery_end=recovery_end,                              time_resolution=time_resolution,                              time_interval=time_interval,                              mother_folder_name=mother_folder_name,                              using_SDS=using_SDS)  In\u00a0[4]: Copied! <pre># load data\nraw_bg, total_wl = process_raw_data.read_file(background_filenpath)\n\n# select range 700nm~850nm\nidx_700nm = np.argmin(np.abs(total_wl-700))\nidx_850nm = np.argmin(np.abs(total_wl-850))\n\n# get background data\nraw_bg, total_wl = raw_bg[:, idx_700nm:idx_850nm], total_wl[idx_700nm:idx_850nm]\nbg_no_spike = Processer.remove_spike(wl=total_wl, \n                                    data=raw_bg, \n                                    normalStdTimes=3, \n                                    ts=0)\nbg_time_mean = bg_no_spike.mean(0) # averaged at time axis --&gt; get spectrum (wavelength*intensity)\nbg_wl_mean = bg_no_spike.mean(1) # averaged at wavelength axis --&gt; get signal (time*intensity)\n\n# plot background data\nProcesser.plot_all_time_spectrum(data=raw_bg,\n                    wavelength=total_wl,\n                    name='background',\n                    start_time=0,\n                    end_time=round(raw_bg.shape[0]*time_resolution),\n                    is_show = True)\nProcesser.plot_signal(data=raw_bg,\n            name='background',\n            start_time=0,\n            end_time=round(raw_bg.shape[0]*time_resolution),\n            is_show = True)\nfor ts in range(0,round(raw_bg.shape[0]*time_resolution),time_interval):\n    td = ts + time_interval\n    if ts == 0:\n            is_show = True\n    else:\n            is_show = False\n    Processer.plot_signal(data=raw_bg,\n            name='background',\n            start_time=ts,\n            end_time=td,\n            is_show = is_show)\n</pre> # load data raw_bg, total_wl = process_raw_data.read_file(background_filenpath)  # select range 700nm~850nm idx_700nm = np.argmin(np.abs(total_wl-700)) idx_850nm = np.argmin(np.abs(total_wl-850))  # get background data raw_bg, total_wl = raw_bg[:, idx_700nm:idx_850nm], total_wl[idx_700nm:idx_850nm] bg_no_spike = Processer.remove_spike(wl=total_wl,                                      data=raw_bg,                                      normalStdTimes=3,                                      ts=0) bg_time_mean = bg_no_spike.mean(0) # averaged at time axis --&gt; get spectrum (wavelength*intensity) bg_wl_mean = bg_no_spike.mean(1) # averaged at wavelength axis --&gt; get signal (time*intensity)  # plot background data Processer.plot_all_time_spectrum(data=raw_bg,                     wavelength=total_wl,                     name='background',                     start_time=0,                     end_time=round(raw_bg.shape[0]*time_resolution),                     is_show = True) Processer.plot_signal(data=raw_bg,             name='background',             start_time=0,             end_time=round(raw_bg.shape[0]*time_resolution),             is_show = True) for ts in range(0,round(raw_bg.shape[0]*time_resolution),time_interval):     td = ts + time_interval     if ts == 0:             is_show = True     else:             is_show = False     Processer.plot_signal(data=raw_bg,             name='background',             start_time=ts,             end_time=td,             is_show = is_show) <pre>target = [0, 1, 2, 5, 6, 7, 8, 10, 11, 13, 14, 16, 17, 19, 20, 22, 23, 24, 25, 26, 27, 29, 31, 32, 34, 35, 37, 38, 41, 43, 44, 45, 46, 47, 48, 49, 53, 54, 55, 56, 57, 58, 59, 60, 61, 65, 67, 68, 69, 71, 73, 77, 78, 80, 82, 83, 84, 85, 86, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 109, 111, 113, 115, 116, 118, 120, 122, 123, 124, 125, 126, 128, 129, 131, 132, 133, 134, 142, 143, 144, 146, 147, 148, 149, 151, 152, 153, 154, 155, 157, 159, 160, 161, 163, 165, 166, 167, 168, 170, 171, 172, 173, 174, 180, 182, 183, 184, 185, 186, 189, 192, 193, 194, 196, 199, 200, 201, 203, 204, 206, 207, 208, 210, 211, 213, 214, 215, 217, 221, 222, 224, 225, 229, 230, 232, 233, 234, 235, 236, 239, 240, 241, 242, 244, 245, 247, 248, 251, 254, 255, 256, 258, 259, 260, 262, 263, 264, 265, 267, 268, 271, 272, 273, 276, 278, 280, 283, 285, 286, 287, 289, 290, 291, 292, 293, 297, 299, 301, 302, 303, 304, 305, 306, 307, 308, 310, 313, 314, 316, 317, 319, 320, 324, 327, 328, 329, 330, 331, 332, 333, 334, 336, 337, 338, 339, 340, 341, 342, 345, 346, 348, 349, 352, 355, 356, 357, 359, 361, 362, 364, 365, 366, 368, 373, 376, 377, 378, 380, 381, 384, 386, 389, 390, 391, 392, 393, 394, 395, 396, 399, 400, 401, 402, 403, 405, 406, 407, 410, 411, 416, 420, 422, 424, 425, 426, 427, 428, 429, 430, 434, 435, 436, 437, 438, 439, 440, 441, 442, 444, 445, 447, 449, 450, 451, 452, 453, 454, 456, 460]\n</pre> In\u00a0[5]: Copied! <pre># load raw data\nraw_data, total_wl = Processer.read_file(data_filepath)\n\n# select range 700nm~850nm\nidx_700nm = np.argmin(np.abs(total_wl-700))\nidx_850nm = np.argmin(np.abs(total_wl-850))\nraw_data, total_wl = raw_data[:round(recovery_end/time_resolution), idx_700nm:idx_850nm], total_wl[idx_700nm:idx_850nm] # clipped signal window\n\n# remove spike\ndata_no_spike = raw_data.copy()\nfor ts in range(0, recovery_end, time_interval):\n    td = ts + time_interval\n    if ((ts&gt;=exp_start) &amp; (ts&lt;=exp_end)) or ((td&gt;=exp_start) &amp; (td&lt;=exp_end)): # while in the experiment period, don't remove spike\n        pass # do nothing\n    else:\n        data_no_spike[round(ts/time_resolution):round(td/time_resolution)] = Processer.remove_spike(total_wl, \n                                                                                                    raw_data[round(ts/time_resolution):round(td/time_resolution)], \n                                                                                                    normalStdTimes=5, \n                                                                                                    ts=ts)\n        \n# subtract background\ndata_sub_bg = data_no_spike - bg_time_mean\n\n# moving average\nfor i in range(data_sub_bg.shape[0]):\n    if i == 0:\n        moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_sub_bg[i,:])\n        moving_avg_I_data = moving_avg_I_data.reshape(1,-1)\n        data_moving_avg = moving_avg_I_data\n    else:\n        moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_sub_bg[i,:])\n        moving_avg_I_data = moving_avg_I_data.reshape(1,-1) \n        data_moving_avg = np.concatenate((data_moving_avg,moving_avg_I_data))\n\n## EMD\ndata_EMD = data_moving_avg.copy()\n# remove all-time signal based at first\nimfs = EMD().emd(data_moving_avg.mean(axis=1))\nimfs[-1] -= imfs[-1].mean()\nartifact = imfs[-1] \n# remove artifact\nfor art in artifact:\n    data_EMD -= art.reshape(-1, 1)\n\n# detect peak \n# get straight signal to find peaks\nstraight_signal = data_moving_avg.copy()\n# remove all-time signal based at first\nimfs = EMD().emd(data_moving_avg.mean(axis=1))\nimfs[-1] -= imfs[-1].mean()\nartifact = imfs[2:] \n# remove artifact\nfor art in artifact:\n    straight_signal -= art.reshape(-1, 1)\nis_peak = np.zeros(straight_signal.shape[0])\nfor ts in range(0, recovery_end, time_interval):\n    td = ts+time_interval\n    data_signal = straight_signal[round(ts/time_resolution):round(td/time_resolution), :].mean(axis=1)\n    max_idx, min_idx = process_raw_data.get_peak_final(data_signal)\n    is_peak[min_idx + round(ts/time_resolution)] = -1\n    is_peak[max_idx + round(ts/time_resolution)] = 1\n\n# save result \nsave_result = {}\ntime = [i*time_resolution for i in range(data_moving_avg.shape[0])]\nsave_result['time(s)'] = time\nsave_result['peak'] = is_peak # max:+1, min:-1\nfor idx, using_wl in enumerate(moving_avg_wl_data):\n    save_result[f'{using_wl}nm'] = data_EMD[:,idx]\nsave_result = pd.DataFrame(save_result)\nsave_result.to_csv(os.path.join(\"dataset\", mother_folder_name, f\"in_vivo_result_{exp}.csv\"), index=False)\n</pre>  # load raw data raw_data, total_wl = Processer.read_file(data_filepath)  # select range 700nm~850nm idx_700nm = np.argmin(np.abs(total_wl-700)) idx_850nm = np.argmin(np.abs(total_wl-850)) raw_data, total_wl = raw_data[:round(recovery_end/time_resolution), idx_700nm:idx_850nm], total_wl[idx_700nm:idx_850nm] # clipped signal window  # remove spike data_no_spike = raw_data.copy() for ts in range(0, recovery_end, time_interval):     td = ts + time_interval     if ((ts&gt;=exp_start) &amp; (ts&lt;=exp_end)) or ((td&gt;=exp_start) &amp; (td&lt;=exp_end)): # while in the experiment period, don't remove spike         pass # do nothing     else:         data_no_spike[round(ts/time_resolution):round(td/time_resolution)] = Processer.remove_spike(total_wl,                                                                                                      raw_data[round(ts/time_resolution):round(td/time_resolution)],                                                                                                      normalStdTimes=5,                                                                                                      ts=ts)          # subtract background data_sub_bg = data_no_spike - bg_time_mean  # moving average for i in range(data_sub_bg.shape[0]):     if i == 0:         moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_sub_bg[i,:])         moving_avg_I_data = moving_avg_I_data.reshape(1,-1)         data_moving_avg = moving_avg_I_data     else:         moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_sub_bg[i,:])         moving_avg_I_data = moving_avg_I_data.reshape(1,-1)          data_moving_avg = np.concatenate((data_moving_avg,moving_avg_I_data))  ## EMD data_EMD = data_moving_avg.copy() # remove all-time signal based at first imfs = EMD().emd(data_moving_avg.mean(axis=1)) imfs[-1] -= imfs[-1].mean() artifact = imfs[-1]  # remove artifact for art in artifact:     data_EMD -= art.reshape(-1, 1)  # detect peak  # get straight signal to find peaks straight_signal = data_moving_avg.copy() # remove all-time signal based at first imfs = EMD().emd(data_moving_avg.mean(axis=1)) imfs[-1] -= imfs[-1].mean() artifact = imfs[2:]  # remove artifact for art in artifact:     straight_signal -= art.reshape(-1, 1) is_peak = np.zeros(straight_signal.shape[0]) for ts in range(0, recovery_end, time_interval):     td = ts+time_interval     data_signal = straight_signal[round(ts/time_resolution):round(td/time_resolution), :].mean(axis=1)     max_idx, min_idx = process_raw_data.get_peak_final(data_signal)     is_peak[min_idx + round(ts/time_resolution)] = -1     is_peak[max_idx + round(ts/time_resolution)] = 1  # save result  save_result = {} time = [i*time_resolution for i in range(data_moving_avg.shape[0])] save_result['time(s)'] = time save_result['peak'] = is_peak # max:+1, min:-1 for idx, using_wl in enumerate(moving_avg_wl_data):     save_result[f'{using_wl}nm'] = data_EMD[:,idx] save_result = pd.DataFrame(save_result) save_result.to_csv(os.path.join(\"dataset\", mother_folder_name, f\"in_vivo_result_{exp}.csv\"), index=False)  <pre>target = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = [9]\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = [89]\ntarget = []\ntarget = [9]\ntarget = []\ntarget = []\ntarget = [41]\ntarget = [21]\ntarget = []\ntarget = []\n</pre> In\u00a0[6]: Copied! <pre>plt.rcParams.update({'font.size': 20})\nplt.figure(figsize=(20,8))\ntime = [i*time_resolution for i in range(raw_data.shape[0])]\nplt.plot(time, raw_data.mean(1))\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label=f'{exp}_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', mother_folder_name, 'time', 'raw_all_time_SDS20.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.rcParams.update({'font.size': 20}) plt.figure(figsize=(20,8)) time = [i*time_resolution for i in range(raw_data.shape[0])] plt.plot(time, raw_data.mean(1)) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label=f'{exp}_end') plt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel(\"time [sec]\") plt.ylabel(\"Intensity\") plt.title('SDS20mm') plt.savefig(os.path.join('pic', mother_folder_name, 'time', 'raw_all_time_SDS20.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[7]: Copied! <pre>max_id = np.where(save_result['peak']==1)[0]\nmin_id = np.where(save_result['peak']==-1)[0]\n\nmax_id = max_id[np.where(max_id&lt;round(recovery_end/time_resolution))[0]]\nmin_id = min_id[np.where(min_id&lt;round(recovery_end/time_resolution))[0]]\n\nplt.figure(figsize=(20,8))\ntime = save_result['time(s)']\nplt.plot(time, data_EMD.mean(1))\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label=f'{exp}_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.plot(time[max_id], data_EMD.mean(1)[max_id], 'r.')\nplt.plot(time[min_id], data_EMD.mean(1)[min_id], 'b.')\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', mother_folder_name, 'time', 'processed_all_time_SDS20.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre>  max_id = np.where(save_result['peak']==1)[0] min_id = np.where(save_result['peak']==-1)[0]  max_id = max_id[np.where(max_id In\u00a0[8]: Copied! <pre>plt.rcParams.update({'font.size': 12})\nProcesser.long_plot_all_fig(data=raw_data, \n            wavelength=total_wl,\n            name='raw')\nProcesser.long_plot_all_fig(data=data_sub_bg, \n            wavelength=total_wl,\n            name='remove_spike_and_bg')\nProcesser.long_plot_all_fig(data=data_moving_avg, \n            wavelength=moving_avg_wl_data,\n            name='moving_average')\n    \nProcesser.long_plot_all_fig(data=data_EMD, \n            wavelength=moving_avg_wl_data,\n            name='EMD')\n\nProcesser.plot_Rmax_Rmin(data=data_EMD,\n                         wavelength=moving_avg_wl_data,\n                         max_idx_Set=max_idx,\n                         min_idx_Set=min_idx,\n                         name=\"get_peak\",\n                         start_time=0,\n                         end_time=recovery_end)\n\nfor using_num_IMF in [1,2,3]:\n    Processer.plot_time_EMD(data=data_moving_avg,\n                    name='EMD',\n                    start_time=0,\n                    end_time=recovery_end,\n                    using_num_IMF=using_num_IMF)\n\n    Processer.plot_compare_time_EMD(data=data_moving_avg,\n                        name='compare',\n                        start_time=0,\n                        end_time=recovery_end,\n                        using_num_IMF=using_num_IMF)\n\n    \n    for ts in range(0,recovery_end,time_interval):\n        td = ts + time_interval\n        Processer.plot_time_EMD(data=data_moving_avg,\n                    name='EMD',\n                    start_time=ts,\n                    end_time=td,\n                    using_num_IMF=using_num_IMF)\n\n        Processer.plot_compare_time_EMD(data=data_moving_avg,\n                            name='compare',\n                            start_time=ts,\n                            end_time=td,\n                            using_num_IMF=using_num_IMF)\n</pre> plt.rcParams.update({'font.size': 12}) Processer.long_plot_all_fig(data=raw_data,              wavelength=total_wl,             name='raw') Processer.long_plot_all_fig(data=data_sub_bg,              wavelength=total_wl,             name='remove_spike_and_bg') Processer.long_plot_all_fig(data=data_moving_avg,              wavelength=moving_avg_wl_data,             name='moving_average')      Processer.long_plot_all_fig(data=data_EMD,              wavelength=moving_avg_wl_data,             name='EMD')  Processer.plot_Rmax_Rmin(data=data_EMD,                          wavelength=moving_avg_wl_data,                          max_idx_Set=max_idx,                          min_idx_Set=min_idx,                          name=\"get_peak\",                          start_time=0,                          end_time=recovery_end)  for using_num_IMF in [1,2,3]:     Processer.plot_time_EMD(data=data_moving_avg,                     name='EMD',                     start_time=0,                     end_time=recovery_end,                     using_num_IMF=using_num_IMF)      Processer.plot_compare_time_EMD(data=data_moving_avg,                         name='compare',                         start_time=0,                         end_time=recovery_end,                         using_num_IMF=using_num_IMF)           for ts in range(0,recovery_end,time_interval):         td = ts + time_interval         Processer.plot_time_EMD(data=data_moving_avg,                     name='EMD',                     start_time=ts,                     end_time=td,                     using_num_IMF=using_num_IMF)          Processer.plot_compare_time_EMD(data=data_moving_avg,                             name='compare',                             start_time=ts,                             end_time=td,                             using_num_IMF=using_num_IMF) <pre>d:\\ijv_code_new\\IJV-Project\\in_vivo_experiments\\utils.py:460: RuntimeWarning: Mean of empty slice.\n  R_max_spec = data[max_idx_subset, :].mean(0)\nd:\\ijv_code_new\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\nd:\\ijv_code_new\\IJV-Project\\in_vivo_experiments\\utils.py:461: RuntimeWarning: Mean of empty slice.\n  R_min_spec = data[min_idx_subset, :].mean(0)\n</pre> In\u00a0[9]: Copied! <pre># get background Nx : time, Ny : intensity\nbackground = pd.read_csv(os.path.join(\"dataset\", subject, 'SDS2', date, \"background.csv\"))\nused_wl = []\nfor k in background.keys().to_list()[1:]:\n    used_wl += [float(k)]\nbackground = background.to_numpy()[:,1:]\nbackground = np.array(background, dtype=np.float64)\n\nos.makedirs(os.path.join(\"pic\", mother_folder_name, 'phantom', 'background'), exist_ok=True)\nremove_spike_background = process_phantom.remove_spike(used_wl, background, \n                                                       normalStdTimes=6,\n                                                       savepath=os.path.join(\"pic\", mother_folder_name, 'phantom', 'background')) # remove spike\ntime_mean_background = remove_spike_background.mean(axis=0) # mean of background signal\n\n# get measured phantom data\nphantom_data = {} # CHIK3456\nfor ID in phantom_measured_ID:\n    # define plot savepath\n    os.makedirs(os.path.join(\"pic\", mother_folder_name, 'phantom', ID), exist_ok=True)\n    \n    # import measured data\n    data = pd.read_csv(os.path.join('dataset', subject, 'SDS2', date, f'{ID}.csv'))\n    data = data.to_numpy()[:,1:]\n    data = np.array(data, dtype=np.float64)\n    \n    # remove spike\n    remove_spike_data = process_phantom.remove_spike(used_wl, data, \n                                                     normalStdTimes=5, \n                                                     savepath=os.path.join(\"pic\", mother_folder_name,'phantom', ID)) # remove spike\n    time_mean_data = remove_spike_data.mean(0) # mean of measured signal\n    \n    # subtract background\n    time_mean_data_sub_background = time_mean_data - time_mean_background\n    \n    # Do moving avg of spectrum\n    moving_avg_I_data, moving_avg_wl_data = process_phantom.moving_avg(used_wl_bandwidth = 3,\n                                                       used_wl = used_wl,\n                                                       time_mean_arr = time_mean_data_sub_background)\n    \n    phantom_data[ID] = moving_avg_I_data\n</pre> # get background Nx : time, Ny : intensity background = pd.read_csv(os.path.join(\"dataset\", subject, 'SDS2', date, \"background.csv\")) used_wl = [] for k in background.keys().to_list()[1:]:     used_wl += [float(k)] background = background.to_numpy()[:,1:] background = np.array(background, dtype=np.float64)  os.makedirs(os.path.join(\"pic\", mother_folder_name, 'phantom', 'background'), exist_ok=True) remove_spike_background = process_phantom.remove_spike(used_wl, background,                                                         normalStdTimes=6,                                                        savepath=os.path.join(\"pic\", mother_folder_name, 'phantom', 'background')) # remove spike time_mean_background = remove_spike_background.mean(axis=0) # mean of background signal  # get measured phantom data phantom_data = {} # CHIK3456 for ID in phantom_measured_ID:     # define plot savepath     os.makedirs(os.path.join(\"pic\", mother_folder_name, 'phantom', ID), exist_ok=True)          # import measured data     data = pd.read_csv(os.path.join('dataset', subject, 'SDS2', date, f'{ID}.csv'))     data = data.to_numpy()[:,1:]     data = np.array(data, dtype=np.float64)          # remove spike     remove_spike_data = process_phantom.remove_spike(used_wl, data,                                                       normalStdTimes=5,                                                       savepath=os.path.join(\"pic\", mother_folder_name,'phantom', ID)) # remove spike     time_mean_data = remove_spike_data.mean(0) # mean of measured signal          # subtract background     time_mean_data_sub_background = time_mean_data - time_mean_background          # Do moving avg of spectrum     moving_avg_I_data, moving_avg_wl_data = process_phantom.moving_avg(used_wl_bandwidth = 3,                                                        used_wl = used_wl,                                                        time_mean_arr = time_mean_data_sub_background)          phantom_data[ID] = moving_avg_I_data <pre>target = []\ntarget = []\ntarget = []\ntarget = [0, 7, 18, 491]\ntarget = [214]\n</pre> In\u00a0[10]: Copied! <pre># load used wavelength\nwith open(os.path.join(\"OPs_used\", \"wavelength.json\"), \"r\") as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\n\n# binning meaurement wavelength \nbinning_wavlength = 2 # nm\nfind_wl_idx = {}\nfor used_wl in wavelength:\n    row = []\n    for idx, test_wl in enumerate(moving_avg_wl_data):\n        if abs(test_wl-used_wl) &lt; binning_wavlength:\n            row += [idx]          \n    find_wl_idx[used_wl] = row\nfind_wl_idx\n</pre> # load used wavelength with open(os.path.join(\"OPs_used\", \"wavelength.json\"), \"r\") as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength']  # binning meaurement wavelength  binning_wavlength = 2 # nm find_wl_idx = {} for used_wl in wavelength:     row = []     for idx, test_wl in enumerate(moving_avg_wl_data):         if abs(test_wl-used_wl) &lt; binning_wavlength:             row += [idx]               find_wl_idx[used_wl] = row find_wl_idx Out[10]: <pre>{700: [95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105],\n 710: [122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132],\n 717: [141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151],\n 725: [163, 164, 165, 166, 167, 168, 169, 170, 171, 172],\n 732: [181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191],\n 740: [203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213],\n 743: [211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221],\n 748: [225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235],\n 751: [233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243],\n 753: [238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248],\n 758: [252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262],\n 763: [266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276],\n 768: [279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289],\n 780: [312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322],\n 792: [346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356],\n 798: [362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372],\n 805: [382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392],\n 815: [409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420],\n 830: [451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462],\n 850: [508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518]}</pre> In\u00a0[11]: Copied! <pre>matplotlib.rcParams.update({'font.size': 18})\n## Get the same simulated wavelength point from measured phantom\nmeasured_phantom_data = []\nplt.figure(figsize=(12,8))\nfor idx, out_k in enumerate(phantom_data.keys()):\n    data = phantom_data[out_k]\n    avg_wl_as_data = []\n    for k in find_wl_idx.keys():\n        average_idx = find_wl_idx[k]\n        avg_wl_as_data += [data[average_idx].mean()]\n    measured_phantom_data.append(avg_wl_as_data)\n    plt.plot(wavelength, avg_wl_as_data, 'o--', label=f'phantom_{phantom_measured_ID[idx]}')\nplt.title(\"SDS=20mm, measured phantom spectrum\")\nplt.xlabel('wavelength (nm)')\nplt.ylabel('intensity')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\nmeasured_phantom_data = np.array(measured_phantom_data)\n\n## Get simulated phantom data\nsim_phantom_data = []\nplt.figure(figsize=(12,8))\nfor c in phantom_measured_ID:\n    data = np.load(os.path.join(\"dataset\", \"phantom_simulated\", f'{c}.npy'))\n    sim_phantom_data.append(data[:,SDS_idx].tolist())\n    plt.plot(wavelength, data[:,SDS_idx], 'o--',label=f'phantom_{c}')\nplt.title(\"SDS=20mm, simulated phantom spectrum\")\nplt.xlabel('wavelength (nm)')\nplt.ylabel('intensity')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"simulated_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\nsim_phantom_data = np.array(sim_phantom_data)\n</pre> matplotlib.rcParams.update({'font.size': 18}) ## Get the same simulated wavelength point from measured phantom measured_phantom_data = [] plt.figure(figsize=(12,8)) for idx, out_k in enumerate(phantom_data.keys()):     data = phantom_data[out_k]     avg_wl_as_data = []     for k in find_wl_idx.keys():         average_idx = find_wl_idx[k]         avg_wl_as_data += [data[average_idx].mean()]     measured_phantom_data.append(avg_wl_as_data)     plt.plot(wavelength, avg_wl_as_data, 'o--', label=f'phantom_{phantom_measured_ID[idx]}') plt.title(\"SDS=20mm, measured phantom spectrum\") plt.xlabel('wavelength (nm)') plt.ylabel('intensity') plt.legend() plt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() measured_phantom_data = np.array(measured_phantom_data)  ## Get simulated phantom data sim_phantom_data = [] plt.figure(figsize=(12,8)) for c in phantom_measured_ID:     data = np.load(os.path.join(\"dataset\", \"phantom_simulated\", f'{c}.npy'))     sim_phantom_data.append(data[:,SDS_idx].tolist())     plt.plot(wavelength, data[:,SDS_idx], 'o--',label=f'phantom_{c}') plt.title(\"SDS=20mm, simulated phantom spectrum\") plt.xlabel('wavelength (nm)') plt.ylabel('intensity') plt.legend() plt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"simulated_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() sim_phantom_data = np.array(sim_phantom_data)  In\u00a0[12]: Copied! <pre>fig = plt.figure(figsize=(18,12))\nfig.suptitle(f\"SDS = {using_SDS} mm\", fontsize=16)\ncount = 1\nfor idx, used_wl in enumerate(wavelength):\n    ## fit measured phantom and simulated phantom\n    z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)\n    plotx = np.linspace(measured_phantom_data[-1, idx]*0.8,  measured_phantom_data[0, idx]*1.2,100)\n    ploty = plotx*z[0] + z[1]\n    calibrate_data = measured_phantom_data[:, idx]*z[0] + z[1]\n    R_square = process_phantom.cal_R_square(calibrate_data, sim_phantom_data[:,idx]) # cal R square\n    \n    ## plot result\n    ax = plt.subplot(5,4, count)\n    ax.set_title(f\"@wavelength={used_wl} nm\")\n    ax.set_title(f'{used_wl}nm, $R^{2}$={R_square:.2f}')\n    for ID_idx, ID in enumerate(phantom_measured_ID):\n        ax.plot(measured_phantom_data[ID_idx, idx], sim_phantom_data[ID_idx,idx], 's', label=f'phantom_{ID}')\n    ax.plot(plotx, ploty, '--')\n    ax.set_xlabel(\"measure intensity\")\n    ax.set_ylabel(\"sim intensity\")\n    count += 1\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                    fancybox=True, shadow=True)\nplt.tight_layout()\nplt.savefig(os.path.join('pic', mother_folder_name, 'phantom', \"all.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> fig = plt.figure(figsize=(18,12)) fig.suptitle(f\"SDS = {using_SDS} mm\", fontsize=16) count = 1 for idx, used_wl in enumerate(wavelength):     ## fit measured phantom and simulated phantom     z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)     plotx = np.linspace(measured_phantom_data[-1, idx]*0.8,  measured_phantom_data[0, idx]*1.2,100)     ploty = plotx*z[0] + z[1]     calibrate_data = measured_phantom_data[:, idx]*z[0] + z[1]     R_square = process_phantom.cal_R_square(calibrate_data, sim_phantom_data[:,idx]) # cal R square          ## plot result     ax = plt.subplot(5,4, count)     ax.set_title(f\"@wavelength={used_wl} nm\")     ax.set_title(f'{used_wl}nm, $R^{2}$={R_square:.2f}')     for ID_idx, ID in enumerate(phantom_measured_ID):         ax.plot(measured_phantom_data[ID_idx, idx], sim_phantom_data[ID_idx,idx], 's', label=f'phantom_{ID}')     ax.plot(plotx, ploty, '--')     ax.set_xlabel(\"measure intensity\")     ax.set_ylabel(\"sim intensity\")     count += 1 plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                     fancybox=True, shadow=True) plt.tight_layout() plt.savefig(os.path.join('pic', mother_folder_name, 'phantom', \"all.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[13]: Copied! <pre>result = {}\nfor idx, used_wl in enumerate(wavelength):\n    z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)\n    result[used_wl] = z\nresult = pd.DataFrame(result)\nos.makedirs(os.path.join(\"dataset\",  subject, 'calibration_result', date) , exist_ok=True)\nresult.to_csv(os.path.join(\"dataset\",  subject, 'calibration_result', date, \"calibrate_SDS_2.csv\"), index=False)\n</pre> result = {} for idx, used_wl in enumerate(wavelength):     z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)     result[used_wl] = z result = pd.DataFrame(result) os.makedirs(os.path.join(\"dataset\",  subject, 'calibration_result', date) , exist_ok=True) result.to_csv(os.path.join(\"dataset\",  subject, 'calibration_result', date, \"calibrate_SDS_2.csv\"), index=False) In\u00a0[14]: Copied! <pre>for idx, k in enumerate(phantom_data.keys()):\n    data = phantom_data[k]\n    plt.plot(moving_avg_wl_data, data, label=f'phantom_{phantom_measured_ID[idx]}')\nplt.title('phantom spectrum')\nplt.xlabel('wavelength (nm)')\nplt.ylabel('intensity')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_2345_phantom_result.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> for idx, k in enumerate(phantom_data.keys()):     data = phantom_data[k]     plt.plot(moving_avg_wl_data, data, label=f'phantom_{phantom_measured_ID[idx]}') plt.title('phantom spectrum') plt.xlabel('wavelength (nm)') plt.ylabel('intensity') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                 fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_2345_phantom_result.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()"},{"location":"in_vivo_experiments/S1_preprocess_long/#format-setting","title":"Format Setting\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#initialize-processer-instance","title":"Initialize Processer Instance\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#get-background-data","title":"Get Background Data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#process-raw-in-vivo-data","title":"Process Raw in-vivo Data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#plot-raw-data","title":"Plot Raw Data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#plot-the-results","title":"Plot the Results\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#analyze-phantom-data-for-calibration","title":"Analyze Phantom Data for Calibration\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#load-measured-phantom-data","title":"Load measured phantom data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#load-simulated-phantom-data","title":"Load simulated phantom data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#fit-measured-phantom-and-simulated-phantom","title":"Fit measured phantom and simulated phantom\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#save-fitting-result-as-csv-file","title":"Save fitting result as csv file\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_long/#plot-all-measured-phantom-together","title":"Plot all measured phantom together\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/","title":"Preprocess Short Channel","text":"Analyze the short channel data collected from Spectrapro+EMDCCD  <p>Below is the process structure of this jupyter notebook. The data you get containing three informations such as time, wavelength and intensity, represented as: $ I(t,\\lambda) $</p> <p>Note that when you do in-vivo experiment, if you use binning, no need to do extra moving average on spectrum. Like this example notebook. However, if you don't use binning, you need to adjust the moving average on spectrum decreaing the noise on the spectrum</p> <ul> <li><p>Format Setting</p> </li> <li><p>Process Spectrapro+EMCCD raw data (image to csv file) (on image mode)</p> <ul> <li>Get Background Data $$ \\bar{I}_{bg}(\\lambda) = \\frac{\\sum_{t=0}^{N} I_{bg}(t,\\lambda)}{N} $$</li> <li>Subtract background $$ \\hat{I}(t, \\lambda) = I_{NoSpike}(t,\\lambda) - \\bar{I}_{bg}(\\lambda)$$</li> </ul> </li> <li><p>Sync Spectrapro image data to QEpro format csv file (time*wavelength)</p> </li> <li><p>Initialize Processer Instance</p> </li> <li><p>Analyze in-vivo Data</p> <ul> <li>Process Raw in-vivo Data<ul> <li>Load sync in-vivo data</li> <li>Remove spike</li> <li>EMD</li> <li>Get diastolic and systolic peaks</li> <li>Plot Raw Data</li> <li>Plot all results</li> </ul> </li> </ul> </li> <li><p>Analyze Phantom Data for Calibration</p> <ul> <li>Process spectrapro+EMCCD raw phantom data<ul> <li>Sync spectraprofile to QEpro file</li> </ul> </li> <li>Load sync measured phantom data</li> <li>Load simulated phantom data</li> <li>fit measured phantom and simulated phantom</li> <li>Save fitting result as csv file</li> <li>Plot all measured phantom together</li> </ul> </li> </ul> In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nfrom PyEMD import EMD \nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib\nfrom utils import process_raw_data, process_phantom\nfrom glob import glob\nfrom tqdm import tqdm\nimport matplotlib.ticker as mtick\nimport json\nfrom scipy.interpolate import interp1d\nfrom natsort import natsorted \n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\n</pre> import numpy as np import pandas as pd from PyEMD import EMD  import os import matplotlib.pyplot as plt import matplotlib as mpl import matplotlib from utils import process_raw_data, process_phantom from glob import glob from tqdm import tqdm import matplotlib.ticker as mtick import json from scipy.interpolate import interp1d from natsort import natsorted  # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_15812\\3514809.py:17: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[11]: Copied! <pre>subject_name = 'KB'\nexp = 'VM'\ndate = '20230912'\npname = ['VM']\n\ntime_resolution = 0.10404 # [sec] # 0.10404, 0.10129\nusing_SDS = 10 # [mm]\nphantom_measured_ID = ['22', '33', '44', '55']\nphantom_simulated_ID = ['2', '3', '4', '5']\nSDS_idx = 3  # SDS=10 mm, phantom simulated idx\n\n## exp setting\nbaseline_start = 0 # [sec]\nbaseline_end = 60 # [sec]\nexp_start = 60 # [sec]\nexp_end = 75 # [sec]\nrecovery_start = 75 # [sec]\nrecovery_end = 670 # [sec] \n\n# setting\nmoving_window = 3 # [nm]\ntime_interval = 30 # [sec]\n\n## EMCCD setting \nifolder = os.path.join('dataset', subject_name, 'SDS1', f'{date}')\n# used short channel\nused_ch = 'ch3'\nSDS_LIST = [4.5, 7.5, 10.5]\nofolder = os.path.join('dataset', subject_name, 'SDS1', f'm_out_{date}_{exp}')\nos.makedirs(ofolder, exist_ok=True)\n\n# Wavelength calibration factor defined : y = ax + b\na = 3.0375\nb = 331.88\n\n# Choose wavelength\nstart_wl = 119\nstop_wl = 183\n\n# Choose fiber output range\nrow_choose = ((2, 5), (6, 9), (11, 14))\n\nmother_folder_name = os.path.join(subject_name, \"SDS1\", date, exp)\nbackground_filenpath = os.path.join(\"dataset\", subject_name, \"SDS1\", date,'background.csv')\ndata_filepath = os.path.join(ofolder, f'{subject_name}_SDS1_sync.csv')\n</pre> subject_name = 'KB' exp = 'VM' date = '20230912' pname = ['VM']  time_resolution = 0.10404 # [sec] # 0.10404, 0.10129 using_SDS = 10 # [mm] phantom_measured_ID = ['22', '33', '44', '55'] phantom_simulated_ID = ['2', '3', '4', '5'] SDS_idx = 3  # SDS=10 mm, phantom simulated idx  ## exp setting baseline_start = 0 # [sec] baseline_end = 60 # [sec] exp_start = 60 # [sec] exp_end = 75 # [sec] recovery_start = 75 # [sec] recovery_end = 670 # [sec]   # setting moving_window = 3 # [nm] time_interval = 30 # [sec]  ## EMCCD setting  ifolder = os.path.join('dataset', subject_name, 'SDS1', f'{date}') # used short channel used_ch = 'ch3' SDS_LIST = [4.5, 7.5, 10.5] ofolder = os.path.join('dataset', subject_name, 'SDS1', f'm_out_{date}_{exp}') os.makedirs(ofolder, exist_ok=True)  # Wavelength calibration factor defined : y = ax + b a = 3.0375 b = 331.88  # Choose wavelength start_wl = 119 stop_wl = 183  # Choose fiber output range row_choose = ((2, 5), (6, 9), (11, 14))  mother_folder_name = os.path.join(subject_name, \"SDS1\", date, exp) background_filenpath = os.path.join(\"dataset\", subject_name, \"SDS1\", date,'background.csv') data_filepath = os.path.join(ofolder, f'{subject_name}_SDS1_sync.csv') In\u00a0[3]: Copied! <pre># %% Load target spectrum\nfolder_list = glob(os.path.join(ifolder, '*'))\nstdname = 'standard0.1'\nif any(stdname in folder_list[i] for i in range(len(folder_list))):\n    path_det_bg = glob(os.path.join(ifolder, stdname, 'background*'))\n    path_det_bg = natsorted(path_det_bg)\n    \n    path_det = glob(os.path.join(ifolder, stdname, 'std*')) \n    path_det = natsorted(path_det)\n    \n    bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))\n    df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_mean.csv'), index = False)\n    df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch1.csv'), index = False)\n    df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch2.csv'), index = False)\n    df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch3.csv'), index = False)\n    \nfor tar_name in tqdm(pname):\n    path_det_bg = glob(os.path.join(ifolder, tar_name, 'background*'))\n    path_det_bg = natsorted(path_det_bg)\n    \n    path_det = glob(os.path.join(ifolder, tar_name, subject_name + '*')) \n    path_det = natsorted(path_det)\n    \n    bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))\n    df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_mean.csv'), index = False)\n    df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch1.csv'), index = False)\n    df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch2.csv'), index = False)\n    df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch3.csv'), index = False)\n    det_list = [df_det_ch1, df_det_ch2, df_det_ch3]\n    stat_list = process_raw_data.get_stat(det_list, start_wl, stop_wl)\n    fig, ax = plt.subplots(1, 3, figsize=(16, 8), dpi=300)\n    for i in range(3):\n        for j in range(df_det_ch1.shape[1]-1):\n            ax[i].plot(det_list[i].loc[start_wl:stop_wl, 'wl'], det_list[i].loc[start_wl:stop_wl, f'shot_{j}'])\n        ax[i].set_title(f'SDS = {SDS_LIST[i]} mm')\n        ax[i].set_xticks(np.arange(700, 881, 30))\n        ax[i].set_xlabel('Wavelength (nm)')\n        ax[i].set_ylabel('Intensity (counts)')\n        ax2 = ax[i].twinx()\n        ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f%%'))\n        ax2.plot(stat_list[i]['wl'], 100 * stat_list[i]['cv'], color='black', linestyle='--')\n        ax2.set_ylabel('CV')\n        ax2.set_yticks(np.linspace(0, 10, 6))\n        ax2.tick_params(axis='y')\n        \n    fig.suptitle(f'Phantom {tar_name}')\n    fig.tight_layout()\n    fig.savefig(os.path.join(ofolder, f'{tar_name}.png'))\n</pre> # %% Load target spectrum folder_list = glob(os.path.join(ifolder, '*')) stdname = 'standard0.1' if any(stdname in folder_list[i] for i in range(len(folder_list))):     path_det_bg = glob(os.path.join(ifolder, stdname, 'background*'))     path_det_bg = natsorted(path_det_bg)          path_det = glob(os.path.join(ifolder, stdname, 'std*'))      path_det = natsorted(path_det)          bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))     df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_mean.csv'), index = False)     df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch1.csv'), index = False)     df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch2.csv'), index = False)     df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch3.csv'), index = False)      for tar_name in tqdm(pname):     path_det_bg = glob(os.path.join(ifolder, tar_name, 'background*'))     path_det_bg = natsorted(path_det_bg)          path_det = glob(os.path.join(ifolder, tar_name, subject_name + '*'))      path_det = natsorted(path_det)          bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))     df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_mean.csv'), index = False)     df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch1.csv'), index = False)     df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch2.csv'), index = False)     df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch3.csv'), index = False)     det_list = [df_det_ch1, df_det_ch2, df_det_ch3]     stat_list = process_raw_data.get_stat(det_list, start_wl, stop_wl)     fig, ax = plt.subplots(1, 3, figsize=(16, 8), dpi=300)     for i in range(3):         for j in range(df_det_ch1.shape[1]-1):             ax[i].plot(det_list[i].loc[start_wl:stop_wl, 'wl'], det_list[i].loc[start_wl:stop_wl, f'shot_{j}'])         ax[i].set_title(f'SDS = {SDS_LIST[i]} mm')         ax[i].set_xticks(np.arange(700, 881, 30))         ax[i].set_xlabel('Wavelength (nm)')         ax[i].set_ylabel('Intensity (counts)')         ax2 = ax[i].twinx()         ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f%%'))         ax2.plot(stat_list[i]['wl'], 100 * stat_list[i]['cv'], color='black', linestyle='--')         ax2.set_ylabel('CV')         ax2.set_yticks(np.linspace(0, 10, 6))         ax2.tick_params(axis='y')              fig.suptitle(f'Phantom {tar_name}')     fig.tight_layout()     fig.savefig(os.path.join(ofolder, f'{tar_name}.png')) <pre>  0%|          | 0/1 [00:00&lt;?, ?it/s]</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:51&lt;00:00, 51.59s/it]\n</pre> In\u00a0[12]: Copied! <pre>data = pd.read_csv(os.path.join(ofolder, f'{pname[0]}_det_{used_ch}.csv'))\ntotal_wl = data['wl'].to_numpy(dtype=np.float64)\ndata = data.to_numpy().T\ntime = [i*time_resolution for i in range(1,data.shape[0])]\ndf = {'Wavelength (nm)': time}\nfor idx, wl in enumerate(total_wl):\n    df[wl] = data[1:,idx]\ndf = pd.DataFrame(df)\ndf.to_csv(os.path.join(ofolder, f'{subject_name}_SDS1_sync.csv'), index=False)\ndf\n</pre> data = pd.read_csv(os.path.join(ofolder, f'{pname[0]}_det_{used_ch}.csv')) total_wl = data['wl'].to_numpy(dtype=np.float64) data = data.to_numpy().T time = [i*time_resolution for i in range(1,data.shape[0])] df = {'Wavelength (nm)': time} for idx, wl in enumerate(total_wl):     df[wl] = data[1:,idx] df = pd.DataFrame(df) df.to_csv(os.path.join(ofolder, f'{subject_name}_SDS1_sync.csv'), index=False) df Out[12]: Wavelength (nm) 693.3425 696.38 699.4175 702.4549999999999 705.4925000000001 708.53 711.5675 714.605 717.6424999999999 ... 860.405 863.4425 866.48 869.5175 872.5550000000001 875.5925 878.63 881.6675 884.705 887.7425000000001 0 0.10404 350.70 487.96 720.86 873.42 842.06 717.82 732.28 809.84 850.34 ... 587.64 535.98 564.36 511.66 440.60 473.2 452.56 407.66 421.30 388.26 1 0.20808 346.70 495.96 710.86 854.42 803.06 697.82 736.28 803.84 864.34 ... 594.64 555.98 541.36 484.66 464.60 461.2 475.56 422.66 406.30 386.26 2 0.31212 344.70 449.96 703.86 862.42 774.06 714.82 731.28 797.84 814.34 ... 530.64 554.98 513.36 481.66 456.60 443.2 467.56 405.66 413.30 368.26 3 0.41616 355.70 480.96 765.86 856.42 776.06 705.82 745.28 809.84 908.34 ... 545.64 531.98 527.36 493.66 449.60 468.2 469.56 386.66 373.30 392.26 4 0.52020 354.42 477.96 736.86 930.42 824.06 737.82 785.28 780.84 911.34 ... 568.64 531.98 544.36 478.66 469.84 464.2 432.56 421.66 402.30 383.26 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 6745 701.85384 23.24 33.96 31.58 54.94 35.66 58.32 80.50 62.32 70.28 ... 48.64 27.50 49.24 38.88 39.84 47.9 29.14 40.22 39.02 24.04 6746 701.95788 4.24 30.28 21.58 51.28 41.62 25.32 65.28 57.28 77.28 ... 60.48 44.50 50.24 42.88 38.60 45.9 28.56 26.22 48.30 43.04 6747 702.06192 7.24 28.28 30.58 43.28 48.62 43.32 57.54 68.84 65.28 ... 51.48 51.98 58.36 59.66 33.84 39.9 18.14 63.22 27.84 26.26 6748 702.16596 27.24 23.28 28.58 34.28 47.66 36.50 59.50 51.32 68.34 ... 50.64 48.98 47.36 42.88 45.84 38.9 41.14 34.22 33.30 46.26 6749 702.27000 25.24 11.28 21.58 57.28 48.62 64.82 35.50 59.32 64.34 ... 25.48 42.68 52.36 49.88 28.60 19.2 41.14 36.22 31.02 35.26 <p>6750 rows \u00d7 66 columns</p> In\u00a0[7]: Copied! <pre>process_raw_data.create_folder(mother_folder_name)\n\nProcesser = process_raw_data(baseline_start=baseline_start,\n                             baseline_end=baseline_end,\n                             exp_start=exp_start,\n                             exp_end=exp_end,\n                             recovery_start=recovery_start,\n                             recovery_end=recovery_end,\n                             time_resolution=time_resolution,\n                             time_interval=time_interval,\n                             mother_folder_name=mother_folder_name,\n                             using_SDS=using_SDS)\n</pre> process_raw_data.create_folder(mother_folder_name)  Processer = process_raw_data(baseline_start=baseline_start,                              baseline_end=baseline_end,                              exp_start=exp_start,                              exp_end=exp_end,                              recovery_start=recovery_start,                              recovery_end=recovery_end,                              time_resolution=time_resolution,                              time_interval=time_interval,                              mother_folder_name=mother_folder_name,                              using_SDS=using_SDS)  In\u00a0[26]: Copied! <pre># load raw data\nraw_data, total_wl = Processer.read_file(data_filepath)\n\n# select range 700nm~850nm\nidx_700nm = np.argmin(np.abs(total_wl-700))\nidx_850nm = np.argmin(np.abs(total_wl-850))\nraw_data, total_wl = raw_data[:, idx_700nm:idx_850nm], total_wl[idx_700nm:idx_850nm]\n\n# remove spike\ndata_no_spike = raw_data.copy()\nfor ts in range(0, recovery_end, time_interval):\n    td = ts + time_interval\n    if ((ts&gt;=exp_start) &amp; (ts&lt;=exp_end)) or ((td&gt;=exp_start) &amp; (td&lt;=exp_end)): # while in the experiment period, don't remove spike\n        pass # do nothing\n    else:\n        data_no_spike[round(ts/time_resolution):round(td/time_resolution)] = Processer.remove_spike(total_wl, \n                                                                                                    raw_data[round(ts/time_resolution):round(td/time_resolution)], \n                                                                                                    normalStdTimes=5, \n                                                                                                    ts=ts)\n        \n# # moving average\n# for i in range(data_no_spike.shape[0]):\n#     if i == 0:\n#         moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_no_spike[i,:])\n#         moving_avg_I_data = moving_avg_I_data.reshape(1,-1)\n#         data_moving_avg = moving_avg_I_data\n#     else:\n#         moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_no_spike[i,:])\n#         moving_avg_I_data = moving_avg_I_data.reshape(1,-1) \n#         data_moving_avg = np.concatenate((data_moving_avg,moving_avg_I_data))\n\n## EMD\ndata_EMD = data_no_spike.copy()\n# remove all-time signal based at first\nimfs = EMD().emd(data_no_spike.mean(axis=1))\nimfs[-1] -= imfs[-1].mean()\nartifact = imfs[-1] \n# remove artifact\nfor art in artifact:\n    data_EMD -= art.reshape(-1, 1)\n\n# detect peak \n# get straight signal to find peaks\nstraight_signal = data_no_spike.copy()\n# remove all-time signal based at first\nimfs = EMD().emd(data_no_spike.mean(axis=1))\nimfs[-1] -= imfs[-1].mean()\nartifact = imfs[2:] \n# remove artifact\nfor art in artifact:\n    straight_signal -= art.reshape(-1, 1)\nis_peak = np.zeros(straight_signal.shape[0])\nfor ts in range(0, recovery_end, time_interval):\n    td = ts+time_interval\n    data_signal = straight_signal[round(ts/time_resolution):round(td/time_resolution), :].mean(axis=1)\n    max_idx, min_idx = process_raw_data.get_peak_final(data_signal)\n    is_peak[min_idx + round(ts/time_resolution)] = -1\n    is_peak[max_idx + round(ts/time_resolution)] = 1\n\n# save result \nsave_result = {}\ntime = [i*time_resolution for i in range(data_no_spike.shape[0])]\nsave_result['time(s)'] = time\nsave_result['peak'] = is_peak # max:+1, min:-1\nfor idx, using_wl in enumerate(total_wl):\n    save_result[f'{using_wl}nm'] = data_EMD[:,idx]\nsave_result = pd.DataFrame(save_result)\nsave_result.to_csv(os.path.join(\"dataset\", mother_folder_name, f\"in_vivo_result_{exp}.csv\"), index=False)\n</pre> # load raw data raw_data, total_wl = Processer.read_file(data_filepath)  # select range 700nm~850nm idx_700nm = np.argmin(np.abs(total_wl-700)) idx_850nm = np.argmin(np.abs(total_wl-850)) raw_data, total_wl = raw_data[:, idx_700nm:idx_850nm], total_wl[idx_700nm:idx_850nm]  # remove spike data_no_spike = raw_data.copy() for ts in range(0, recovery_end, time_interval):     td = ts + time_interval     if ((ts&gt;=exp_start) &amp; (ts&lt;=exp_end)) or ((td&gt;=exp_start) &amp; (td&lt;=exp_end)): # while in the experiment period, don't remove spike         pass # do nothing     else:         data_no_spike[round(ts/time_resolution):round(td/time_resolution)] = Processer.remove_spike(total_wl,                                                                                                      raw_data[round(ts/time_resolution):round(td/time_resolution)],                                                                                                      normalStdTimes=5,                                                                                                      ts=ts)          # # moving average # for i in range(data_no_spike.shape[0]): #     if i == 0: #         moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_no_spike[i,:]) #         moving_avg_I_data = moving_avg_I_data.reshape(1,-1) #         data_moving_avg = moving_avg_I_data #     else: #         moving_avg_I_data, moving_avg_wl_data = process_raw_data.moving_avg(moving_window, total_wl, data_no_spike[i,:]) #         moving_avg_I_data = moving_avg_I_data.reshape(1,-1)  #         data_moving_avg = np.concatenate((data_moving_avg,moving_avg_I_data))  ## EMD data_EMD = data_no_spike.copy() # remove all-time signal based at first imfs = EMD().emd(data_no_spike.mean(axis=1)) imfs[-1] -= imfs[-1].mean() artifact = imfs[-1]  # remove artifact for art in artifact:     data_EMD -= art.reshape(-1, 1)  # detect peak  # get straight signal to find peaks straight_signal = data_no_spike.copy() # remove all-time signal based at first imfs = EMD().emd(data_no_spike.mean(axis=1)) imfs[-1] -= imfs[-1].mean() artifact = imfs[2:]  # remove artifact for art in artifact:     straight_signal -= art.reshape(-1, 1) is_peak = np.zeros(straight_signal.shape[0]) for ts in range(0, recovery_end, time_interval):     td = ts+time_interval     data_signal = straight_signal[round(ts/time_resolution):round(td/time_resolution), :].mean(axis=1)     max_idx, min_idx = process_raw_data.get_peak_final(data_signal)     is_peak[min_idx + round(ts/time_resolution)] = -1     is_peak[max_idx + round(ts/time_resolution)] = 1  # save result  save_result = {} time = [i*time_resolution for i in range(data_no_spike.shape[0])] save_result['time(s)'] = time save_result['peak'] = is_peak # max:+1, min:-1 for idx, using_wl in enumerate(total_wl):     save_result[f'{using_wl}nm'] = data_EMD[:,idx] save_result = pd.DataFrame(save_result) save_result.to_csv(os.path.join(\"dataset\", mother_folder_name, f\"in_vivo_result_{exp}.csv\"), index=False) <pre>target = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\ntarget = []\n</pre> In\u00a0[31]: Copied! <pre>plt.rcParams.update({'font.size': 20})\nplt.figure(figsize=(20,8))\ntime = [i*time_resolution for i in range(raw_data.shape[0])]\nplt.plot(time, raw_data.mean(1))\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label=f'{exp}_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', mother_folder_name, 'time', 'raw_all_time_SDS20.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.rcParams.update({'font.size': 20}) plt.figure(figsize=(20,8)) time = [i*time_resolution for i in range(raw_data.shape[0])] plt.plot(time, raw_data.mean(1)) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label=f'{exp}_end') plt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel(\"time [sec]\") plt.ylabel(\"Intensity\") plt.title('SDS20mm') plt.savefig(os.path.join('pic', mother_folder_name, 'time', 'raw_all_time_SDS20.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[30]: Copied! <pre>max_id = np.where(save_result['peak']==1)[0]\nmin_id = np.where(save_result['peak']==-1)[0]\n\nmax_id = max_id[np.where(max_id&lt;round(recovery_end/time_resolution))[0]]\nmin_id = min_id[np.where(min_id&lt;round(recovery_end/time_resolution))[0]]\n\nplt.figure(figsize=(20,8))\ntime = save_result['time(s)']\nplt.plot(time, data_EMD.mean(1))\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label=f'{exp}_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.plot(time[max_id], data_EMD.mean(1)[max_id], 'r.')\nplt.plot(time[min_id], data_EMD.mean(1)[min_id], 'b.')\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', mother_folder_name, 'time', 'processed_all_time_SDS10.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre>  max_id = np.where(save_result['peak']==1)[0] min_id = np.where(save_result['peak']==-1)[0]  max_id = max_id[np.where(max_id In\u00a0[32]: Copied! <pre>plt.rcParams.update({'font.size': 12})\nProcesser.long_plot_all_fig(data=raw_data, \n            wavelength=total_wl,\n            name='raw')\n\nProcesser.long_plot_all_fig(data=data_no_spike, \n            wavelength=total_wl,\n            name='remove_spike_and_bg')\n    \nProcesser.long_plot_all_fig(data=data_EMD, \n            wavelength=total_wl,\n            name='EMD')\n\nProcesser.plot_Rmax_Rmin(data=data_EMD,\n                         wavelength=total_wl,\n                         max_idx_Set=max_idx,\n                         min_idx_Set=min_idx,\n                         name=\"get_peak\",\n                         start_time=0,\n                         end_time=recovery_end)\n\nfor using_num_IMF in [1,2,3]:\n    Processer.plot_time_EMD(data=data_no_spike,\n                    name='EMD',\n                    start_time=0,\n                    end_time=recovery_end,\n                    using_num_IMF=using_num_IMF)\n\n    Processer.plot_compare_time_EMD(data=data_no_spike,\n                        name='compare',\n                        start_time=0,\n                        end_time=recovery_end,\n                        using_num_IMF=using_num_IMF)\n\n    \n    for ts in range(0,recovery_end,time_interval):\n        td = ts + time_interval\n        Processer.plot_time_EMD(data=data_no_spike,\n                    name='EMD',\n                    start_time=ts,\n                    end_time=td,\n                    using_num_IMF=using_num_IMF)\n\n        Processer.plot_compare_time_EMD(data=data_no_spike,\n                            name='compare',\n                            start_time=ts,\n                            end_time=td,\n                            using_num_IMF=using_num_IMF)\n</pre> plt.rcParams.update({'font.size': 12}) Processer.long_plot_all_fig(data=raw_data,              wavelength=total_wl,             name='raw')  Processer.long_plot_all_fig(data=data_no_spike,              wavelength=total_wl,             name='remove_spike_and_bg')      Processer.long_plot_all_fig(data=data_EMD,              wavelength=total_wl,             name='EMD')  Processer.plot_Rmax_Rmin(data=data_EMD,                          wavelength=total_wl,                          max_idx_Set=max_idx,                          min_idx_Set=min_idx,                          name=\"get_peak\",                          start_time=0,                          end_time=recovery_end)  for using_num_IMF in [1,2,3]:     Processer.plot_time_EMD(data=data_no_spike,                     name='EMD',                     start_time=0,                     end_time=recovery_end,                     using_num_IMF=using_num_IMF)      Processer.plot_compare_time_EMD(data=data_no_spike,                         name='compare',                         start_time=0,                         end_time=recovery_end,                         using_num_IMF=using_num_IMF)           for ts in range(0,recovery_end,time_interval):         td = ts + time_interval         Processer.plot_time_EMD(data=data_no_spike,                     name='EMD',                     start_time=ts,                     end_time=td,                     using_num_IMF=using_num_IMF)          Processer.plot_compare_time_EMD(data=data_no_spike,                             name='compare',                             start_time=ts,                             end_time=td,                             using_num_IMF=using_num_IMF) <pre>d:\\ijv_code_new\\IJV-Project\\in_vivo_experiments\\utils.py:460: RuntimeWarning: Mean of empty slice.\n  R_max_spec = data[max_idx_subset, :].mean(0)\nd:\\ijv_code_new\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\nd:\\ijv_code_new\\IJV-Project\\in_vivo_experiments\\utils.py:461: RuntimeWarning: Mean of empty slice.\n  R_min_spec = data[min_idx_subset, :].mean(0)\n</pre> In\u00a0[36]: Copied! <pre># %% Load target spectrum\nfolder_list = glob(os.path.join(ifolder, '*'))\nstdname = 'standard0.1'\nif any(stdname in folder_list[i] for i in range(len(folder_list))):\n    path_det_bg = glob(os.path.join(ifolder, stdname, 'background*'))\n    path_det_bg = natsorted(path_det_bg)\n    \n    path_det = glob(os.path.join(ifolder, stdname, 'std*')) \n    path_det = natsorted(path_det)\n    \n    bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))\n    df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_mean.csv'), index = False)\n    df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch1.csv'), index = False)\n    df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch2.csv'), index = False)\n    df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch3.csv'), index = False)\n    \nfor tar_name in tqdm(phantom_measured_ID):\n    path_det_bg = glob(os.path.join(ifolder, tar_name, 'background*'))\n    path_det_bg = natsorted(path_det_bg)\n    \n    path_det = glob(os.path.join(ifolder, tar_name, tar_name + '*')) \n    path_det = natsorted(path_det)\n    \n    bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))\n    df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_mean.csv'), index = False)\n    df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch1.csv'), index = False)\n    df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch2.csv'), index = False)\n    df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch3.csv'), index = False)\n    det_list = [df_det_ch1, df_det_ch2, df_det_ch3]\n    stat_list = process_raw_data.get_stat(det_list, start_wl, stop_wl)\n    fig, ax = plt.subplots(1, 3, figsize=(16, 8), dpi=300)\n    for i in range(3):\n        for j in range(df_det_ch1.shape[1]-1):\n            ax[i].plot(det_list[i].loc[start_wl:stop_wl, 'wl'], det_list[i].loc[start_wl:stop_wl, f'shot_{j}'])\n        ax[i].set_title(f'SDS = {SDS_LIST[i]} mm')\n        ax[i].set_xticks(np.arange(700, 881, 30))\n        ax[i].set_xlabel('Wavelength (nm)')\n        ax[i].set_ylabel('Intensity (counts)')\n        ax2 = ax[i].twinx()\n        ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f%%'))\n        ax2.plot(stat_list[i]['wl'], 100 * stat_list[i]['cv'], color='black', linestyle='--')\n        ax2.set_ylabel('CV')\n        ax2.set_yticks(np.linspace(0, 10, 6))\n        ax2.tick_params(axis='y')\n        \n    fig.suptitle(f'Phantom {tar_name}')\n    fig.tight_layout()\n    fig.savefig(os.path.join(ofolder, f'{tar_name}.png'))\n</pre> # %% Load target spectrum folder_list = glob(os.path.join(ifolder, '*')) stdname = 'standard0.1' if any(stdname in folder_list[i] for i in range(len(folder_list))):     path_det_bg = glob(os.path.join(ifolder, stdname, 'background*'))     path_det_bg = natsorted(path_det_bg)          path_det = glob(os.path.join(ifolder, stdname, 'std*'))      path_det = natsorted(path_det)          bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))     df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_mean.csv'), index = False)     df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch1.csv'), index = False)     df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch2.csv'), index = False)     df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, stdname+'_det_ch3.csv'), index = False)      for tar_name in tqdm(phantom_measured_ID):     path_det_bg = glob(os.path.join(ifolder, tar_name, 'background*'))     path_det_bg = natsorted(path_det_bg)          path_det = glob(os.path.join(ifolder, tar_name, tar_name + '*'))      path_det = natsorted(path_det)          bg_arr, df_det_mean, df_det_ch1, df_det_ch2, df_det_ch3 = process_raw_data.get_spec(path_det_bg, path_det, row_choose, a, b, img_size=(20, 320))     df_det_mean.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_mean.csv'), index = False)     df_det_ch1.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch1.csv'), index = False)     df_det_ch2.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch2.csv'), index = False)     df_det_ch3.loc[start_wl:stop_wl, :].to_csv(os.path.join(ofolder, tar_name+'_det_ch3.csv'), index = False)     det_list = [df_det_ch1, df_det_ch2, df_det_ch3]     stat_list = process_raw_data.get_stat(det_list, start_wl, stop_wl)     fig, ax = plt.subplots(1, 3, figsize=(16, 8), dpi=300)     for i in range(3):         for j in range(df_det_ch1.shape[1]-1):             ax[i].plot(det_list[i].loc[start_wl:stop_wl, 'wl'], det_list[i].loc[start_wl:stop_wl, f'shot_{j}'])         ax[i].set_title(f'SDS = {SDS_LIST[i]} mm')         ax[i].set_xticks(np.arange(700, 881, 30))         ax[i].set_xlabel('Wavelength (nm)')         ax[i].set_ylabel('Intensity (counts)')         ax2 = ax[i].twinx()         ax2.yaxis.set_major_formatter(mtick.FormatStrFormatter('%.1f%%'))         ax2.plot(stat_list[i]['wl'], 100 * stat_list[i]['cv'], color='black', linestyle='--')         ax2.set_ylabel('CV')         ax2.set_yticks(np.linspace(0, 10, 6))         ax2.tick_params(axis='y')              fig.suptitle(f'Phantom {tar_name}')     fig.tight_layout()     fig.savefig(os.path.join(ofolder, f'{tar_name}.png')) <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07&lt;00:00,  1.82s/it]\n</pre> In\u00a0[37]: Copied! <pre>for phantom_ID in phantom_measured_ID:\n    data = pd.read_csv(os.path.join(ofolder, f'{phantom_ID}_det_ch3.csv'))\n    total_wl =  data['wl'].to_numpy(dtype=np.float64)\n    data = data.to_numpy().T\n    time = [i*time_resolution for i in range(1,data.shape[0])]\n    df = {'Wavelength (nm)': time}\n    for idx, wl in enumerate(total_wl):\n        df[wl] = data[1:,idx]\n    df = pd.DataFrame(df)\n    df.to_csv(os.path.join(ofolder, f'{phantom_ID}_SDS1_sync.csv'), index=False)\ndf\n</pre> for phantom_ID in phantom_measured_ID:     data = pd.read_csv(os.path.join(ofolder, f'{phantom_ID}_det_ch3.csv'))     total_wl =  data['wl'].to_numpy(dtype=np.float64)     data = data.to_numpy().T     time = [i*time_resolution for i in range(1,data.shape[0])]     df = {'Wavelength (nm)': time}     for idx, wl in enumerate(total_wl):         df[wl] = data[1:,idx]     df = pd.DataFrame(df)     df.to_csv(os.path.join(ofolder, f'{phantom_ID}_SDS1_sync.csv'), index=False) df Out[37]: Wavelength (nm) 693.3425 696.38 699.4175 702.4549999999999 705.4925000000001 708.53 711.5675 714.605 717.6424999999999 ... 860.405 863.4425 866.48 869.5175 872.5550000000001 875.5925 878.63 881.6675 884.705 887.7425000000001 0 0.10404 225.04 273.6 315.66 394.44 415.86 558.08 671.5 776.1 872.5 ... 2733.9 2653.7 2626.4 2502.22 2552.84 2401.76 2340.26 2256.66 2073.3 2075.82 1 0.20808 234.04 267.6 299.66 396.44 433.86 534.08 644.5 771.1 871.5 ... 2761.9 2657.7 2574.4 2474.22 2481.84 2394.76 2342.26 2274.66 2111.3 2075.82 2 0.31212 221.04 257.6 324.66 373.44 433.86 570.08 656.5 785.1 859.5 ... 2801.9 2656.7 2630.4 2517.22 2556.84 2446.76 2352.26 2205.66 2078.3 2072.82 3 0.41616 237.04 254.6 295.66 372.44 434.86 577.08 680.5 788.1 870.5 ... 2761.9 2663.7 2660.4 2523.22 2491.84 2402.76 2288.26 2268.66 2067.3 2050.82 4 0.52020 200.04 270.6 305.66 350.44 463.86 550.08 664.5 809.1 862.5 ... 2775.9 2748.7 2637.4 2493.22 2510.84 2422.76 2352.26 2263.66 2032.3 2109.82 5 0.62424 236.04 247.6 321.66 379.44 444.86 553.08 671.5 770.1 882.5 ... 2774.9 2640.7 2645.4 2515.22 2533.84 2392.76 2374.26 2214.66 2117.3 2065.82 6 0.72828 224.04 231.6 305.66 369.44 443.86 561.08 651.5 793.1 872.5 ... 2782.9 2660.7 2619.4 2485.22 2519.84 2406.76 2336.26 2261.66 2064.3 2044.82 7 0.83232 227.04 266.6 321.66 375.44 447.86 549.08 659.5 778.1 872.5 ... 2771.9 2625.7 2634.4 2467.22 2540.84 2423.76 2381.26 2259.66 2129.3 2080.82 8 0.93636 223.04 263.6 338.66 387.44 435.86 548.08 646.5 760.1 893.5 ... 2783.9 2664.7 2580.4 2515.22 2521.84 2402.76 2374.26 2260.66 2126.3 2052.82 9 1.04040 223.04 266.6 295.66 365.44 448.86 562.08 647.5 748.1 884.5 ... 2768.9 2708.7 2616.4 2516.22 2505.84 2410.76 2335.26 2259.66 2117.3 2068.82 10 1.14444 225.04 265.6 323.66 363.44 438.86 537.08 660.5 776.1 871.5 ... 2824.9 2675.7 2593.4 2540.22 2530.84 2370.76 2359.26 2274.66 2096.3 2070.82 11 1.24848 207.04 270.6 296.66 365.44 480.86 534.08 649.5 779.1 886.5 ... 2802.9 2654.7 2630.4 2480.22 2522.84 2390.76 2346.26 2213.66 2049.3 2035.82 12 1.35252 230.04 250.6 334.66 394.44 455.86 559.08 680.5 782.1 865.5 ... 2797.9 2621.7 2648.4 2531.22 2542.84 2388.76 2339.26 2267.66 2032.3 2041.82 13 1.45656 224.04 255.6 294.66 379.44 448.86 542.08 654.5 775.1 851.5 ... 2794.9 2682.7 2637.4 2526.22 2515.84 2426.76 2354.26 2242.66 2070.3 2083.82 14 1.56060 217.04 272.6 293.66 366.44 453.86 550.08 655.5 760.1 842.5 ... 2797.9 2673.7 2624.4 2559.22 2533.84 2422.76 2363.26 2252.66 2082.3 2036.82 15 1.66464 225.04 261.6 321.66 386.44 452.86 538.08 644.5 769.1 868.5 ... 2753.9 2699.7 2616.4 2512.22 2525.84 2394.76 2320.26 2255.66 2093.3 2060.82 16 1.76868 221.04 281.6 313.66 378.44 455.86 557.08 659.5 792.1 859.5 ... 2789.9 2678.7 2659.4 2485.22 2562.84 2393.76 2353.26 2238.66 2100.3 2102.82 17 1.87272 228.04 249.6 316.66 369.44 456.86 556.08 648.5 784.1 852.5 ... 2796.9 2686.7 2656.4 2508.22 2534.84 2367.76 2396.26 2250.66 2062.3 2078.82 18 1.97676 217.04 263.6 316.66 391.44 435.86 529.08 654.5 758.1 871.5 ... 2798.9 2657.7 2595.4 2503.22 2493.84 2425.76 2325.26 2233.66 2131.3 2105.82 19 2.08080 207.04 242.6 311.66 381.44 443.86 526.08 666.5 789.1 856.5 ... 2840.9 2624.7 2640.4 2512.22 2537.84 2417.76 2301.26 2275.66 2124.3 2084.82 20 2.18484 227.04 242.6 330.66 380.44 430.86 537.08 669.5 767.1 866.5 ... 2809.9 2673.7 2640.4 2500.22 2483.84 2373.76 2396.26 2242.66 2108.3 2085.82 21 2.28888 226.04 260.6 323.66 359.44 440.86 546.08 644.5 778.1 873.5 ... 2797.9 2681.7 2646.4 2498.22 2572.84 2396.76 2365.26 2231.66 2070.3 2062.82 22 2.39292 221.04 288.6 310.66 369.44 447.86 552.08 663.5 749.1 893.5 ... 2802.9 2663.7 2634.4 2490.22 2517.84 2407.76 2382.26 2244.66 2075.3 2116.82 23 2.49696 235.04 233.6 320.66 346.44 455.86 536.08 663.5 757.1 883.5 ... 2793.9 2647.7 2608.4 2477.22 2462.84 2417.76 2329.26 2217.66 2071.3 2097.82 24 2.60100 241.04 259.6 293.66 372.44 445.86 545.08 666.5 764.1 870.5 ... 2798.9 2664.7 2636.4 2521.22 2546.84 2388.76 2313.26 2266.66 2072.3 2073.82 25 2.70504 212.04 256.6 303.66 386.44 455.86 568.08 664.5 747.1 861.5 ... 2785.9 2712.7 2651.4 2502.22 2525.84 2399.76 2351.26 2228.66 2101.3 2039.82 26 2.80908 220.04 267.6 320.66 385.44 438.86 546.08 667.5 740.1 863.5 ... 2856.9 2621.7 2664.4 2484.22 2466.84 2380.76 2357.26 2298.66 2068.3 2083.82 27 2.91312 221.04 256.6 326.66 370.44 440.86 545.08 656.5 796.1 858.5 ... 2747.9 2704.7 2621.4 2484.22 2550.84 2411.76 2390.26 2244.66 2052.3 2075.82 28 3.01716 219.04 273.6 296.66 371.44 459.86 541.08 618.5 762.1 875.5 ... 2815.9 2666.7 2592.4 2503.22 2535.84 2387.76 2334.26 2247.66 2064.3 2055.82 29 3.12120 223.04 272.6 315.66 378.44 444.86 559.08 662.5 761.1 848.5 ... 2731.9 2650.7 2581.4 2509.22 2487.84 2418.76 2395.26 2278.66 2111.3 2070.82 30 3.22524 237.04 254.6 316.66 368.44 448.86 555.08 664.5 777.1 879.5 ... 2765.9 2671.7 2658.4 2494.22 2542.84 2386.76 2376.26 2282.66 2068.3 2070.82 31 3.32928 230.04 254.6 332.66 379.44 453.86 560.08 665.5 733.1 869.5 ... 2792.9 2650.7 2688.4 2483.22 2500.84 2405.76 2367.26 2216.66 2091.3 2085.82 32 3.43332 237.04 266.6 300.66 395.44 462.86 545.08 667.5 772.1 840.5 ... 2823.9 2588.7 2654.4 2502.22 2500.84 2396.76 2358.26 2242.66 2048.3 2049.82 33 3.53736 222.04 244.6 315.66 369.44 480.86 518.08 666.5 757.1 859.5 ... 2733.9 2672.7 2585.4 2526.22 2538.84 2362.76 2344.26 2282.66 2079.3 2057.82 34 3.64140 230.04 253.6 317.66 373.44 455.86 540.08 678.5 766.1 856.5 ... 2816.9 2606.7 2683.4 2499.22 2531.84 2414.76 2347.26 2252.66 2053.3 2070.82 35 3.74544 216.04 262.6 332.66 379.44 470.86 545.08 636.5 813.1 858.5 ... 2756.9 2673.7 2625.4 2471.22 2608.84 2382.76 2294.26 2259.66 2107.3 2118.82 36 3.84948 231.04 266.6 313.66 374.44 459.86 519.08 669.5 774.1 846.5 ... 2755.9 2687.7 2606.4 2473.22 2483.84 2376.76 2363.26 2229.66 2066.3 2039.82 37 3.95352 224.04 278.6 308.66 359.44 447.86 551.08 675.5 781.1 849.5 ... 2789.9 2653.7 2601.4 2558.22 2488.84 2351.76 2359.26 2232.66 2089.3 2108.82 38 4.05756 232.04 275.6 312.66 386.44 431.86 545.08 636.5 783.1 837.5 ... 2738.9 2694.7 2661.4 2507.22 2539.84 2361.76 2408.26 2270.66 2087.3 2053.82 39 4.16160 229.04 255.6 313.66 368.44 439.86 552.08 650.5 756.1 850.5 ... 2816.9 2636.7 2644.4 2491.22 2535.84 2364.76 2330.26 2200.66 2068.3 2078.82 40 4.26564 228.04 262.6 311.66 362.44 456.86 552.08 663.5 770.1 871.5 ... 2780.9 2655.7 2645.4 2498.22 2560.84 2418.76 2329.26 2263.66 2122.3 2082.82 41 4.36968 218.04 243.6 316.66 362.44 441.86 537.08 669.5 774.1 867.5 ... 2769.9 2680.7 2581.4 2519.22 2518.84 2365.76 2336.26 2267.66 2113.3 2026.82 42 4.47372 229.04 242.6 306.66 363.44 441.86 547.08 676.5 793.1 873.5 ... 2786.9 2645.7 2608.4 2491.22 2526.84 2365.76 2341.26 2254.66 2042.3 2068.82 43 4.57776 232.04 259.6 333.66 364.44 454.86 543.08 649.5 786.1 853.5 ... 2773.9 2642.7 2630.4 2486.22 2534.84 2386.76 2352.26 2208.66 2047.3 2064.82 44 4.68180 206.04 252.6 325.66 360.44 444.86 564.08 655.5 789.1 865.5 ... 2780.9 2680.7 2585.4 2524.22 2527.84 2376.76 2302.26 2247.66 2084.3 2079.82 45 4.78584 210.04 255.6 321.66 382.44 443.86 540.08 671.5 767.1 879.5 ... 2724.9 2705.7 2653.4 2480.22 2536.84 2364.76 2364.26 2226.66 2066.3 2077.82 46 4.88988 211.04 252.6 318.66 360.44 443.86 567.08 650.5 753.1 868.5 ... 2788.9 2727.7 2648.4 2526.22 2548.84 2366.76 2314.26 2234.66 2123.3 2064.82 47 4.99392 229.04 246.6 307.66 390.44 421.86 529.08 670.5 785.1 865.5 ... 2796.9 2657.7 2635.4 2483.22 2568.84 2397.76 2348.26 2252.66 2071.3 2043.82 48 5.09796 236.04 262.6 326.66 378.44 436.86 564.08 651.5 752.1 885.5 ... 2803.9 2646.7 2616.4 2519.22 2463.84 2383.76 2366.26 2257.66 2091.3 2018.82 49 5.20200 221.04 266.6 309.66 367.44 472.86 551.08 700.5 765.1 883.5 ... 2733.9 2678.7 2662.4 2559.22 2487.84 2374.76 2341.26 2263.66 2073.3 2067.82 <p>50 rows \u00d7 66 columns</p> In\u00a0[\u00a0]: Copied! <pre># get measured phantom data\nphantom_data = {} # CHIK3456\nfor ID in phantom_measured_ID:\n    # define plot savepath\n    os.makedirs(os.path.join(\"pic\", mother_folder_name, 'phantom', ID), exist_ok=True)\n    \n    # import measured data\n    data, total_wl = process_raw_data.read_file(os.path.join(ofolder, f'{ID}_SDS1_sync.csv'))\n    \n    # remove spike\n    remove_spike_data = process_phantom.remove_spike(total_wl, data, \n                                                     normalStdTimes=2, \n                                                     savepath=os.path.join(\"pic\", mother_folder_name,'phantom', ID)) # remove spike\n    time_mean_data = remove_spike_data.mean(0) # mean of measured signal\n    \n    # plt.plot(data.mean(1))\n    # plt.plot(remove_spike_data.mean(1), 'o--')\n    # plt.show()\n    phantom_data[ID] = time_mean_data\n</pre> # get measured phantom data phantom_data = {} # CHIK3456 for ID in phantom_measured_ID:     # define plot savepath     os.makedirs(os.path.join(\"pic\", mother_folder_name, 'phantom', ID), exist_ok=True)          # import measured data     data, total_wl = process_raw_data.read_file(os.path.join(ofolder, f'{ID}_SDS1_sync.csv'))          # remove spike     remove_spike_data = process_phantom.remove_spike(total_wl, data,                                                       normalStdTimes=2,                                                       savepath=os.path.join(\"pic\", mother_folder_name,'phantom', ID)) # remove spike     time_mean_data = remove_spike_data.mean(0) # mean of measured signal          # plt.plot(data.mean(1))     # plt.plot(remove_spike_data.mean(1), 'o--')     # plt.show()     phantom_data[ID] = time_mean_data In\u00a0[39]: Copied! <pre># load used wavelength\nwith open(os.path.join(\"OPs_used\", \"wavelength.json\"), \"r\") as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\n</pre> # load used wavelength with open(os.path.join(\"OPs_used\", \"wavelength.json\"), \"r\") as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength']  In\u00a0[40]: Copied! <pre>matplotlib.rcParams.update({'font.size': 18})\n## Get the same simulated wavelength point from measured phantom\nmeasured_phantom_data = []\nplt.figure(figsize=(12,8))\n# Cubic spline interpolation\"\nfor idx, out_k in enumerate(phantom_data.keys()):\n    data = phantom_data[out_k]\n    f_interpolate = interp1d(total_wl, data, kind='linear', bounds_error=False, fill_value='extrapolate')\n    used_wl_data = f_interpolate(wavelength)\n    measured_phantom_data.append(used_wl_data)\n    plt.plot(wavelength, used_wl_data, 'o--', label=f'phantom_{phantom_measured_ID[idx]}')\nplt.title(\"SDS=10mm, measured phantom spectrum\")\nplt.xlabel('wavelength (nm)')\nplt.ylabel('intensity')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\nmeasured_phantom_data = np.array(measured_phantom_data)\n\n## Get simulated phantom data\nsim_phantom_data = []\nplt.figure(figsize=(12,8))\nfor c in phantom_simulated_ID:\n    data = np.load(os.path.join(\"dataset\", \"phantom_simulated\", f'{c}.npy'))\n    sim_phantom_data.append(data[:,SDS_idx].tolist())\n    plt.plot(wavelength, data[:,SDS_idx], 'o--',label=f'phantom_{c}')\nplt.title(\"SDS=10mm, simulated phantom spectrum\")\nplt.xlabel('wavelength (nm)')\nplt.ylabel('intensity')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"simulated_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\nsim_phantom_data = np.array(sim_phantom_data)\n</pre> matplotlib.rcParams.update({'font.size': 18}) ## Get the same simulated wavelength point from measured phantom measured_phantom_data = [] plt.figure(figsize=(12,8)) # Cubic spline interpolation\" for idx, out_k in enumerate(phantom_data.keys()):     data = phantom_data[out_k]     f_interpolate = interp1d(total_wl, data, kind='linear', bounds_error=False, fill_value='extrapolate')     used_wl_data = f_interpolate(wavelength)     measured_phantom_data.append(used_wl_data)     plt.plot(wavelength, used_wl_data, 'o--', label=f'phantom_{phantom_measured_ID[idx]}') plt.title(\"SDS=10mm, measured phantom spectrum\") plt.xlabel('wavelength (nm)') plt.ylabel('intensity') plt.legend() plt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() measured_phantom_data = np.array(measured_phantom_data)  ## Get simulated phantom data sim_phantom_data = [] plt.figure(figsize=(12,8)) for c in phantom_simulated_ID:     data = np.load(os.path.join(\"dataset\", \"phantom_simulated\", f'{c}.npy'))     sim_phantom_data.append(data[:,SDS_idx].tolist())     plt.plot(wavelength, data[:,SDS_idx], 'o--',label=f'phantom_{c}') plt.title(\"SDS=10mm, simulated phantom spectrum\") plt.xlabel('wavelength (nm)') plt.ylabel('intensity') plt.legend() plt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"simulated_phantom_adjust_wl.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() sim_phantom_data = np.array(sim_phantom_data) In\u00a0[41]: Copied! <pre>fig = plt.figure(figsize=(18,12))\nfig.suptitle(f\"SDS = {using_SDS} mm\", fontsize=16)\ncount = 1\nfor idx, used_wl in enumerate(wavelength):\n    ## fit measured phantom and simulated phantom\n    z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)\n    plotx = np.linspace(measured_phantom_data[-1, idx]*0.8,  measured_phantom_data[0, idx]*1.2,100)\n    ploty = plotx*z[0] + z[1]\n    calibrate_data = measured_phantom_data[:, idx]*z[0] + z[1]\n    R_square = process_phantom.cal_R_square(calibrate_data, sim_phantom_data[:,idx]) # cal R square\n    \n    ## plot result\n    ax = plt.subplot(5,4, count)\n    ax.set_title(f\"@wavelength={used_wl} nm\")\n    ax.set_title(f'{used_wl}nm, $R^{2}$={R_square:.2f}')\n    for ID_idx, ID in enumerate(phantom_measured_ID):\n        ax.plot(measured_phantom_data[ID_idx, idx], sim_phantom_data[ID_idx,idx], 's', label=f'phantom_{ID}')\n    ax.plot(plotx, ploty, '--')\n    ax.set_xlabel(\"measure intensity\")\n    ax.set_ylabel(\"sim intensity\")\n    count += 1\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                    fancybox=True, shadow=True)\nplt.tight_layout()\nplt.savefig(os.path.join('pic', mother_folder_name, 'phantom', \"all.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> fig = plt.figure(figsize=(18,12)) fig.suptitle(f\"SDS = {using_SDS} mm\", fontsize=16) count = 1 for idx, used_wl in enumerate(wavelength):     ## fit measured phantom and simulated phantom     z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)     plotx = np.linspace(measured_phantom_data[-1, idx]*0.8,  measured_phantom_data[0, idx]*1.2,100)     ploty = plotx*z[0] + z[1]     calibrate_data = measured_phantom_data[:, idx]*z[0] + z[1]     R_square = process_phantom.cal_R_square(calibrate_data, sim_phantom_data[:,idx]) # cal R square          ## plot result     ax = plt.subplot(5,4, count)     ax.set_title(f\"@wavelength={used_wl} nm\")     ax.set_title(f'{used_wl}nm, $R^{2}$={R_square:.2f}')     for ID_idx, ID in enumerate(phantom_measured_ID):         ax.plot(measured_phantom_data[ID_idx, idx], sim_phantom_data[ID_idx,idx], 's', label=f'phantom_{ID}')     ax.plot(plotx, ploty, '--')     ax.set_xlabel(\"measure intensity\")     ax.set_ylabel(\"sim intensity\")     count += 1 plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                     fancybox=True, shadow=True) plt.tight_layout() plt.savefig(os.path.join('pic', mother_folder_name, 'phantom', \"all.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[42]: Copied! <pre>result = {}\nfor idx, used_wl in enumerate(wavelength):\n    z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)\n    result[used_wl] = z\nresult = pd.DataFrame(result)\nos.makedirs(os.path.join(\"dataset\",  subject_name, 'calibration_result', date) , exist_ok=True)\nresult.to_csv(os.path.join(\"dataset\",  subject_name, 'calibration_result', date, \"calibrate_SDS_1.csv\"), index=False)\n</pre> result = {} for idx, used_wl in enumerate(wavelength):     z = np.polyfit(measured_phantom_data[:, idx], sim_phantom_data[:,idx], 1)     result[used_wl] = z result = pd.DataFrame(result) os.makedirs(os.path.join(\"dataset\",  subject_name, 'calibration_result', date) , exist_ok=True) result.to_csv(os.path.join(\"dataset\",  subject_name, 'calibration_result', date, \"calibrate_SDS_1.csv\"), index=False) In\u00a0[43]: Copied! <pre>for idx, k in enumerate(phantom_data.keys()):\n    data = phantom_data[k]\n    plt.plot(total_wl, data, label=f'phantom_{phantom_measured_ID[idx]}')\nplt.title('phantom spectrum')\nplt.xlabel('wavelength (nm)')\nplt.ylabel('intensity')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_2345_phantom_result.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> for idx, k in enumerate(phantom_data.keys()):     data = phantom_data[k]     plt.plot(total_wl, data, label=f'phantom_{phantom_measured_ID[idx]}') plt.title('phantom spectrum') plt.xlabel('wavelength (nm)') plt.ylabel('intensity') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                 fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", mother_folder_name, 'phantom', \"measured_2345_phantom_result.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()"},{"location":"in_vivo_experiments/S1_preprocess_short/#format-setting","title":"Format Setting\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#process-spectraproemccd-raw-data-image-to-csv-file","title":"Process Spectrapro+EMCCD raw data (image to csv file)\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#sync-spectrapro-image-data-to-qepro-format-csv-file","title":"Sync Spectrapro image data to QEpro format csv file\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#initialize-processer-instance","title":"Initialize Processer Instance\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#process-raw-in-vivo-data","title":"Process Raw in-vivo Data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#plot-raw-data","title":"Plot Raw Data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#phantom-calibration","title":"Phantom Calibration\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#process-spectraproemccd-raw-phantom-data","title":"Process spectrapro+EMCCD raw phantom data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#sync-spectraprofile-to-qepro-file","title":"Sync spectraprofile to QEpro file\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#load-measured-phantom-data","title":"Load measured phantom data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#load-simulated-phantom-data","title":"Load simulated phantom data\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#fit-measured-phantom-and-simulated-phantom","title":"Fit measured phantom and simulated phantom\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#save-fitting-result-as-csv","title":"Save fitting result as csv\u00b6","text":""},{"location":"in_vivo_experiments/S1_preprocess_short/#plot-all-measured-phantom-together","title":"Plot all measured phantom together\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/","title":"Process Data and Predict by ANN","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib as mpl\nimport json\nimport matplotlib.pyplot as plt\nfrom scipy.interpolate import interp1d\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\nplt.rcParams.update({'font.size': 20})\n</pre> import numpy as np import pandas as pd import os import matplotlib as mpl import json import matplotlib.pyplot as plt from scipy.interpolate import interp1d # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") plt.rcParams.update({'font.size': 20}) <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24312\\2527255600.py:14: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[2]: Copied! <pre>date = \"20230912\"\nsubject = 'KB'\nexp = 'VM'\nSDS1_time_resolution = 0.10404 # [sec/point]\nSDS2_time_resolution = 0.1 # [sec/point]\nbaseline_end = 60 # [sec]\nexp_end = 75 #[sec]\nrecovery_end = 675 #[sec]\n</pre> date = \"20230912\" subject = 'KB' exp = 'VM' SDS1_time_resolution = 0.10404 # [sec/point] SDS2_time_resolution = 0.1 # [sec/point] baseline_end = 60 # [sec] exp_end = 75 #[sec] recovery_end = 675 #[sec] In\u00a0[\u00a0]: Copied! <pre># moving avg\ndef before_after_moving_average(data, avg_points=30):\n    '''\n    1D array\n    '''\n    process_data = data.copy()\n    original_data = data.copy()\n    def moving_average(a, n=3):\n        ret = np.cumsum(a, dtype=float)\n        ret[n:] = ret[n:] - ret[:-n]\n        return ret[n - 1:] / n\n    process_data[avg_points - 1:] = moving_average(process_data, n = avg_points)\n    process_data[:avg_points - 1] = process_data[avg_points - 1 : avg_points - 2 + avg_points]\n    return original_data, process_data\n</pre> # moving avg def before_after_moving_average(data, avg_points=30):     '''     1D array     '''     process_data = data.copy()     original_data = data.copy()     def moving_average(a, n=3):         ret = np.cumsum(a, dtype=float)         ret[n:] = ret[n:] - ret[:-n]         return ret[n - 1:] / n     process_data[avg_points - 1:] = moving_average(process_data, n = avg_points)     process_data[:avg_points - 1] = process_data[avg_points - 1 : avg_points - 2 + avg_points]     return original_data, process_data In\u00a0[3]: Copied! <pre># get processed long ch. data\ndata2 = pd.read_csv(os.path.join(\"dataset\", subject, \"SDS2\", date, exp, f'in_vivo_result_{exp}.csv')) # wl resolution = 0.171 nm, time resolution = 0.1 secs\nnp_data2 = data2.to_numpy()[:round(recovery_end/SDS2_time_resolution),2:]\nused_wl2 = [float(k.split('nm')[0]) for k in data2.keys().to_list()[2:]]\nmax_id2 = np.where(data2['peak']==1)[0]\nmin_id2 = np.where(data2['peak']==-1)[0]\n\nmax_id2 = max_id2[np.where(max_id2&lt;round(recovery_end/SDS2_time_resolution))[0]]\nmin_id2 = min_id2[np.where(min_id2&lt;round(recovery_end/SDS2_time_resolution))[0]]\n\n# get processed short ch. data &amp; sync to use same peak.\ndata = pd.read_csv(os.path.join(\"dataset\", subject, \"SDS1\", date, exp, f'in_vivo_result_{exp}.csv')) # wl resolution = 0.171 nm, time resolution =  secs\nnp_data = data.to_numpy()[:round(recovery_end/SDS1_time_resolution),2:]\nused_wl = [float(k.split('nm')[0]) for k in data.keys().to_list()[2:]]\nmax_id1 = np.where(data['peak']==1)[0]\nmin_id1 = np.where(data['peak']==-1)[0]\n\nmax_id1 = max_id1[np.where(max_id1&lt;round(recovery_end/SDS1_time_resolution))[0]]\nmin_id1 = min_id1[np.where(min_id1&lt;round(recovery_end/SDS1_time_resolution))[0]]\n</pre> # get processed long ch. data data2 = pd.read_csv(os.path.join(\"dataset\", subject, \"SDS2\", date, exp, f'in_vivo_result_{exp}.csv')) # wl resolution = 0.171 nm, time resolution = 0.1 secs np_data2 = data2.to_numpy()[:round(recovery_end/SDS2_time_resolution),2:] used_wl2 = [float(k.split('nm')[0]) for k in data2.keys().to_list()[2:]] max_id2 = np.where(data2['peak']==1)[0] min_id2 = np.where(data2['peak']==-1)[0]  max_id2 = max_id2[np.where(max_id2 In\u00a0[4]: Copied! <pre>with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\nwavelength = np.array(wavelength)\n</pre> with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength'] wavelength = np.array(wavelength) In\u00a0[5]: Copied! <pre>## adjust short ch.\n# Cubic spline interpolation\nf_interpolate = interp1d(used_wl, np_data, kind='linear', bounds_error=False, fill_value='extrapolate')\nused_wl_data = f_interpolate(wavelength)\n\n## adjust long ch.\n# get cumulate wavelength index\nacumulate_table = {}\naccmulate_range_of_wl = 2 # [nm]\nfor comp_wl in wavelength:\n    cumulate_index = []\n    for idx, each_used_wl in enumerate(used_wl2):\n        if abs(float(each_used_wl) - comp_wl) &lt; accmulate_range_of_wl:\n            cumulate_index += [idx]\n    acumulate_table[comp_wl] = cumulate_index\n\n# used cumulate wavelength index to binning\nfor idx, wl in enumerate(acumulate_table.keys()):\n    accmulate_idx = acumulate_table[wl]\n    each_wl_data = np_data2[:, accmulate_idx]\n    mean_of_each_wl_data = each_wl_data.mean(1).reshape(-1,1)\n    if idx == 0:\n        used_wl_data2 = mean_of_each_wl_data\n    else:\n        used_wl_data2 = np.concatenate((used_wl_data2, mean_of_each_wl_data), axis=1)\n</pre> ## adjust short ch. # Cubic spline interpolation f_interpolate = interp1d(used_wl, np_data, kind='linear', bounds_error=False, fill_value='extrapolate') used_wl_data = f_interpolate(wavelength)  ## adjust long ch. # get cumulate wavelength index acumulate_table = {} accmulate_range_of_wl = 2 # [nm] for comp_wl in wavelength:     cumulate_index = []     for idx, each_used_wl in enumerate(used_wl2):         if abs(float(each_used_wl) - comp_wl) &lt; accmulate_range_of_wl:             cumulate_index += [idx]     acumulate_table[comp_wl] = cumulate_index  # used cumulate wavelength index to binning for idx, wl in enumerate(acumulate_table.keys()):     accmulate_idx = acumulate_table[wl]     each_wl_data = np_data2[:, accmulate_idx]     mean_of_each_wl_data = each_wl_data.mean(1).reshape(-1,1)     if idx == 0:         used_wl_data2 = mean_of_each_wl_data     else:         used_wl_data2 = np.concatenate((used_wl_data2, mean_of_each_wl_data), axis=1) In\u00a0[6]: Copied! <pre>os.makedirs(os.path.join('pic', subject, f'{date}_invivo_result', exp), exist_ok=True)\n## plot raw data\nplt.figure(figsize=(20,8))\ntime = np.linspace(0,recovery_end, used_wl_data.shape[0])\nplt.plot(time, used_wl_data.mean(1))\nplt.axvline(x=420, linestyle='--', color='c')\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.plot(time[max_id1], used_wl_data.mean(1)[max_id1], 'r.')\nplt.plot(time[min_id1], used_wl_data.mean(1)[min_id1], 'b.')\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = np.linspace(0,recovery_end, used_wl_data2.shape[0])\nplt.plot(time, used_wl_data2.mean(1))\nplt.axvline(x=430, linestyle='--', color='c')\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.plot(time[max_id2], used_wl_data2.mean(1)[max_id2], 'r.')\nplt.plot(time[min_id2], used_wl_data2.mean(1)[min_id2], 'b.')\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> os.makedirs(os.path.join('pic', subject, f'{date}_invivo_result', exp), exist_ok=True) ## plot raw data plt.figure(figsize=(20,8)) time = np.linspace(0,recovery_end, used_wl_data.shape[0]) plt.plot(time, used_wl_data.mean(1)) plt.axvline(x=420, linestyle='--', color='c') plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.plot(time[max_id1], used_wl_data.mean(1)[max_id1], 'r.') plt.plot(time[min_id1], used_wl_data.mean(1)[min_id1], 'b.') plt.xlabel(\"time [sec]\") plt.ylabel(\"Intensity\") plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = np.linspace(0,recovery_end, used_wl_data2.shape[0]) plt.plot(time, used_wl_data2.mean(1)) plt.axvline(x=430, linestyle='--', color='c') plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.plot(time[max_id2], used_wl_data2.mean(1)[max_id2], 'r.') plt.plot(time[min_id2], used_wl_data2.mean(1)[min_id2], 'b.') plt.xlabel(\"time [sec]\") plt.ylabel(\"Intensity\") plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[7]: Copied! <pre>## plot raw data\nts = 0\ntd = round(20/SDS1_time_resolution)\nplt.figure(figsize=(20,8))\ntime = np.linspace(0,recovery_end, used_wl_data.shape[0])\nplt.plot(time[ts:td], used_wl_data[ts:td].mean(1))\nplt.plot(time[max_id1[np.where((max_id1&lt;=td)&amp;(max_id1&gt;=ts))]], used_wl_data.mean(1)[max_id1[np.where((max_id1&lt;=td)&amp;(max_id1&gt;=ts))]], 'r.', ms=10, label='R max')\nplt.plot(time[min_id1[np.where((min_id1&lt;=td)&amp;(min_id1&gt;=ts))]], used_wl_data.mean(1)[min_id1[np.where((min_id1&lt;=td)&amp;(min_id1&gt;=ts))]], 'b.', ms=10, label='R min')\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS10mm')\nplt.legend(loc='upper left', fancybox=True, shadow=True)\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS1_{ts}_{td}.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nts = 0\ntd = round(20/SDS1_time_resolution)\nplt.figure(figsize=(20,8))\ntime = np.linspace(0,recovery_end, used_wl_data2.shape[0])\nplt.plot(time[ts:td], used_wl_data2[ts:td].mean(1))\nplt.plot(time[max_id2[np.where((max_id2&lt;=td)&amp;(max_id2&gt;=ts))]], used_wl_data2.mean(1)[max_id2[np.where((max_id2&lt;=td)&amp;(max_id2&gt;=ts))]], 'r.', ms=10, label='R max')\nplt.plot(time[min_id2[np.where((min_id2&lt;=td)&amp;(min_id2&gt;=ts))]], used_wl_data2.mean(1)[min_id2[np.where((min_id2&lt;=td)&amp;(min_id2&gt;=ts))]], 'b.', ms=10, label='R min')\nplt.xlabel(\"time [sec]\")\nplt.ylabel(\"Intensity\")\nplt.title('SDS20mm')\nplt.legend(loc='upper left', fancybox=True, shadow=True)\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS2_{ts}_{td}.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> ## plot raw data ts = 0 td = round(20/SDS1_time_resolution) plt.figure(figsize=(20,8)) time = np.linspace(0,recovery_end, used_wl_data.shape[0]) plt.plot(time[ts:td], used_wl_data[ts:td].mean(1)) plt.plot(time[max_id1[np.where((max_id1&lt;=td)&amp;(max_id1&gt;=ts))]], used_wl_data.mean(1)[max_id1[np.where((max_id1&lt;=td)&amp;(max_id1&gt;=ts))]], 'r.', ms=10, label='R max') plt.plot(time[min_id1[np.where((min_id1&lt;=td)&amp;(min_id1&gt;=ts))]], used_wl_data.mean(1)[min_id1[np.where((min_id1&lt;=td)&amp;(min_id1&gt;=ts))]], 'b.', ms=10, label='R min') plt.xlabel(\"time [sec]\") plt.ylabel(\"Intensity\") plt.title('SDS10mm') plt.legend(loc='upper left', fancybox=True, shadow=True) plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS1_{ts}_{td}.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  ts = 0 td = round(20/SDS1_time_resolution) plt.figure(figsize=(20,8)) time = np.linspace(0,recovery_end, used_wl_data2.shape[0]) plt.plot(time[ts:td], used_wl_data2[ts:td].mean(1)) plt.plot(time[max_id2[np.where((max_id2&lt;=td)&amp;(max_id2&gt;=ts))]], used_wl_data2.mean(1)[max_id2[np.where((max_id2&lt;=td)&amp;(max_id2&gt;=ts))]], 'r.', ms=10, label='R max') plt.plot(time[min_id2[np.where((min_id2&lt;=td)&amp;(min_id2&gt;=ts))]], used_wl_data2.mean(1)[min_id2[np.where((min_id2&lt;=td)&amp;(min_id2&gt;=ts))]], 'b.', ms=10, label='R min') plt.xlabel(\"time [sec]\") plt.ylabel(\"Intensity\") plt.title('SDS20mm') plt.legend(loc='upper left', fancybox=True, shadow=True) plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS2_{ts}_{td}.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[8]: Copied! <pre>plt.rcParams.update({'font.size': 12})\nts = 180\ntd = round(200/SDS1_time_resolution)\nplt.figure(figsize=(8,6))\nplt.plot(wavelength, used_wl_data[max_id1[np.where((max_id1&lt;=td)&amp;(max_id1&gt;=ts))]].mean(0), 'o-', label='R max')\nplt.plot(wavelength, used_wl_data[min_id1[np.where((min_id1&lt;=td)&amp;(min_id1&gt;=ts))]].mean(0), 'o-', label='R min')\nplt.xlabel('wavelength')\nplt.ylabel('Intensity')\nplt.title('SDS10mm')\nplt.legend(loc='upper left', fancybox=True, shadow=True)\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS1_{ts}_{td}_spec.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nts = 180\ntd = round(200/SDS2_time_resolution)\nplt.figure(figsize=(8,6))\nplt.plot(wavelength, used_wl_data2[max_id2[np.where((max_id2&lt;=td)&amp;(max_id2&gt;=ts))]].mean(0), 'o-', label='R max')\nplt.plot(wavelength, used_wl_data2[min_id2[np.where((min_id2&lt;=td)&amp;(min_id2&gt;=ts))]].mean(0), 'o-', label='R min')\nplt.xlabel('wavelength')\nplt.ylabel('Intensity')\nplt.title('SDS20mm')\nplt.legend(loc='upper left', fancybox=True, shadow=True)\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS2_{ts}_{td}_spec.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.rcParams.update({'font.size': 20})\n</pre> plt.rcParams.update({'font.size': 12}) ts = 180 td = round(200/SDS1_time_resolution) plt.figure(figsize=(8,6)) plt.plot(wavelength, used_wl_data[max_id1[np.where((max_id1&lt;=td)&amp;(max_id1&gt;=ts))]].mean(0), 'o-', label='R max') plt.plot(wavelength, used_wl_data[min_id1[np.where((min_id1&lt;=td)&amp;(min_id1&gt;=ts))]].mean(0), 'o-', label='R min') plt.xlabel('wavelength') plt.ylabel('Intensity') plt.title('SDS10mm') plt.legend(loc='upper left', fancybox=True, shadow=True) plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS1_{ts}_{td}_spec.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  ts = 180 td = round(200/SDS2_time_resolution) plt.figure(figsize=(8,6)) plt.plot(wavelength, used_wl_data2[max_id2[np.where((max_id2&lt;=td)&amp;(max_id2&gt;=ts))]].mean(0), 'o-', label='R max') plt.plot(wavelength, used_wl_data2[min_id2[np.where((min_id2&lt;=td)&amp;(min_id2&gt;=ts))]].mean(0), 'o-', label='R min') plt.xlabel('wavelength') plt.ylabel('Intensity') plt.title('SDS20mm') plt.legend(loc='upper left', fancybox=True, shadow=True) plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'raw_SDS2_{ts}_{td}_spec.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.rcParams.update({'font.size': 20}) In\u00a0[9]: Copied! <pre># plot all the wavelength\nplt.figure(figsize=(20,8))\ntime = [i*SDS1_time_resolution for i in range(used_wl_data.shape[0])]\nfor i in range(20):\n    plt.plot(time, used_wl_data[:,i], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True, ncol=2)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_each_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS2_time_resolution for i in range(used_wl_data2.shape[0])]\nfor i in range(20):\n    plt.plot(time,used_wl_data2[:,i], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True, ncol=2)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_each_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # plot all the wavelength plt.figure(figsize=(20,8)) time = [i*SDS1_time_resolution for i in range(used_wl_data.shape[0])] for i in range(20):     plt.plot(time, used_wl_data[:,i], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True, ncol=2) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_each_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = [i*SDS2_time_resolution for i in range(used_wl_data2.shape[0])] for i in range(20):     plt.plot(time,used_wl_data2[:,i], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True, ncol=2) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_each_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[11]: Copied! <pre># plot interested wavelength [763nm , 805nm, 850nm]\nplt.figure(figsize=(20,8))\ntime = [i*SDS1_time_resolution for i in range(used_wl_data.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data[:,i])\n    plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)\n    plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS2_time_resolution for i in range(used_wl_data2.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data2[:,i])\n    plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)\n    plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\n\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS1_time_resolution for i in range(used_wl_data.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data[:,i])\n    plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)\n    plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Normalized Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_normalized_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS2_time_resolution for i in range(used_wl_data2.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data2[:,i])\n    plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)\n    plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Normalized Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_normalized_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # plot interested wavelength [763nm , 805nm, 850nm] plt.figure(figsize=(20,8)) time = [i*SDS1_time_resolution for i in range(used_wl_data.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data[:,i])     plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)     plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = [i*SDS2_time_resolution for i in range(used_wl_data2.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data2[:,i])     plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)     plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show()    plt.figure(figsize=(20,8)) time = [i*SDS1_time_resolution for i in range(used_wl_data.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data[:,i])     plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)     plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Normalized Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_normalized_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = [i*SDS2_time_resolution for i in range(used_wl_data2.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=used_wl_data2[:,i])     plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)     plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Normalized Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'raw_normalized_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  In\u00a0[12]: Copied! <pre>from scipy.interpolate import CubicSpline\n\nused_bloodConc = 138\nepsilonHbO2HbPath = os.path.join(\"OPs_used\", \"absorption\", \"epsilon.txt\")\nepsilonHbO2Hb = pd.read_csv(epsilonHbO2HbPath, sep=\" \", names=[\"wl\", \"HbO2\", \"Hb\"])\ncs = CubicSpline(epsilonHbO2Hb.wl.values,\n                    epsilonHbO2Hb.HbO2.values, extrapolate=False)\nepsilonHbO2Used = cs(wavelength)  # [cm-1/M]\ncs = CubicSpline(epsilonHbO2Hb.wl.values,\n                    epsilonHbO2Hb.Hb.values, extrapolate=False)\nepsilonHbUsed = cs(wavelength)  # [cm-1/M]\nmuaHbO2Set = 2.303 * epsilonHbO2Used * (used_bloodConc / 64532) *0.1 # [1/mm]\nmuaHbSet = 2.303 * epsilonHbUsed * (used_bloodConc / 64500) *0.1  # [1/mm]\n\nijv_ppath = np.load(os.path.join(\"OPs_used\", \"ijv_small_spec_ppath_set_SDS20.npy\"))\n</pre> from scipy.interpolate import CubicSpline  used_bloodConc = 138 epsilonHbO2HbPath = os.path.join(\"OPs_used\", \"absorption\", \"epsilon.txt\") epsilonHbO2Hb = pd.read_csv(epsilonHbO2HbPath, sep=\" \", names=[\"wl\", \"HbO2\", \"Hb\"]) cs = CubicSpline(epsilonHbO2Hb.wl.values,                     epsilonHbO2Hb.HbO2.values, extrapolate=False) epsilonHbO2Used = cs(wavelength)  # [cm-1/M] cs = CubicSpline(epsilonHbO2Hb.wl.values,                     epsilonHbO2Hb.Hb.values, extrapolate=False) epsilonHbUsed = cs(wavelength)  # [cm-1/M] muaHbO2Set = 2.303 * epsilonHbO2Used * (used_bloodConc / 64532) *0.1 # [1/mm] muaHbSet = 2.303 * epsilonHbUsed * (used_bloodConc / 64500) *0.1  # [1/mm]  ijv_ppath = np.load(os.path.join(\"OPs_used\", \"ijv_small_spec_ppath_set_SDS20.npy\"))  In\u00a0[13]: Copied! <pre>plt.rcParams.update({'font.size': 12})\nos.makedirs(os.path.join('pic', subject, f'{date}_invivo_result', 'MBLL'), exist_ok=True)\nbase_time = 10 # sec\naverage_time = 10\ntime_resolution = SDS1_time_resolution\ntime_resolution2 = SDS2_time_resolution\nbased_sds1_ijv_small = used_wl_data[max_id1[np.where(abs(max_id1-round(base_time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\nbased_sds1_ijv_large = used_wl_data[min_id1[np.where(abs(min_id1-round(base_time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\nbased_sds2_ijv_small = used_wl_data2[max_id2[np.where(abs(max_id2-round(base_time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\nbased_sds2_ijv_large = used_wl_data2[min_id2[np.where(abs(min_id2-round(base_time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\n\nalpha = []\nbeta = []\nMBLL_SO2 = {'time':[],\n            'delta_SO2_20_10':[],\n            'error_SO2_20_10':[],\n            'delta_SO2_20_7':[],\n            'error_SO2_20_7':[],\n            'delta_SO2_20_4':[],\n            'error_SO2_20_4':[],}\nfor change_time in range(30,840,10):\n    average_time = 6\n    change_sds1_ijv_small = used_wl_data[max_id1[np.where(abs(max_id1-round(change_time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\n    change_sds1_ijv_large = used_wl_data[min_id1[np.where(abs(min_id1-round(change_time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\n    change_sds2_ijv_small = used_wl_data2[max_id2[np.where(abs(max_id2-round(change_time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\n    change_sds2_ijv_large = used_wl_data2[min_id2[np.where(abs(min_id2-round(change_time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\n\n    delta_OD_sds1_ijv_small = np.log(based_sds1_ijv_small/change_sds1_ijv_small)\n    delta_OD_sds2_ijv_small = np.log(based_sds2_ijv_small/change_sds2_ijv_small)\n\n    wavelength_805nm_idx = 16\n    wavelength_start_idx = 3\n\n    delta_OD_20_10 = np.log((based_sds2_ijv_small/based_sds1_ijv_small)/ (change_sds2_ijv_small/change_sds1_ijv_small))\n    delta_OD_matrix = (delta_OD_20_10[wavelength_start_idx:] - delta_OD_20_10[wavelength_805nm_idx-wavelength_start_idx]) # move 805nm to zero\n    epsilon_matrix = (muaHbO2Set[wavelength_start_idx:].reshape(-1,1) + (-1*muaHbSet[wavelength_start_idx:].reshape(-1,1))) \n    pseudo_inverse = np.matmul(np.linalg.inv(np.matmul(epsilon_matrix.T,epsilon_matrix)), epsilon_matrix.T)\n    delta_SO2 = np.matmul(pseudo_inverse, delta_OD_matrix)\n    fit_delta_OD = np.matmul(epsilon_matrix, delta_SO2) + delta_OD_20_10[wavelength_805nm_idx-wavelength_start_idx]\n    RMSPE = np.sqrt(np.mean(np.square((fit_delta_OD - delta_OD_20_10[wavelength_start_idx:])/delta_OD_20_10[wavelength_start_idx:])))*100\n    \n    plt.figure(figsize=(12,8))\n    plt.plot(wavelength[wavelength_start_idx:], fit_delta_OD, 'ro--', alpha=0.9, label=f'fitted_delta_OD (20mm, 10mm) $\\Delta$SO2={100*delta_SO2[0]:.1f}%')\n    plt.plot(wavelength, delta_OD_20_10, 'r',label='ratio_of_ratio_delta_OD (20mm, 10mm)')\n    plt.title(f't1={base_time}s, t2={change_time}s \\n baseline:0s~60s,HP:60s~240s,recovery:240s~840s \\n $\\Delta$SO2 = {100*delta_SO2[0]:.1f}%, RMSPE = {RMSPE:.4f}%')\n    plt.xlabel('wavelength (nm)')\n    plt.ylabel('\\u0394OD')\n    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n            fancybox=True, shadow=True)\n    plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result',  'MBLL', f'MBLL_{change_time}s.png'), dpi=300, format='png', bbox_inches='tight')\n    plt.close()\n    # plt.show()\n    MBLL_SO2['time'] += [change_time]\n    MBLL_SO2['delta_SO2_20_10'] += [100*delta_SO2[0]]\n    MBLL_SO2['error_SO2_20_10'] += [RMSPE]\n</pre> plt.rcParams.update({'font.size': 12}) os.makedirs(os.path.join('pic', subject, f'{date}_invivo_result', 'MBLL'), exist_ok=True) base_time = 10 # sec average_time = 10 time_resolution = SDS1_time_resolution time_resolution2 = SDS2_time_resolution based_sds1_ijv_small = used_wl_data[max_id1[np.where(abs(max_id1-round(base_time/time_resolution)) <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24004\\2433222078.py:35: RuntimeWarning: invalid value encountered in log\n  delta_OD_sds1_ijv_small = np.log(based_sds1_ijv_small/change_sds1_ijv_small)\nC:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24004\\2433222078.py:47: RuntimeWarning: invalid value encountered in log\n  delta_OD_20_10 = np.log((based_sds2_ijv_small/based_sds1_ijv_small)/ (change_sds2_ijv_small/change_sds1_ijv_small))\nC:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24004\\2433222078.py:26: RuntimeWarning: Mean of empty slice.\n  change_sds1_ijv_small = used_wl_data[max_id1[np.where(abs(max_id1-round(change_time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\nd:\\ijv_code_organized\\venv\\lib\\site-packages\\numpy\\core\\_methods.py:184: RuntimeWarning: invalid value encountered in divide\n  ret = um.true_divide(\nC:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24004\\2433222078.py:27: RuntimeWarning: Mean of empty slice.\n  change_sds1_ijv_large = used_wl_data[min_id1[np.where(abs(min_id1-round(change_time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\nC:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24004\\2433222078.py:28: RuntimeWarning: Mean of empty slice.\n  change_sds2_ijv_small = used_wl_data2[max_id2[np.where(abs(max_id2-round(change_time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\nC:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_24004\\2433222078.py:29: RuntimeWarning: Mean of empty slice.\n  change_sds2_ijv_large = used_wl_data2[min_id2[np.where(abs(min_id2-round(change_time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\n</pre> In\u00a0[14]: Copied! <pre>time = [i for i in range(30,840,10)]\nplt.figure(figsize=(8,6))\nplt.plot(time, MBLL_SO2['error_SO2_20_10'], 'o-', label='fit error 20mm-10mm')\na = MBLL_SO2['error_SO2_20_10'].copy()\na.sort()\nplt.ylim((0, a[-20]))\nplt.ylabel('RMSPE(%)')\nplt.xlabel('time [sec]')\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result',  exp, 'MBLL', f'fitting_RMSPE_result.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> time = [i for i in range(30,840,10)] plt.figure(figsize=(8,6)) plt.plot(time, MBLL_SO2['error_SO2_20_10'], 'o-', label='fit error 20mm-10mm') a = MBLL_SO2['error_SO2_20_10'].copy() a.sort() plt.ylim((0, a[-20])) plt.ylabel('RMSPE(%)') plt.xlabel('time [sec]') plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result',  exp, 'MBLL', f'fitting_RMSPE_result.png'), dpi=300, format='png', bbox_inches='tight') plt.show() <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nd:\\ijv_code_organized\\Internal-Jugular-Vein-Project\\test_prediction_model_in_vivo\\S2_predict_measured_data.ipynb \u5132\u5b58\u683c 18 line 1\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X23sZmlsZQ%3D%3D?line=9'&gt;10&lt;/a&gt; a = MBLL_SO2['error_SO2_20_10'].copy()\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X23sZmlsZQ%3D%3D?line=10'&gt;11&lt;/a&gt; a.sort()\n---&gt; &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X23sZmlsZQ%3D%3D?line=11'&gt;12&lt;/a&gt; plt.ylim((0, a[-20]))\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X23sZmlsZQ%3D%3D?line=12'&gt;13&lt;/a&gt; plt.ylabel('RMSPE(%)')\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X23sZmlsZQ%3D%3D?line=13'&gt;14&lt;/a&gt; plt.xlabel('time [sec]')\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\matplotlib\\pyplot.py:1831, in ylim(*args, **kwargs)\n   1829 if not args and not kwargs:\n   1830     return ax.get_ylim()\n-&gt; 1831 ret = ax.set_ylim(*args, **kwargs)\n   1832 return ret\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\matplotlib\\_api\\deprecation.py:454, in make_keyword_only.&lt;locals&gt;.wrapper(*args, **kwargs)\n    448 if len(args) &gt; name_idx:\n    449     warn_deprecated(\n    450         since, message=\"Passing the %(name)s %(obj_type)s \"\n    451         \"positionally is deprecated since Matplotlib %(since)s; the \"\n    452         \"parameter will become keyword-only %(removal)s.\",\n    453         name=name, obj_type=f\"parameter of {func.__name__}()\")\n--&gt; 454 return func(*args, **kwargs)\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:3882, in _AxesBase.set_ylim(self, bottom, top, emit, auto, ymin, ymax)\n   3880         raise TypeError(\"Cannot pass both 'top' and 'ymax'\")\n   3881     top = ymax\n-&gt; 3882 return self.yaxis._set_lim(bottom, top, emit=emit, auto=auto)\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\matplotlib\\axis.py:1185, in Axis._set_lim(self, v0, v1, emit, auto)\n   1183 self.axes._process_unit_info([(name, (v0, v1))], convert=False)\n   1184 v0 = self.axes._validate_converted_limits(v0, self.convert_units)\n-&gt; 1185 v1 = self.axes._validate_converted_limits(v1, self.convert_units)\n   1187 if v0 is None or v1 is None:\n   1188     # Axes init calls set_xlim(0, 1) before get_xlim() can be called,\n   1189     # so only grab the limits if we really need them.\n   1190     old0, old1 = self.get_view_interval()\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\matplotlib\\axes\\_base.py:3570, in _AxesBase._validate_converted_limits(self, limit, convert)\n   3567 converted_limit = convert(limit)\n   3568 if (isinstance(converted_limit, Real)\n   3569         and not np.isfinite(converted_limit)):\n-&gt; 3570     raise ValueError(\"Axis limits cannot be NaN or Inf\")\n   3571 return converted_limit\n\nValueError: Axis limits cannot be NaN or Inf</pre> In\u00a0[15]: Copied! <pre>plt.rcParams.update({'font.size': 12})\n\nblood_spec = pd.read_csv(os.path.join('OPs_used', 'ijv_mua_bloodConc_138.csv'))\nused_SO2 = blood_spec['SO2'].to_numpy()\nblood_spec = blood_spec.to_numpy()[:,1:]\nplt.figure(figsize=(8,6))\nfor i in range(0,blood_spec.shape[0],10):\n    plt.plot(wavelength, blood_spec[i], 'ro-', alpha=(i/(2*blood_spec.shape[0])+0.3),label=f'SO2 = {used_SO2[i]*100:2.0f}%')\nplt.xlabel('wavelength (nm)')\nplt.ylabel('absorption coefficient (1/mm)')\nplt.legend()\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'blood_absorption.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.rcParams.update({'font.size': 12})  blood_spec = pd.read_csv(os.path.join('OPs_used', 'ijv_mua_bloodConc_138.csv')) used_SO2 = blood_spec['SO2'].to_numpy() blood_spec = blood_spec.to_numpy()[:,1:] plt.figure(figsize=(8,6)) for i in range(0,blood_spec.shape[0],10):     plt.plot(wavelength, blood_spec[i], 'ro-', alpha=(i/(2*blood_spec.shape[0])+0.3),label=f'SO2 = {used_SO2[i]*100:2.0f}%') plt.xlabel('wavelength (nm)') plt.ylabel('absorption coefficient (1/mm)') plt.legend() plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'blood_absorption.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[\u00a0]: Copied! <pre>for i in range(0,blood_spec.shape[0],10):\n    plt.plot(wavelength, blood_spec[i] - blood_spec[30], 'ro-', alpha=(i/(2*blood_spec.shape[0])+0.3),label=f'$\\Delta$SO2 = {(used_SO2[i]-0.7)*100:2.0f}%')\nplt.xlabel('wavelength (nm)')\nplt.ylabel('delta absorption coefficient (1/mm)')\nplt.legend()\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'blood_delta_absorption.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.rcParams.update({'font.size': 20})\n</pre> for i in range(0,blood_spec.shape[0],10):     plt.plot(wavelength, blood_spec[i] - blood_spec[30], 'ro-', alpha=(i/(2*blood_spec.shape[0])+0.3),label=f'$\\Delta$SO2 = {(used_SO2[i]-0.7)*100:2.0f}%') plt.xlabel('wavelength (nm)') plt.ylabel('delta absorption coefficient (1/mm)') plt.legend() plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, f'blood_delta_absorption.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.rcParams.update({'font.size': 20}) In\u00a0[\u00a0]: Copied! <pre>import cv2 \nimport os\nimport random\nfrom glob import glob\n\nimg = cv2.imread(os.path.join('pic', subject, f'{date}_invivo_result',  exp, 'MBLL', 'MBLL_30s.png'))\nfps = 3\nsize = (img.shape[1], img.shape[0]) # \u5bec \u548c \u9ad8\nprint(size)\nfourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') #\u5f71\u7247\u7de8\u78bc\u683c\u5f0f\nvideoWrite = cv2.VideoWriter(os.path.join('pic', subject, f'{date}_invivo_result', exp,  'MBLL', 'video.mp4'), fourcc, fps, size)\n\nfiles = glob(os.path.join('pic', subject, f'{date}_invivo_result',  exp, 'MBLL', \"MBLL_*.png\"))\nfiles = sorted(files, key=lambda x: int(x.split('.')[-2].split('_')[-1].split('s')[-2]))\n# images = [img for img in os.listdir(os.path.join('pic', subject, f'{date}_invivo_result',  'MBLL')) if img.endswith(\".png\")]\nout_num = len(files)\nfor i in range(0, out_num, 1):\n    fileName = files[i]\n    img = cv2.imread(fileName)\n    img = cv2.resize(img, size, interpolation = cv2.INTER_AREA)\n    videoWrite.write(img)\nvideoWrite.release()\n</pre> import cv2  import os import random from glob import glob  img = cv2.imread(os.path.join('pic', subject, f'{date}_invivo_result',  exp, 'MBLL', 'MBLL_30s.png')) fps = 3 size = (img.shape[1], img.shape[0]) # \u5bec \u548c \u9ad8 print(size) fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v') #\u5f71\u7247\u7de8\u78bc\u683c\u5f0f videoWrite = cv2.VideoWriter(os.path.join('pic', subject, f'{date}_invivo_result', exp,  'MBLL', 'video.mp4'), fourcc, fps, size)  files = glob(os.path.join('pic', subject, f'{date}_invivo_result',  exp, 'MBLL', \"MBLL_*.png\")) files = sorted(files, key=lambda x: int(x.split('.')[-2].split('_')[-1].split('s')[-2])) # images = [img for img in os.listdir(os.path.join('pic', subject, f'{date}_invivo_result',  'MBLL')) if img.endswith(\".png\")] out_num = len(files) for i in range(0, out_num, 1):     fileName = files[i]     img = cv2.imread(fileName)     img = cv2.resize(img, size, interpolation = cv2.INTER_AREA)     videoWrite.write(img) videoWrite.release() <pre>(4416, 2231)\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nd:\\ijv_code_organized\\Internal-Jugular-Vein-Project\\test_prediction_model_in_vivo\\S2_predict_measured_data.ipynb \u5132\u5b58\u683c 21 line 1\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X26sZmlsZQ%3D%3D?line=16'&gt;17&lt;/a&gt; for i in range(0, out_num, 1):\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X26sZmlsZQ%3D%3D?line=17'&gt;18&lt;/a&gt;     fileName = files[i]\n---&gt; &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X26sZmlsZQ%3D%3D?line=18'&gt;19&lt;/a&gt;     img = cv2.imread(fileName)\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X26sZmlsZQ%3D%3D?line=19'&gt;20&lt;/a&gt;     img = cv2.resize(img, size, interpolation = cv2.INTER_AREA)\n     &lt;a href='vscode-notebook-cell:/d%3A/ijv_code_organized/Internal-Jugular-Vein-Project/test_prediction_model_in_vivo/S2_predict_measured_data.ipynb#X26sZmlsZQ%3D%3D?line=20'&gt;21&lt;/a&gt;     videoWrite.write(img)\n\nKeyboardInterrupt: </pre> In\u00a0[6]: Copied! <pre># cali short ch.\ncali = pd.read_csv(os.path.join('dataset', subject, 'calibration_result', date, 'calibrate_SDS_1.csv'))\ncali = cali.to_numpy()\ncali_used_wl_data = (used_wl_data*cali[0] + cali[1])\n\n# cali long ch.\ncali2 = pd.read_csv(os.path.join('dataset', subject, 'calibration_result', date, 'calibrate_SDS_2.csv'))\ncali2 = cali2.to_numpy()\ncali_used_wl_data2 = (used_wl_data2*cali2[0] + cali2[1])\n</pre> # cali short ch. cali = pd.read_csv(os.path.join('dataset', subject, 'calibration_result', date, 'calibrate_SDS_1.csv')) cali = cali.to_numpy() cali_used_wl_data = (used_wl_data*cali[0] + cali[1])  # cali long ch. cali2 = pd.read_csv(os.path.join('dataset', subject, 'calibration_result', date, 'calibrate_SDS_2.csv')) cali2 = cali2.to_numpy() cali_used_wl_data2 = (used_wl_data2*cali2[0] + cali2[1])  In\u00a0[17]: Copied! <pre># plot calibrated data\nplt.figure(figsize=(20,8))\ntime = np.linspace(0,recovery_end, cali_used_wl_data.shape[0])\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.plot(time, cali_used_wl_data.mean(1))\nplt.plot(time[max_id1], cali_used_wl_data.mean(1)[max_id1], 'r.')\nplt.plot(time[min_id1], cali_used_wl_data.mean(1)[min_id1], 'b.')\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = np.linspace(0,recovery_end, cali_used_wl_data2.shape[0])\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.plot(time, cali_used_wl_data2.mean(1))\nplt.plot(time[max_id2], cali_used_wl_data2.mean(1)[max_id2], 'r.')\nplt.plot(time[min_id2], cali_used_wl_data2.mean(1)[min_id2], 'b.')\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # plot calibrated data plt.figure(figsize=(20,8)) time = np.linspace(0,recovery_end, cali_used_wl_data.shape[0]) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.plot(time, cali_used_wl_data.mean(1)) plt.plot(time[max_id1], cali_used_wl_data.mean(1)[max_id1], 'r.') plt.plot(time[min_id1], cali_used_wl_data.mean(1)[min_id1], 'b.') plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = np.linspace(0,recovery_end, cali_used_wl_data2.shape[0]) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.plot(time, cali_used_wl_data2.mean(1)) plt.plot(time[max_id2], cali_used_wl_data2.mean(1)[max_id2], 'r.') plt.plot(time[min_id2], cali_used_wl_data2.mean(1)[min_id2], 'b.') plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[18]: Copied! <pre># plot all the wavelength\nplt.figure(figsize=(20,8))\ntime = [i*SDS1_time_resolution for i in range(cali_used_wl_data.shape[0])]\nfor i in range(20):\n    plt.plot(time, cali_used_wl_data[:,i], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True, ncol=2)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_each_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS2_time_resolution for i in range(cali_used_wl_data2.shape[0])]\nfor i in range(20):\n    plt.plot(time,cali_used_wl_data2[:,i], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True, ncol=2)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_each_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # plot all the wavelength plt.figure(figsize=(20,8)) time = [i*SDS1_time_resolution for i in range(cali_used_wl_data.shape[0])] for i in range(20):     plt.plot(time, cali_used_wl_data[:,i], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True, ncol=2) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_each_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = [i*SDS2_time_resolution for i in range(cali_used_wl_data2.shape[0])] for i in range(20):     plt.plot(time,cali_used_wl_data2[:,i], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True, ncol=2) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_each_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[19]: Copied! <pre># plot interested wavelength [763nm , 805nm, 850nm]\nplt.figure(figsize=(20,8))\ntime = [i*SDS1_time_resolution for i in range(cali_used_wl_data.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data[:,i])\n    plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)\n    plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS2_time_resolution for i in range(cali_used_wl_data2.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data2[:,i])\n    plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)\n    plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\n\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS1_time_resolution for i in range(cali_used_wl_data.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data[:,i])\n    plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)\n    plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Normalized Intensity')\nplt.title('SDS10mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_normalized_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nplt.figure(figsize=(20,8))\ntime = [i*SDS2_time_resolution for i in range(cali_used_wl_data2.shape[0])]\ncolor = ['blue', 'green', 'red']\nfor c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):\n    BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data2[:,i])\n    plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)\n    plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')\n\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Normalized Intensity')\nplt.title('SDS20mm')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_normalized_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # plot interested wavelength [763nm , 805nm, 850nm] plt.figure(figsize=(20,8)) time = [i*SDS1_time_resolution for i in range(cali_used_wl_data.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data[:,i])     plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)     plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = [i*SDS2_time_resolution for i in range(cali_used_wl_data2.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data2[:,i])     plt.plot(time, BF_used_wl_data, color=color[c_idx], alpha=0.3)     plt.plot(time, AF_used_wl_data, color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show()    plt.figure(figsize=(20,8)) time = [i*SDS1_time_resolution for i in range(cali_used_wl_data.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data[:,i])     plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)     plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Normalized Intensity') plt.title('SDS10mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_normalized_focus_wl_SDS1.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  plt.figure(figsize=(20,8)) time = [i*SDS2_time_resolution for i in range(cali_used_wl_data2.shape[0])] color = ['blue', 'green', 'red'] for c_idx, i in enumerate([np.where(wavelength==763)[0][0], np.where(wavelength==805)[0][0], np.where(wavelength==850)[0][0]]):     BF_used_wl_data, AF_used_wl_data = before_after_moving_average(data=cali_used_wl_data2[:,i])     plt.plot(time, (BF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], alpha=0.1)     plt.plot(time, (AF_used_wl_data- BF_used_wl_data.min())/(BF_used_wl_data.max()-BF_used_wl_data.min()), color=color[c_idx], label=f'{wavelength[i]}nm')  plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Normalized Intensity') plt.title('SDS20mm') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cali_normalized_focus_wl_SDS2.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  In\u00a0[20]: Copied! <pre>SDS1_Rmax_Rmin_data = []\nSDS2_Rmax_Rmin_data = []\nfor ts in range(0, recovery_end, 10):\n    td = ts + 10\n    used_max_idx1 = np.where(((max_id1&lt;round(td/SDS1_time_resolution))&amp;(max_id1&gt;=round(ts/SDS1_time_resolution))))\n    used_min_idx1 = np.where(((min_id1&lt;round(td/SDS1_time_resolution))&amp;(min_id1&gt;=round(ts/SDS1_time_resolution))))\n    SDS1_Rmax_Rmin_data += [used_wl_data[max_id1[used_max_idx1]].mean() / used_wl_data[min_id1[used_min_idx1]].mean()]\n    \n    used_max_idx2 = np.where(((max_id2&lt;round(td/SDS1_time_resolution))&amp;(max_id2&gt;=round(ts/SDS1_time_resolution))))\n    used_min_idx2 = np.where(((min_id2&lt;round(td/SDS1_time_resolution))&amp;(min_id2&gt;=round(ts/SDS1_time_resolution))))\n    SDS2_Rmax_Rmin_data += [used_wl_data2[max_id2[used_max_idx2]].mean() / used_wl_data2[min_id2[used_min_idx2]].mean()]\n</pre> SDS1_Rmax_Rmin_data = [] SDS2_Rmax_Rmin_data = [] for ts in range(0, recovery_end, 10):     td = ts + 10     used_max_idx1 = np.where(((max_id1=round(ts/SDS1_time_resolution))))     used_min_idx1 = np.where(((min_id1=round(ts/SDS1_time_resolution))))     SDS1_Rmax_Rmin_data += [used_wl_data[max_id1[used_max_idx1]].mean() / used_wl_data[min_id1[used_min_idx1]].mean()]          used_max_idx2 = np.where(((max_id2=round(ts/SDS1_time_resolution))))     used_min_idx2 = np.where(((min_id2=round(ts/SDS1_time_resolution))))     SDS2_Rmax_Rmin_data += [used_wl_data2[max_id2[used_max_idx2]].mean() / used_wl_data2[min_id2[used_min_idx2]].mean()] In\u00a0[21]: Copied! <pre>plt.rcParams.update({'font.size': 12})\ntime = np.linspace(0, recovery_end, len(SDS1_Rmax_Rmin_data))\nplt.plot(time, SDS1_Rmax_Rmin_data, label='SDS=10mm')\nplt.plot(time, SDS2_Rmax_Rmin_data, label='SDS=20mm')\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end')\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.xlabel('time [sec]')\nplt.ylabel('Rmax/Rmin')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp,'Rmax_Rmin.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.rcParams.update({'font.size': 12}) time = np.linspace(0, recovery_end, len(SDS1_Rmax_Rmin_data)) plt.plot(time, SDS1_Rmax_Rmin_data, label='SDS=10mm') plt.plot(time, SDS2_Rmax_Rmin_data, label='SDS=20mm') plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=recovery_end, linestyle='--', color='g', label='recovery_end') plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.xlabel('time [sec]') plt.ylabel('Rmax/Rmin') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp,'Rmax_Rmin.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[7]: Copied! <pre>plt.rcParams.update({'font.size': 12})\n</pre> plt.rcParams.update({'font.size': 12}) In\u00a0[8]: Copied! <pre># load model \nimport torch\nfrom ANN_models import PredictionModel, PredictionModel2, PredictionModel3, PredictionModel4, PredictionModel5, PredictionModel6\nwith open(os.path.join(\"model_save\", \"prediction_model_formula24\", subject, \"trlog.json\"), 'r') as f:\n    trlog = json.load(f)\n    best_model = trlog['best_model']\nmodel = PredictionModel5(neuronsize=5).cuda()\nmodel.load_state_dict(torch.load(best_model))\n</pre> # load model  import torch from ANN_models import PredictionModel, PredictionModel2, PredictionModel3, PredictionModel4, PredictionModel5, PredictionModel6 with open(os.path.join(\"model_save\", \"prediction_model_formula24\", subject, \"trlog.json\"), 'r') as f:     trlog = json.load(f)     best_model = trlog['best_model'] model = PredictionModel5(neuronsize=5).cuda() model.load_state_dict(torch.load(best_model)) Out[8]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[9]: Copied! <pre>def get_OD(used_wl_data, used_wl_data2, time, average_time=6):\n    time_resolution = SDS1_time_resolution\n    time_resolution2 = SDS2_time_resolution\n    sds1_ijv_small = used_wl_data[max_id1[np.where(abs(max_id1-round(time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\n    sds1_ijv_large = used_wl_data[min_id1[np.where(abs(min_id1-round(time/time_resolution))&lt;round(average_time/time_resolution))]].mean(0)\n    \n    sds2_ijv_small = used_wl_data2[max_id2[np.where(abs(max_id2-round(time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\n    sds2_ijv_large = used_wl_data2[min_id2[np.where(abs(min_id2-round(time/time_resolution2))&lt;round(average_time/time_resolution2))]].mean(0)\n    \n    # sds1_ijv_small = used_wl_data[max_id[idx:idx+average_point]].mean(0)\n    # sds1_ijv_large = used_wl_data[min_id[idx:idx+average_point]].mean(0)\n\n    # sds2_ijv_small = used_wl_data2[max_id2[idx:idx+average_point]].mean(0)\n    # sds2_ijv_large = used_wl_data2[min_id2[idx:idx+average_point]].mean(0)\n    OD_spec = []\n\n    # for sds2_large in sds2_ijv_large:\n    #     for sds1_large in sds1_ijv_large:\n    #         OD_spec += [sds1_large/sds2_large]\n\n    # for sds2_small in sds2_ijv_small:\n    #     for sds1_small in sds1_ijv_small:\n    #         OD_spec += [sds1_small/sds2_small]\n    for sds1_large in sds1_ijv_large:\n        for sds2_large in sds2_ijv_large:\n            OD_spec += [sds2_large/sds1_large]\n            \n    for sds1_small in sds1_ijv_small:\n        for sds2_small in sds2_ijv_small:\n            OD_spec += [sds2_small/sds1_small]\n\n    \n    return np.array(OD_spec)\n</pre> def get_OD(used_wl_data, used_wl_data2, time, average_time=6):     time_resolution = SDS1_time_resolution     time_resolution2 = SDS2_time_resolution     sds1_ijv_small = used_wl_data[max_id1[np.where(abs(max_id1-round(time/time_resolution)) In\u00a0[10]: Copied! <pre>total_predict = []\nusing_time = []\nfor idx, time in enumerate(range(10,recovery_end-50,10)):\n    using_time += [time]\n    OD1_spec = get_OD(used_wl_data=cali_used_wl_data, \n                      used_wl_data2=cali_used_wl_data2, \n                      time=time,\n                      average_time=6)\n    \n    OD2_spec = get_OD(used_wl_data=cali_used_wl_data, \n                      used_wl_data2=cali_used_wl_data2, \n                      time=time+10,\n                      average_time=6)\n    if idx == 0:\n        result_OD1_spec = OD1_spec.reshape(1,-1)\n        result_OD2_spec = OD2_spec.reshape(1,-1)\n    else:\n        result_OD1_spec = np.concatenate((result_OD1_spec, OD1_spec.reshape(1,-1)))\n        result_OD2_spec = np.concatenate((result_OD2_spec, OD2_spec.reshape(1,-1)))\n        \n    \n    # # # normalize \n    # for i in range(40):\n    #     OD1_spec[i*20:i*20+20] = (OD1_spec[i*20:i*20+20] - OD1_spec[i*20:i*20+20].mean()) / (OD1_spec[i*20:i*20+20].max() - OD1_spec[i*20:i*20+20].min())\n    \n    # for i in range(40):\n    #     OD2_spec[i*20:i*20+20] = (OD2_spec[i*20:i*20+20] - OD2_spec[i*20:i*20+20].mean()) / (OD2_spec[i*20:i*20+20].max() - OD2_spec[i*20:i*20+20].min())\n        \n    # delta_OD = OD2_spec - OD1_spec\n    delta_OD = OD1_spec/OD2_spec\n    \n    # # normalize \n    # for i in range(40):\n    #     delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].mean()) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min())\n    \n    # normalize \n    for i in range(40):\n        delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].min() + 1e-9) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min() + 1e-9)\n    delta_OD = np.log(delta_OD)\n    \n    \n    \n    \n    model_input = torch.tensor(delta_OD)\n    model_input = model_input.to(torch.float32).cuda()\n    predict = model(model_input)\n    total_predict += [predict.item()]\ntotal_predict = np.array(total_predict)*100\n\n## fix nan value\nfor nan_idx in np.argwhere(np.isnan(total_predict)):\n    \n    prev_idx = nan_idx-1\n    while np.isnan(total_predict[prev_idx]):\n        prev_idx = prev_idx - 1\n\n    next_idx = nan_idx+1\n    while np.isnan(total_predict[next_idx]):\n        next_idx = next_idx + 1\n    \n    total_predict[nan_idx] = (total_predict[prev_idx] +  total_predict[next_idx])/2\n\n## save result\nsave_result = pd.DataFrame({'time [sec]' : using_time, \n              'predict_result' : total_predict.tolist()})\nsave_result.to_csv(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'each_predict_result.csv'), index=False)\ntotal_predict\n</pre>  total_predict = [] using_time = [] for idx, time in enumerate(range(10,recovery_end-50,10)):     using_time += [time]     OD1_spec = get_OD(used_wl_data=cali_used_wl_data,                        used_wl_data2=cali_used_wl_data2,                        time=time,                       average_time=6)          OD2_spec = get_OD(used_wl_data=cali_used_wl_data,                        used_wl_data2=cali_used_wl_data2,                        time=time+10,                       average_time=6)     if idx == 0:         result_OD1_spec = OD1_spec.reshape(1,-1)         result_OD2_spec = OD2_spec.reshape(1,-1)     else:         result_OD1_spec = np.concatenate((result_OD1_spec, OD1_spec.reshape(1,-1)))         result_OD2_spec = np.concatenate((result_OD2_spec, OD2_spec.reshape(1,-1)))                   # # # normalize      # for i in range(40):     #     OD1_spec[i*20:i*20+20] = (OD1_spec[i*20:i*20+20] - OD1_spec[i*20:i*20+20].mean()) / (OD1_spec[i*20:i*20+20].max() - OD1_spec[i*20:i*20+20].min())          # for i in range(40):     #     OD2_spec[i*20:i*20+20] = (OD2_spec[i*20:i*20+20] - OD2_spec[i*20:i*20+20].mean()) / (OD2_spec[i*20:i*20+20].max() - OD2_spec[i*20:i*20+20].min())              # delta_OD = OD2_spec - OD1_spec     delta_OD = OD1_spec/OD2_spec          # # normalize      # for i in range(40):     #     delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].mean()) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min())          # normalize      for i in range(40):         delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].min() + 1e-9) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min() + 1e-9)     delta_OD = np.log(delta_OD)                         model_input = torch.tensor(delta_OD)     model_input = model_input.to(torch.float32).cuda()     predict = model(model_input)     total_predict += [predict.item()] total_predict = np.array(total_predict)*100  ## fix nan value for nan_idx in np.argwhere(np.isnan(total_predict)):          prev_idx = nan_idx-1     while np.isnan(total_predict[prev_idx]):         prev_idx = prev_idx - 1      next_idx = nan_idx+1     while np.isnan(total_predict[next_idx]):         next_idx = next_idx + 1          total_predict[nan_idx] = (total_predict[prev_idx] +  total_predict[next_idx])/2  ## save result save_result = pd.DataFrame({'time [sec]' : using_time,                'predict_result' : total_predict.tolist()}) save_result.to_csv(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'each_predict_result.csv'), index=False) total_predict Out[10]: <pre>array([ 24.53108281,  -8.57851058,   7.26404488,   8.4195748 ,\n        -6.64985254, -60.03581882,   9.10969973,   5.75581044,\n        -0.27230084,   4.85174358,   0.50482824, -24.06681925,\n        21.47475034,  11.96319908,   3.61428559,   5.33672124,\n         4.21024561,   9.13827866,   6.46792352,  20.77490538,\n         9.95377451,  -0.76072216,   6.27360791,  10.52872539,\n        14.38611001, -22.05187678, -21.01626396,   5.03113717,\n       -18.5854882 , -26.85339451,  11.70798093,   4.78378981,\n         6.07749969, -36.78811193,  10.64918935,  11.95651442,\n       -40.61893225, -42.46386886,   9.25014466, -38.65004182,\n        10.13460755,  -7.14831054,   9.293966  ,   6.18876666,\n       -20.03290057, -37.89363503,   4.59891856, -51.07743144,\n       -31.08494878,  -3.70794088,   3.58596593,  14.08306062,\n        -4.13230211,   6.50528073,  18.56849045,   8.13537985,\n         9.75320637,   3.90641391,   8.90877098, -43.59393716,\n        -0.81972033,  13.85016888])</pre> In\u00a0[13]: Copied! <pre>BF_total_predict, AF_total_predict = before_after_moving_average(data=total_predict, avg_points=3)\n\nplt.plot(using_time, AF_total_predict, \"b.-\")\nplt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"predict SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.title(f'$\\Delta$SO2 change with time')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_change_with_time.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> BF_total_predict, AF_total_predict = before_after_moving_average(data=total_predict, avg_points=3)  plt.plot(using_time, AF_total_predict, \"b.-\") plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"predict SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.title(f'$\\Delta$SO2 change with time') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_change_with_time.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[14]: Copied! <pre># shift baseline\nAF_total_predict = (AF_total_predict - AF_total_predict[:4].mean())\n\nplt.plot(using_time, AF_total_predict, \"b.-\")\nplt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"predict SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.title(f'$\\Delta$SO2 change with time')\nplt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_change_with_time_shift.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # shift baseline AF_total_predict = (AF_total_predict - AF_total_predict[:4].mean())  plt.plot(using_time, AF_total_predict, \"b.-\") plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"predict SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.title(f'$\\Delta$SO2 change with time') plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_change_with_time_shift.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[15]: Copied! <pre>total_predict = []\nusing_time = []\n# baseline_time = [i for i in range(0,50,10)]\n# HP_time = [i for i in range(70,230,10)]\n# recovery_time = [i for i in range(250,420,10)]\n# total_time = baseline_time + HP_time + recovery_time\nfor idx, time in enumerate(range(10,recovery_end-50,10)):\n    using_time += [time]\n    OD1_spec = get_OD(used_wl_data=cali_used_wl_data, \n                      used_wl_data2=cali_used_wl_data2, \n                      time=0, average_time=30)\n    \n    OD2_spec = get_OD(used_wl_data=cali_used_wl_data, \n                      used_wl_data2=cali_used_wl_data2, \n                      time=time, average_time=6)\n    if idx == 0:\n        result_OD1_spec = OD1_spec.reshape(1,-1)\n        result_OD2_spec = OD2_spec.reshape(1,-1)\n    else:\n        result_OD1_spec = np.concatenate((result_OD1_spec, OD1_spec.reshape(1,-1)))\n        result_OD2_spec = np.concatenate((result_OD2_spec, OD2_spec.reshape(1,-1)))\n        \n    \n    # # # normalize \n    # for i in range(40):\n    #     OD1_spec[i*20:i*20+20] = (OD1_spec[i*20:i*20+20] - OD1_spec[i*20:i*20+20].mean()) / (OD1_spec[i*20:i*20+20].max() - OD1_spec[i*20:i*20+20].min())\n    \n    # for i in range(40):\n    #     OD2_spec[i*20:i*20+20] = (OD2_spec[i*20:i*20+20] - OD2_spec[i*20:i*20+20].mean()) / (OD2_spec[i*20:i*20+20].max() - OD2_spec[i*20:i*20+20].min())\n    \n    # delta_OD = OD2_spec - OD1_spec\n \n    delta_OD = OD1_spec/OD2_spec\n    \n    # # normalize \n    # for i in range(40):\n    #     delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].mean()) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min())\n    \n    # normalize \n    for i in range(40):\n        delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].min() + 1e-9) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min() + 1e-9)\n    delta_OD = np.log(delta_OD)\n    \n    \n    model_input = torch.tensor(delta_OD)\n    model_input = model_input.to(torch.float32).cuda()\n    predict = model(model_input)\n    total_predict += [predict.item()]\ntotal_predict = np.array(total_predict)*100\n\n## fix nan value\nfor nan_idx in np.argwhere(np.isnan(total_predict)):\n    \n    prev_idx = nan_idx-1\n    while np.isnan(total_predict[prev_idx]):\n        prev_idx = prev_idx - 1\n\n    next_idx = nan_idx+1\n    while np.isnan(total_predict[next_idx]):\n        next_idx = next_idx + 1\n    \n    total_predict[nan_idx] = (total_predict[prev_idx] +  total_predict[next_idx])/2\n\n\nsave_result = pd.DataFrame({'time [sec]' : using_time, \n              'predict_result' : total_predict.tolist()})\n# save_result.to_csv(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cum_predict_result.csv'), index=False)\ntotal_predict\n</pre> total_predict = [] using_time = [] # baseline_time = [i for i in range(0,50,10)] # HP_time = [i for i in range(70,230,10)] # recovery_time = [i for i in range(250,420,10)] # total_time = baseline_time + HP_time + recovery_time for idx, time in enumerate(range(10,recovery_end-50,10)):     using_time += [time]     OD1_spec = get_OD(used_wl_data=cali_used_wl_data,                        used_wl_data2=cali_used_wl_data2,                        time=0, average_time=30)          OD2_spec = get_OD(used_wl_data=cali_used_wl_data,                        used_wl_data2=cali_used_wl_data2,                        time=time, average_time=6)     if idx == 0:         result_OD1_spec = OD1_spec.reshape(1,-1)         result_OD2_spec = OD2_spec.reshape(1,-1)     else:         result_OD1_spec = np.concatenate((result_OD1_spec, OD1_spec.reshape(1,-1)))         result_OD2_spec = np.concatenate((result_OD2_spec, OD2_spec.reshape(1,-1)))                   # # # normalize      # for i in range(40):     #     OD1_spec[i*20:i*20+20] = (OD1_spec[i*20:i*20+20] - OD1_spec[i*20:i*20+20].mean()) / (OD1_spec[i*20:i*20+20].max() - OD1_spec[i*20:i*20+20].min())          # for i in range(40):     #     OD2_spec[i*20:i*20+20] = (OD2_spec[i*20:i*20+20] - OD2_spec[i*20:i*20+20].mean()) / (OD2_spec[i*20:i*20+20].max() - OD2_spec[i*20:i*20+20].min())          # delta_OD = OD2_spec - OD1_spec       delta_OD = OD1_spec/OD2_spec          # # normalize      # for i in range(40):     #     delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].mean()) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min())          # normalize      for i in range(40):         delta_OD[i*20:i*20+20] = (delta_OD[i*20:i*20+20] - delta_OD[i*20:i*20+20].min() + 1e-9) / (delta_OD[i*20:i*20+20].max() - delta_OD[i*20:i*20+20].min() + 1e-9)     delta_OD = np.log(delta_OD)               model_input = torch.tensor(delta_OD)     model_input = model_input.to(torch.float32).cuda()     predict = model(model_input)     total_predict += [predict.item()] total_predict = np.array(total_predict)*100  ## fix nan value for nan_idx in np.argwhere(np.isnan(total_predict)):          prev_idx = nan_idx-1     while np.isnan(total_predict[prev_idx]):         prev_idx = prev_idx - 1      next_idx = nan_idx+1     while np.isnan(total_predict[next_idx]):         next_idx = next_idx + 1          total_predict[nan_idx] = (total_predict[prev_idx] +  total_predict[next_idx])/2   save_result = pd.DataFrame({'time [sec]' : using_time,                'predict_result' : total_predict.tolist()}) # save_result.to_csv(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'cum_predict_result.csv'), index=False) total_predict Out[15]: <pre>array([-26.77738667,  13.4188056 ,   7.83561319,  28.7181139 ,\n        20.85053176,  19.95195299, -59.68509316, -48.19259644,\n       -45.19850016, -48.3774662 , -45.26547194, -45.40652633,\n       -45.49898505, -50.33445954, -51.59769654, -37.98827529,\n        -7.38097429, -37.03697324, -14.41406608,  -2.10883841,\n         7.83847421,   8.87558609,   6.72673434,   4.12131697,\n        10.18086821,  11.13224626,  11.96017563,   3.68358344,\n         7.24709779,  -4.27937657, -37.90470958, -20.19309253,\n       -24.19120818,   6.32759631,   7.16629028,   5.13046831,\n         6.14533722,  21.36383504,   7.30806291,  18.87209862,\n         7.21781999,   3.56833339,   4.534702  ,   3.12797278,\n        11.13709658,   9.514229  ,  19.99760419,  32.07857609,\n         5.69496006, -24.02370572, -24.65475053,  -0.74314028,\n        13.58949393, -41.23163223,  -1.7052561 , -20.03953755,\n        30.07824421,  21.32330984,  31.71640038,  15.40864557,\n        16.7344138 ,  16.85555428])</pre> In\u00a0[16]: Copied! <pre>fitting_SO2 = np.load(os.path.join('pic', subject, f'{date}_invivo_result', exp, \"fitting\", \"SDS2\", \"fit_result_OPs\", \"chromophore\", f\"fitting_SO2.npy\"))\nfitting_SO2 = fitting_SO2[:62]*100\nBF_fitting_SO2, AF_fitting_SO2 = before_after_moving_average(data=fitting_SO2, avg_points=3)\n\nplt.plot(using_time, AF_fitting_SO2, \"b.-\")\nplt.plot(using_time, BF_fitting_SO2, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.title(f'Iterative curve fitting SO2(%)')\n# plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> fitting_SO2 = np.load(os.path.join('pic', subject, f'{date}_invivo_result', exp, \"fitting\", \"SDS2\", \"fit_result_OPs\", \"chromophore\", f\"fitting_SO2.npy\")) fitting_SO2 = fitting_SO2[:62]*100 BF_fitting_SO2, AF_fitting_SO2 = before_after_moving_average(data=fitting_SO2, avg_points=3)  plt.plot(using_time, AF_fitting_SO2, \"b.-\") plt.plot(using_time, BF_fitting_SO2, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.title(f'Iterative curve fitting SO2(%)') # plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[17]: Copied! <pre>change_fitting_SO2 = fitting_SO2 - fitting_SO2[:4].mean()\nBF_change_fitting_SO2, AF_change_fitting_SO2 = before_after_moving_average(data=change_fitting_SO2, avg_points=3)\n\nplt.plot(using_time, AF_change_fitting_SO2, \"b.-\")\nplt.plot(using_time, BF_change_fitting_SO2, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.title(f'Iterative curve fitting $\\Delta$SO2(%)')\n# plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> change_fitting_SO2 = fitting_SO2 - fitting_SO2[:4].mean() BF_change_fitting_SO2, AF_change_fitting_SO2 = before_after_moving_average(data=change_fitting_SO2, avg_points=3)  plt.plot(using_time, AF_change_fitting_SO2, \"b.-\") plt.plot(using_time, BF_change_fitting_SO2, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.title(f'Iterative curve fitting $\\Delta$SO2(%)') # plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative.png'), dpi=300, format='png', bbox_inches='tight') plt.show()  In\u00a0[18]: Copied! <pre>BF_total_predict, AF_total_predict = before_after_moving_average(data=total_predict, avg_points=3)\n\nplt.plot(using_time, AF_total_predict, \"b.-\")\nplt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"predict \\u0394SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.title(f'prediction model $\\Delta$SO2(%)')\n# plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> BF_total_predict, AF_total_predict = before_after_moving_average(data=total_predict, avg_points=3)  plt.plot(using_time, AF_total_predict, \"b.-\") plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"predict \\u0394SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.title(f'prediction model $\\Delta$SO2(%)') # plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[19]: Copied! <pre># shift baseline\n# AF_total_predict = (AF_total_predict - AF_total_predict[:4].mean())\n# AF_change_fitting_SO2 = (AF_change_fitting_SO2 - AF_change_fitting_SO2[:4].mean())\n\n# plt.plot(using_time, AF_change_fitting_SO2, \"r.-\", label='curve fitting')\nplt.plot(using_time, AF_total_predict*0.1, \"b.-\", label='prediction model')\n# plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"predict \\u0394SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\n# plt.title(f'$\\Delta$SO2')\nplt.ylim([-15,15])\n# plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative_shift.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # shift baseline # AF_total_predict = (AF_total_predict - AF_total_predict[:4].mean()) # AF_change_fitting_SO2 = (AF_change_fitting_SO2 - AF_change_fitting_SO2[:4].mean())  # plt.plot(using_time, AF_change_fitting_SO2, \"r.-\", label='curve fitting') plt.plot(using_time, AF_total_predict*0.1, \"b.-\", label='prediction model') # plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='valsalva_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"predict \\u0394SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) # plt.title(f'$\\Delta$SO2') plt.ylim([-15,15]) # plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', exp, 'predict_cumulative_shift.png'), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[31]: Copied! <pre>plt.plot(MBLL_SO2['time'], MBLL_SO2['delta_SO2_20_10'], 'r.--', alpha=0.9, label='MBLL 20mm-10mm')\n# plt.plot(MBLL_SO2['time'], MBLL_SO2['delta_SO2_20_7'], 'r.--', alpha=0.6, label='MBLL 20mm-7.5mm')\n# plt.plot(MBLL_SO2['time'], MBLL_SO2['delta_SO2_20_4'], 'r.--', alpha=0.3, label='MBLL 20mm-4.5mm')\nplt.plot(using_time, AF_total_predict*0.1, \"b.-\", label='ANN prediction model')\n# plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2)\nplt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end')\nplt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end')\nplt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end')\nplt.xlabel(\"time(sec)\")\nplt.ylabel(\"predict $\\Delta$SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.title(f'$\\Delta$SO2 (t1=0s~30s)')\n# plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', 'MBLL', 'predict_cumulative_shift_with_MBLL.png'), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.plot(MBLL_SO2['time'], MBLL_SO2['delta_SO2_20_10'], 'r.--', alpha=0.9, label='MBLL 20mm-10mm') # plt.plot(MBLL_SO2['time'], MBLL_SO2['delta_SO2_20_7'], 'r.--', alpha=0.6, label='MBLL 20mm-7.5mm') # plt.plot(MBLL_SO2['time'], MBLL_SO2['delta_SO2_20_4'], 'r.--', alpha=0.3, label='MBLL 20mm-4.5mm') plt.plot(using_time, AF_total_predict*0.1, \"b.-\", label='ANN prediction model') # plt.plot(using_time, BF_total_predict, \"b.-\", alpha=0.2) plt.axvline(x=baseline_end, linestyle='--', color='b', label='baseline_end') plt.axvline(x=exp_end, linestyle='--', color='r', label='hyperventilation_end') plt.axvline(x=using_time[-1], linestyle='--', color='g', label='recovery_end') plt.xlabel(\"time(sec)\") plt.ylabel(\"predict $\\Delta$SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.title(f'$\\Delta$SO2 (t1=0s~30s)') # plt.savefig(os.path.join('pic', subject, f'{date}_invivo_result', 'MBLL', 'predict_cumulative_shift_with_MBLL.png'), dpi=300, format='png', bbox_inches='tight') plt.show()"},{"location":"in_vivo_experiments/S2_predict_measured_data/#format-setting","title":"Format Setting\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/#get-processed-data","title":"Get processed data\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/#adjust-wavelength","title":"Adjust Wavelength\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/#mbll","title":"MBLL\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/#calibrate-short-long","title":"Calibrate short &amp; long\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/#plot-rmaxrmin","title":"Plot Rmax/Rmin\u00b6","text":""},{"location":"in_vivo_experiments/S2_predict_measured_data/#predict-in-vivo-data","title":"Predict in-vivo data\u00b6","text":""},{"location":"in_vivo_experiments/introduction/","title":"Analyze in vivo raw data","text":""},{"location":"in_vivo_experiments/introduction/#how-to-launch-the-experiments","title":"How to launch the experiments","text":"<ul> <li>Preparations:  </li> <li>You should have the raw data then save them in the directory /dataset/\\/SDS[1 or 2]/\\/ <li>You should have phantom_simulated in the directory /dataset/phantom_simulated </li> <li>keep your pre-trained surrogate model in the directory /surrogate_model/\\/ <li>keep your pre-trained prediction model in the directory /model_save/\\/\\/ <li>Make sure your OPs_used is synchronized as previous experiment (MCX simulation, training surrogate model, training prediction model)</li> <li> <p>Make sure your ANN_models.py is synchronized as previous experiment (training surrogate model, training prediction model) </p> </li> <li> <p>Screenshot of filepath should look like:  </p> </li> <li> <p>Launch: </p> <ul> <li>S1_preprocess_short.ipynb: preprocess raw data of short channel.</li> <li>S1_preprocess_long.ipynb: preprocess raw data of long channel.</li> <li>S2_predict_measured_data.ipynb: use prediction model take processed data as input, get predicted blood oxygen change of IJV.</li> </ul> </li>"},{"location":"in_vivo_experiments/introduction/#diagram-of-preprocessing-of-raw-data","title":"Diagram of preprocessing of raw data","text":""},{"location":"in_vivo_experiments/introduction/#how-to-process-raw-data","title":"How to process raw data","text":"<p>S1_preprocess_long.ipynb S1_preprocess_short.ipynb</p> <ul> <li>After finishing these two process, you would get 4 csv files. <ul> <li>Processed short in_vivo_results_exp.csv</li> <li>Processed long in_vivo_results_exp.csv </li> <li>Processed short calibration.csv : mapping to simulation intensity</li> <li>Processed long calibration.csv : mapping to simulation intensity</li> </ul> </li> </ul>"},{"location":"prediction_model/instructions/","title":"Instructions","text":"<ul> <li>To train prediction model, following the steps:<ul> <li>\\({\\rm\\color{blue}{S1\\_generate\\_spectrum.py}}\\)<ul> <li>Generate \\({\\rm\\mu_s}\\) spectra from the A, K fitted factor from literatures.</li> <li>Generate \\({\\rm\\mu_a}\\) spectra in the range of literatures.</li> </ul> </li> <li>\\({\\rm\\color{blue}{absoprtion\\_spectrum\\_by\\_substance/make\\_spectrum.ipynb}}\\)<ul> <li>Generate \\({\\rm\\mu_a}\\) spectra based on the chromophores such as water, blood, subcutaneous, collagen and melanin.</li> <li>copy the result files skin_mua_spectrum.csv, fat_mua_spectrum.csv, muscle_mua_spectrum and cca_mua_spectrum.csv to the directory prediction_model/OPs_used/mua_chromophore</li> </ul> </li> <li>\\({\\rm\\color{blue}{S2\\_generate\\_surrogate\\_result.py}}\\)</li> <li>\\({\\rm\\color{blue}{S3\\_generate\\_prediction\\_input.py}}\\)</li> <li>\\({\\rm\\color{blue}{S4\\_train\\_prediction\\_model.py}}\\)</li> </ul> </li> </ul>"},{"location":"prediction_model/plot_nested_k_fold_cross_validation/","title":"Plot Nested K-fold Cross Validation","text":"In\u00a0[28]: Copied! <pre>import numpy as np\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\n# plt.style.use('science')\nplt.style.use(\"seaborn-darkgrid\")\nplt.rcParams.update({'font.size': 12})\n</pre> import numpy as np import os import pandas as pd import matplotlib.pyplot as plt import matplotlib as mpl # Default settings mpl.rcParams.update(mpl.rcParamsDefault) # plt.style.use('science') plt.style.use(\"seaborn-darkgrid\") plt.rcParams.update({'font.size': 12}) <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_27368\\1340097018.py:9: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[8]: Copied! <pre>data = pd.read_csv(os.path.join('model_save', 'prediction_model_formula8', 'all_result.csv'))\n</pre> data = pd.read_csv(os.path.join('model_save', 'prediction_model_formula8', 'all_result.csv')) Out[8]: out_fold_idx in_fold_idx hp_idx neuron learning_rate batch_size test_loss 0 0 0 0 5 0.00010 512 0.000070 1 0 0 1 3 0.00010 512 0.000074 2 0 0 2 1 0.00010 512 0.000112 3 0 0 3 5 0.00005 512 0.000074 4 0 0 4 3 0.00005 512 0.000082 ... ... ... ... ... ... ... ... 675 4 4 23 1 0.00005 128 0.000087 676 4 4 24 5 0.00003 128 0.000060 677 4 4 25 3 0.00003 128 0.000072 678 4 4 26 1 0.00003 128 0.000112 679 4 4 26 1 0.00003 128 0.000051 <p>680 rows \u00d7 7 columns</p> In\u00a0[34]: Copied! <pre>plt.figure(figsize=(8,6))\nplt.plot(sorted(data['test_loss'].to_list())[:-3])\nplt.xlabel(\"hyperparameter set #\")\nplt.ylabel(\"loss\")\nplt.savefig(os.path.join(\"pic\", 'prediction_model_formula8', \"nested-k-fold-cross-validation.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.figure(figsize=(8,6)) plt.plot(sorted(data['test_loss'].to_list())[:-3]) plt.xlabel(\"hyperparameter set #\") plt.ylabel(\"loss\") plt.savefig(os.path.join(\"pic\", 'prediction_model_formula8', \"nested-k-fold-cross-validation.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[32]: Copied! <pre>hyper_set = pd.read_csv(os.path.join('model_save', 'prediction_model_formula8', 'hyper_set.csv'))\nhyper_set\n</pre> hyper_set = pd.read_csv(os.path.join('model_save', 'prediction_model_formula8', 'hyper_set.csv')) hyper_set Out[32]: hp_idx batch_size learning_rate neuron 0 0 512 0.00010 5 1 1 512 0.00010 3 2 2 512 0.00010 1 3 3 512 0.00005 5 4 4 512 0.00005 3 5 5 512 0.00005 1 6 6 512 0.00003 5 7 7 512 0.00003 3 8 8 512 0.00003 1 9 9 256 0.00010 5 10 10 256 0.00010 3 11 11 256 0.00010 1 12 12 256 0.00005 5 13 13 256 0.00005 3 14 14 256 0.00005 1 15 15 256 0.00003 5 16 16 256 0.00003 3 17 17 256 0.00003 1 18 18 128 0.00010 5 19 19 128 0.00010 3 20 20 128 0.00010 1 21 21 128 0.00005 5 22 22 128 0.00005 3 23 23 128 0.00005 1 24 24 128 0.00003 5 25 25 128 0.00003 3 26 26 128 0.00003 1"},{"location":"prediction_model/plot_prediction_model_structure/","title":"Plot Prediciton model Structure","text":"In\u00a0[1]: Copied! <pre>import torch\nimport numpy as np\nfrom ANN_models import PredictionModel, PredictionModel2, PredictionModel3, PredictionModel4, PredictionModel_single_SDS, PredictionModel5\nimport random\nimport os\n</pre> import torch import numpy as np from ANN_models import PredictionModel, PredictionModel2, PredictionModel3, PredictionModel4, PredictionModel_single_SDS, PredictionModel5 import random import os In\u00a0[2]: Copied! <pre>model = PredictionModel5(neuronsize=1).cuda()\n# model = PredictionModel3().cuda()\n# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Model visualization\ninput_names = ['spectrum']\noutput_names = ['\\u0394SO2']\ntensor_input = np.array([random.random() for i in range(800)])\ntensor_input = torch.tensor(tensor_input)\ntensor_input = tensor_input.to(torch.float32).to(device)\ntorch.onnx.export(model, tensor_input, os.path.join('pic', 'prediction_model5_neuronsize_1.onnx'), input_names=input_names, output_names=output_names)\n</pre> model = PredictionModel5(neuronsize=1).cuda() # model = PredictionModel3().cuda() # Get cpu or gpu device for training. device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Model visualization input_names = ['spectrum'] output_names = ['\\u0394SO2'] tensor_input = np.array([random.random() for i in range(800)]) tensor_input = torch.tensor(tensor_input) tensor_input = tensor_input.to(torch.float32).to(device) torch.onnx.export(model, tensor_input, os.path.join('pic', 'prediction_model5_neuronsize_1.onnx'), input_names=input_names, output_names=output_names) <pre>============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n</pre>"},{"location":"prediction_model/plot_result/","title":"Plot Result","text":"In\u00a0[50]: Copied! <pre>import matplotlib.pyplot as plt\nfrom ANN_models import PredictionModel, PredictionModel2, PredictionModel3, PredictionModel4, PredictionModel_single_SDS, PredictionModel5\nimport os \nimport json\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nfrom tqdm import tqdm\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\nplt.rcParams.update({'font.size': 22})\n</pre> import matplotlib.pyplot as plt from ANN_models import PredictionModel, PredictionModel2, PredictionModel3, PredictionModel4, PredictionModel_single_SDS, PredictionModel5 import os  import json import torch import numpy as np import pandas as pd import matplotlib as mpl from tqdm import tqdm # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") plt.rcParams.update({'font.size': 22}) <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_7920\\2141461968.py:12: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[51]: Copied! <pre>used_SO2 = 50\nresult_folder = f\"prediction_model_formula24_SO2_{used_SO2}\"\nsubject = \"ctchen\"\n</pre> used_SO2 = 50 result_folder = f\"prediction_model_formula24_SO2_{used_SO2}\" subject = \"ctchen\"  In\u00a0[52]: Copied! <pre>os.makedirs(os.path.join(\"pic\", result_folder, subject), exist_ok=True)\nwith open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:\n    SO2 = json.load(f)\n    test_SO2 = SO2['test_SO2']\nwith open(os.path.join(\"model_save\", result_folder, subject, 'trlog.json'), 'r') as f:\n    config = json.load(f)\ntest_loader = torch.load(os.path.join(\"model_save\", result_folder, subject, 'test_loader.pth'))\nmodel = PredictionModel5(neuronsize=5).cuda()\nmodel.load_state_dict(torch.load(config['best_model']))\nmodel.eval()\n</pre> os.makedirs(os.path.join(\"pic\", result_folder, subject), exist_ok=True) with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:     SO2 = json.load(f)     test_SO2 = SO2['test_SO2'] with open(os.path.join(\"model_save\", result_folder, subject, 'trlog.json'), 'r') as f:     config = json.load(f) test_loader = torch.load(os.path.join(\"model_save\", result_folder, subject, 'test_loader.pth')) model = PredictionModel5(neuronsize=5).cuda() model.load_state_dict(torch.load(config['best_model'])) model.eval()   Out[52]: <pre>PredictionModel5(\n  (net): Sequential(\n    (0): Linear(in_features=800, out_features=640, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=640, out_features=320, bias=True)\n    (3): ReLU()\n    (4): Linear(in_features=320, out_features=160, bias=True)\n    (5): ReLU()\n    (6): Linear(in_features=160, out_features=80, bias=True)\n    (7): ReLU()\n    (8): Linear(in_features=80, out_features=40, bias=True)\n    (9): ReLU()\n    (10): Linear(in_features=40, out_features=1, bias=True)\n  )\n)</pre> In\u00a0[53]: Copied! <pre># def cal_R_square(y_true, y_pred):\n#     y_bar = np.mean(y_true)\n#     numerator = np.sum(np.square(y_true-y_pred))\n#     denominator = np.sum(np.square(y_true-y_bar))\n#     R_square = 1 - numerator/denominator\n    \n#     return R_square\n# df = {'predic' : [], 'true' : [] , 'error' : [], 'abs_error' : []}\n# for i in range(40):\n#     df[f'data_value_{i}'] = []\n    \n# for batch_idx, (data,target, _, _, _) in tqdm(enumerate(test_loader)):\n#     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n#     output = model(data)\n#     output = output.detach().cpu().numpy()\n#     target = target.detach().cpu().numpy()\n#     for idx in range(output.shape[0]):\n#         df['predic'].append(output[idx][0]*100)\n#         df['true'].append(target[idx][0]*100)\n#         df['error'].append(100*(output[idx][0] - target[idx][0]))\n#         df['abs_error'].append(np.abs(100*(output[idx][0] - target[idx][0])))\n        \n#     for row_idx, one_row in enumerate(data):\n#         for idx in range(one_row.shape[0]):\n#             df[f'data_value_{idx}'] += [one_row[idx].item()]\n\n# df = pd.DataFrame(df)\n# df = df.sort_values('abs_error', ascending=False)\n# df.to_csv(os.path.join(\"pic\", result_folder, subject, \"RMSE.csv\"), index=False)\n</pre> # def cal_R_square(y_true, y_pred): #     y_bar = np.mean(y_true) #     numerator = np.sum(np.square(y_true-y_pred)) #     denominator = np.sum(np.square(y_true-y_bar)) #     R_square = 1 - numerator/denominator      #     return R_square # df = {'predic' : [], 'true' : [] , 'error' : [], 'abs_error' : []} # for i in range(40): #     df[f'data_value_{i}'] = []      # for batch_idx, (data,target, _, _, _) in tqdm(enumerate(test_loader)): #     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda() #     output = model(data) #     output = output.detach().cpu().numpy() #     target = target.detach().cpu().numpy() #     for idx in range(output.shape[0]): #         df['predic'].append(output[idx][0]*100) #         df['true'].append(target[idx][0]*100) #         df['error'].append(100*(output[idx][0] - target[idx][0])) #         df['abs_error'].append(np.abs(100*(output[idx][0] - target[idx][0])))          #     for row_idx, one_row in enumerate(data): #         for idx in range(one_row.shape[0]): #             df[f'data_value_{idx}'] += [one_row[idx].item()]  # df = pd.DataFrame(df) # df = df.sort_values('abs_error', ascending=False) # df.to_csv(os.path.join(\"pic\", result_folder, subject, \"RMSE.csv\"), index=False) In\u00a0[54]: Copied! <pre>plt.figure(figsize=(12,8))\nfor batch_idx, (data,target, _,_,_) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    output = output.detach().cpu().numpy()\n    target = target.detach().cpu().numpy()\n    if batch_idx == 0:\n        error = 100*(output - target)\n        RMSE = 100*(output - target)\n    else:\n        error = np.concatenate((error, 100*(output - target)))\n        RMSE = np.concatenate((RMSE, 100*(output - target)))\n    if batch_idx == 0:\n        plt.plot(target*100,output*100, 'r.', markersize=5, label= 'predict')\n        plt.plot(target*100,target*100,'b', label = 'optimal')\n    else:\n        plt.plot(target*100,output*100, 'r.', markersize=5)\n        plt.plot(target*100,target*100,'b')\n\nRMSE = np.sqrt(np.mean(np.square(RMSE)))\nmean = np.mean(np.abs(error))\nstd = np.std(np.abs(error))\nmax_error = np.max(np.abs(error))\nplt.title(f\"based on SO2={used_SO2}% \\nmean error:{mean:.2f}% std:{std:.2f}% \\nmax error:{max_error:.2f}% RMSE:{RMSE:.2f}%\")\nplt.xlabel(\"truth $\\u0394$SO2(%)\")\nplt.ylabel(\"predict $\\u0394$SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", result_folder, subject, \"all_metrics.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.figure(figsize=(12,8)) for batch_idx, (data,target, _,_,_) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     output = output.detach().cpu().numpy()     target = target.detach().cpu().numpy()     if batch_idx == 0:         error = 100*(output - target)         RMSE = 100*(output - target)     else:         error = np.concatenate((error, 100*(output - target)))         RMSE = np.concatenate((RMSE, 100*(output - target)))     if batch_idx == 0:         plt.plot(target*100,output*100, 'r.', markersize=5, label= 'predict')         plt.plot(target*100,target*100,'b', label = 'optimal')     else:         plt.plot(target*100,output*100, 'r.', markersize=5)         plt.plot(target*100,target*100,'b')  RMSE = np.sqrt(np.mean(np.square(RMSE))) mean = np.mean(np.abs(error)) std = np.std(np.abs(error)) max_error = np.max(np.abs(error)) plt.title(f\"based on SO2={used_SO2}% \\nmean error:{mean:.2f}% std:{std:.2f}% \\nmax error:{max_error:.2f}% RMSE:{RMSE:.2f}%\") plt.xlabel(\"truth $\\u0394$SO2(%)\") plt.ylabel(\"predict $\\u0394$SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", result_folder, subject, \"all_metrics.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[55]: Copied! <pre>def cal_R_square(y_true, y_pred):\n    y_bar = np.mean(y_true)\n    numerator = np.sum(np.square(y_true-y_pred))\n    denominator = np.sum(np.square(y_true-y_bar))\n    R_square = 1 - numerator/denominator\n    \n    return R_square\n</pre> def cal_R_square(y_true, y_pred):     y_bar = np.mean(y_true)     numerator = np.sum(np.square(y_true-y_pred))     denominator = np.sum(np.square(y_true-y_bar))     R_square = 1 - numerator/denominator          return R_square In\u00a0[56]: Copied! <pre>plt.rcParams.update({'font.size': 16})\nplt.figure(figsize=(12,8))\nfor batch_idx, (data,target, _,_,_) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    output = output.detach().cpu().numpy()\n    target = target.detach().cpu().numpy()\n    if batch_idx == 0:\n        error = 100*(output - target)\n        accumulate_RMSE = 100*(output - target)\n        accumulate_output = output\n        accumulate_target = target\n    else:\n        error = np.concatenate((error, 100*(output - target)))\n        accumulate_RMSE = np.concatenate((accumulate_RMSE, 100*(output - target)))\n        accumulate_output = np.concatenate((accumulate_output, output))\n        accumulate_target = np.concatenate((accumulate_target, target))\n    if batch_idx == 0:\n        plt.plot(target*100,output*100, 'r.', markersize=10, label= 'predict')\n        plt.plot(target*100,target*100,'b', label = 'optimal')\n    else:\n        plt.plot(target*100,output*100, 'r.', markersize=10)\n        plt.plot(target*100,target*100,'b')\n\nRMSE = np.sqrt(np.mean(np.square(accumulate_RMSE)))\nR_square = cal_R_square(y_true=accumulate_target, y_pred=accumulate_output)\nplt.title(f\"based on SO2={used_SO2}% \\n RMSE:{RMSE:.2f}% $R^{2}$:{R_square:.3f}\")\nplt.xlabel(\"truth $\\u0394$SO2(%)\")\nplt.ylabel(\"predict $\\u0394$SO2(%)\")\nplt.legend()\nplt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE.png\"), dpi=300, format='png', bbox_inches='tight')\n# plt.close()\nplt.show()\n</pre> plt.rcParams.update({'font.size': 16}) plt.figure(figsize=(12,8)) for batch_idx, (data,target, _,_,_) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     output = output.detach().cpu().numpy()     target = target.detach().cpu().numpy()     if batch_idx == 0:         error = 100*(output - target)         accumulate_RMSE = 100*(output - target)         accumulate_output = output         accumulate_target = target     else:         error = np.concatenate((error, 100*(output - target)))         accumulate_RMSE = np.concatenate((accumulate_RMSE, 100*(output - target)))         accumulate_output = np.concatenate((accumulate_output, output))         accumulate_target = np.concatenate((accumulate_target, target))     if batch_idx == 0:         plt.plot(target*100,output*100, 'r.', markersize=10, label= 'predict')         plt.plot(target*100,target*100,'b', label = 'optimal')     else:         plt.plot(target*100,output*100, 'r.', markersize=10)         plt.plot(target*100,target*100,'b')  RMSE = np.sqrt(np.mean(np.square(accumulate_RMSE))) R_square = cal_R_square(y_true=accumulate_target, y_pred=accumulate_output) plt.title(f\"based on SO2={used_SO2}% \\n RMSE:{RMSE:.2f}% $R^{2}$:{R_square:.3f}\") plt.xlabel(\"truth $\\u0394$SO2(%)\") plt.ylabel(\"predict $\\u0394$SO2(%)\") plt.legend() plt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE.png\"), dpi=300, format='png', bbox_inches='tight') # plt.close() plt.show() In\u00a0[57]: Copied! <pre>table = {}\nfor using_SO2 in test_SO2:\n    table[using_SO2] = []\nfor i in range(accumulate_target.shape[0]):\n    key = np.around(accumulate_target[i][0] + used_SO2*0.01,2)\n    table[key] += [accumulate_output[i][0]-accumulate_target[i][0]]\n\naccumulate_std1 = []\naccumulate_std2 = []\naccumulate_x = []\nfor key in table.keys():\n    data = np.array(table[key])\n    std1 = max(data)\n    std2 = min(data)\n    # std1 = np.std(data[np.where(data&gt;=0)])\n    # std2 = np.std(data[np.where(data&lt;=0)])\n    if np.isnan(std1):\n        std1 = 0\n    if np.isnan(std2):\n        std2 = 0\n    ijv_change = np.around(key - used_SO2*0.01, 2)\n    accumulate_std1.append(std1)\n    accumulate_std2.append(std2)\n    accumulate_x.append(ijv_change)\naccumulate_std1 = np.array(accumulate_std1)*100\naccumulate_std2 = np.array(accumulate_std2)*100\naccumulate_x = np.array(accumulate_x)*100\nplt.figure(figsize=(12,8))\nplt.fill_between(accumulate_x, accumulate_x+accumulate_std1, accumulate_x+accumulate_std2, color='b', alpha=0.3, edgecolor=None, label='predict')\nplt.plot(accumulate_x,accumulate_x, 'b', label='optimal')\nplt.title(f\"based on SO2={used_SO2}% \\n RMSE:{RMSE:.2f}% $R^{2}$:{R_square:.3f}\")\nplt.xlabel(\"truth $\\u0394$SO2(%)\")\nplt.ylabel(\"predict $\\u0394$SO2(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE_fill_plot.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> table = {} for using_SO2 in test_SO2:     table[using_SO2] = [] for i in range(accumulate_target.shape[0]):     key = np.around(accumulate_target[i][0] + used_SO2*0.01,2)     table[key] += [accumulate_output[i][0]-accumulate_target[i][0]]  accumulate_std1 = [] accumulate_std2 = [] accumulate_x = [] for key in table.keys():     data = np.array(table[key])     std1 = max(data)     std2 = min(data)     # std1 = np.std(data[np.where(data&gt;=0)])     # std2 = np.std(data[np.where(data&lt;=0)])     if np.isnan(std1):         std1 = 0     if np.isnan(std2):         std2 = 0     ijv_change = np.around(key - used_SO2*0.01, 2)     accumulate_std1.append(std1)     accumulate_std2.append(std2)     accumulate_x.append(ijv_change) accumulate_std1 = np.array(accumulate_std1)*100 accumulate_std2 = np.array(accumulate_std2)*100 accumulate_x = np.array(accumulate_x)*100 plt.figure(figsize=(12,8)) plt.fill_between(accumulate_x, accumulate_x+accumulate_std1, accumulate_x+accumulate_std2, color='b', alpha=0.3, edgecolor=None, label='predict') plt.plot(accumulate_x,accumulate_x, 'b', label='optimal') plt.title(f\"based on SO2={used_SO2}% \\n RMSE:{RMSE:.2f}% $R^{2}$:{R_square:.3f}\") plt.xlabel(\"truth $\\u0394$SO2(%)\") plt.ylabel(\"predict $\\u0394$SO2(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE_fill_plot.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[58]: Copied! <pre>accumulate_x = []\nfor key in table.keys():\n    data = np.array(table[key])\n    RMSE = np.sqrt(np.mean(np.square(data*100)))\n    accumulate_x.append(RMSE)\nplt.figure(figsize=(12,8))\nplt.plot((np.array(test_SO2)-0.7)*100, accumulate_x)\nplt.title(f\"based on SO2={used_SO2}% \\n RMSE of each $\\u0394$SO2(%)\")\nplt.xlabel(\"truth $\\u0394$SO2(%)\")\nplt.ylabel(\"RMSE (%)\")\nplt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE_each_plot.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> accumulate_x = [] for key in table.keys():     data = np.array(table[key])     RMSE = np.sqrt(np.mean(np.square(data*100)))     accumulate_x.append(RMSE) plt.figure(figsize=(12,8)) plt.plot((np.array(test_SO2)-0.7)*100, accumulate_x) plt.title(f\"based on SO2={used_SO2}% \\n RMSE of each $\\u0394$SO2(%)\") plt.xlabel(\"truth $\\u0394$SO2(%)\") plt.ylabel(\"RMSE (%)\") plt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE_each_plot.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[59]: Copied! <pre>mean = np.mean(error)\nstd = np.std(error)\nplt.figure(figsize=(12,8))\nn,bin, pack = plt.hist(error, bins=100)\nplt.vlines([mean+2*std, mean-2*std], 0, max(n), 'r', label='$\\mu$$\\pm$2*$\\sigma$')\nplt.text(mean+2*std, max(n), f'{mean+2*std:.2f}%')\nplt.text(mean-2*std, max(n), f'{mean-2*std:.2f}%')\nplt.xlabel('error(prediction-true)(%)')\nplt.ylabel('count')\nplt.title('error histogram')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", result_folder, subject, \"hist.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> mean = np.mean(error) std = np.std(error) plt.figure(figsize=(12,8)) n,bin, pack = plt.hist(error, bins=100) plt.vlines([mean+2*std, mean-2*std], 0, max(n), 'r', label='$\\mu$$\\pm$2*$\\sigma$') plt.text(mean+2*std, max(n), f'{mean+2*std:.2f}%') plt.text(mean-2*std, max(n), f'{mean-2*std:.2f}%') plt.xlabel('error(prediction-true)(%)') plt.ylabel('count') plt.title('error histogram') plt.legend() plt.savefig(os.path.join(\"pic\", result_folder, subject, \"hist.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()  In\u00a0[60]: Copied! <pre>plt.rcParams.update({'font.size': 16})\nfig = plt.figure(figsize=(18,12))\nfig.suptitle(f\"based on SO2={used_SO2}% \", fontsize=22)\n\ncount = 1\nfor key in table.keys():\n    data = np.array(table[key])\n    if key*100==used_SO2:\n        specific_error = data*100   \n        specific_error = np.sort(specific_error)[1:-1] \n    else:\n        specific_error = data*100    \n    mean = np.mean(specific_error)\n    std = np.std(specific_error)\n    \n    abs_mean = np.mean(np.abs(specific_error))\n    abs_std = np.std(np.abs(specific_error))\n    \n    if key*100%10 == 0:\n        ## plot result\n        ax = plt.subplot(2,3, count)\n        ax.set_title(f'$\\u0394$SO2={int(key*100)-used_SO2}% error histogram \\n absolute error mean={abs_mean:.2f}%, std={abs_std:.2f}%')\n        n,bin, pack = ax.hist(specific_error, bins=100)\n        ax.set_xlabel('error(prediction-true)(%)')\n        ax.set_ylabel('count')\n        ax.vlines([mean+2*std, mean-2*std], 0, max(n), 'r', label='$\\mu$$\\pm$2*$\\sigma$')\n        ax.text(mean+2*std, max(n), f'{mean+2*std:.2f}%')\n        ax.text(mean-2*std, max(n), f'{mean-2*std:.2f}%')\n        count += 1\n    \nplt.tight_layout()\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                    fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", result_folder, subject, f\"each_SO2_hist.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.rcParams.update({'font.size': 16}) fig = plt.figure(figsize=(18,12)) fig.suptitle(f\"based on SO2={used_SO2}% \", fontsize=22)  count = 1 for key in table.keys():     data = np.array(table[key])     if key*100==used_SO2:         specific_error = data*100            specific_error = np.sort(specific_error)[1:-1]      else:         specific_error = data*100         mean = np.mean(specific_error)     std = np.std(specific_error)          abs_mean = np.mean(np.abs(specific_error))     abs_std = np.std(np.abs(specific_error))          if key*100%10 == 0:         ## plot result         ax = plt.subplot(2,3, count)         ax.set_title(f'$\\u0394$SO2={int(key*100)-used_SO2}% error histogram \\n absolute error mean={abs_mean:.2f}%, std={abs_std:.2f}%')         n,bin, pack = ax.hist(specific_error, bins=100)         ax.set_xlabel('error(prediction-true)(%)')         ax.set_ylabel('count')         ax.vlines([mean+2*std, mean-2*std], 0, max(n), 'r', label='$\\mu$$\\pm$2*$\\sigma$')         ax.text(mean+2*std, max(n), f'{mean+2*std:.2f}%')         ax.text(mean-2*std, max(n), f'{mean-2*std:.2f}%')         count += 1      plt.tight_layout() plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                     fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", result_folder, subject, f\"each_SO2_hist.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[61]: Copied! <pre># plt.rcParams.update({'font.size': 18})\n# used_SO2_set = [50, 60, 70]\n# fig = plt.figure(figsize=(16,9))\n# # fig.suptitle(f\"based on SO2={used_SO2}% \", fontsize=16)\n# count = 1\n# for used_SO2 in used_SO2_set:\n#     ax = plt.subplot(1,3, count)\n#     count += 1\n#     result_folder = f\"prediction_model_formula24_SO2_{used_SO2}\"\n#     subject = \"ctchen\"\n#     os.makedirs(os.path.join(\"pic\", result_folder, subject), exist_ok=True)\n#     with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:\n#         SO2 = json.load(f)\n#         test_SO2 = SO2['test_SO2']\n#     with open(os.path.join(\"model_save\", result_folder, subject, 'trlog.json'), 'r') as f:\n#         config = json.load(f)\n#     test_loader = torch.load(os.path.join(\"model_save\", result_folder, subject, 'test_loader.pth'))\n#     model = PredictionModel5(neuronsize=5).cuda()\n#     model.load_state_dict(torch.load(config['best_model']))\n#     model.eval()\n\n \n#     for batch_idx, (data,target, _,_,_) in enumerate(test_loader):\n#         data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n#         output = model(data)\n#         output = output.detach().cpu().numpy()\n#         target = target.detach().cpu().numpy()\n#         if batch_idx == 0:\n#             error = 100*(output - target)\n#             accumulate_RMSE = 100*(output - target)\n#             accumulate_output = output\n#             accumulate_target = target\n#         else:\n#             error = np.concatenate((error, 100*(output - target)))\n#             accumulate_RMSE = np.concatenate((accumulate_RMSE, 100*(output - target)))\n#             accumulate_output = np.concatenate((accumulate_output, output))\n#             accumulate_target = np.concatenate((accumulate_target, target))\n#         if batch_idx == 0:\n#             ax.plot(target*100,output*100, 'r.', markersize=5, label= 'predict')\n#             ax.plot(target*100,target*100,'b', label = 'optimal')\n#         else:\n#             ax.plot(target*100,output*100, 'r.', markersize=5)\n#             ax.plot(target*100,target*100,'b')\n\n#     RMSE = np.sqrt(np.mean(np.square(accumulate_RMSE)))\n#     R_square = cal_R_square(y_true=accumulate_target, y_pred=accumulate_output)\n#     ax.set_title(f\"based on SO2={used_SO2}% \\n RMSE:{RMSE:.2f}% $R^{2}$:{R_square:.3f}\")\n#     ax.set_xlabel(\"truth $\\u0394$SO2(%)\")\n#     ax.set_ylabel(\"predict $\\u0394$SO2(%)\")\n\n# plt.tight_layout()\n# plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n#                     fancybox=True, shadow=True)\n# plt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE_set.png\"), dpi=300, format='png', bbox_inches='tight')\n# plt.show()\n</pre> # plt.rcParams.update({'font.size': 18}) # used_SO2_set = [50, 60, 70] # fig = plt.figure(figsize=(16,9)) # # fig.suptitle(f\"based on SO2={used_SO2}% \", fontsize=16) # count = 1 # for used_SO2 in used_SO2_set: #     ax = plt.subplot(1,3, count) #     count += 1 #     result_folder = f\"prediction_model_formula24_SO2_{used_SO2}\" #     subject = \"ctchen\" #     os.makedirs(os.path.join(\"pic\", result_folder, subject), exist_ok=True) #     with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f: #         SO2 = json.load(f) #         test_SO2 = SO2['test_SO2'] #     with open(os.path.join(\"model_save\", result_folder, subject, 'trlog.json'), 'r') as f: #         config = json.load(f) #     test_loader = torch.load(os.path.join(\"model_save\", result_folder, subject, 'test_loader.pth')) #     model = PredictionModel5(neuronsize=5).cuda() #     model.load_state_dict(torch.load(config['best_model'])) #     model.eval()    #     for batch_idx, (data,target, _,_,_) in enumerate(test_loader): #         data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda() #         output = model(data) #         output = output.detach().cpu().numpy() #         target = target.detach().cpu().numpy() #         if batch_idx == 0: #             error = 100*(output - target) #             accumulate_RMSE = 100*(output - target) #             accumulate_output = output #             accumulate_target = target #         else: #             error = np.concatenate((error, 100*(output - target))) #             accumulate_RMSE = np.concatenate((accumulate_RMSE, 100*(output - target))) #             accumulate_output = np.concatenate((accumulate_output, output)) #             accumulate_target = np.concatenate((accumulate_target, target)) #         if batch_idx == 0: #             ax.plot(target*100,output*100, 'r.', markersize=5, label= 'predict') #             ax.plot(target*100,target*100,'b', label = 'optimal') #         else: #             ax.plot(target*100,output*100, 'r.', markersize=5) #             ax.plot(target*100,target*100,'b')  #     RMSE = np.sqrt(np.mean(np.square(accumulate_RMSE))) #     R_square = cal_R_square(y_true=accumulate_target, y_pred=accumulate_output) #     ax.set_title(f\"based on SO2={used_SO2}% \\n RMSE:{RMSE:.2f}% $R^{2}$:{R_square:.3f}\") #     ax.set_xlabel(\"truth $\\u0394$SO2(%)\") #     ax.set_ylabel(\"predict $\\u0394$SO2(%)\")  # plt.tight_layout() # plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), #                     fancybox=True, shadow=True) # plt.savefig(os.path.join(\"pic\", result_folder, subject, \"RMSE_set.png\"), dpi=300, format='png', bbox_inches='tight') # plt.show()"},{"location":"prediction_model/plot_simulated_Rmax_Rmin/","title":"Plot Simulated Rmax Rmin","text":"In\u00a0[3]: Copied! <pre>import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\nos.makedirs(os.path.join('pic', 'test_Rmax_Rmin'), exist_ok=True)\n</pre> import os import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib as mpl # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") os.makedirs(os.path.join('pic', 'test_Rmax_Rmin'), exist_ok=True) <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_28000\\478471028.py:8: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[30]: Copied! <pre>R_SDS1_set = []\nR_SDS2_set = []\ncount = 0\nfor blc in [138, 151, 174]:\n    for i in range(10000):\n        surrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', f'bloodConc_{blc}', 'SO2_0.7', f'{i}_train.csv'))\n        IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy()\n        IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy()\n        IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy()\n        IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()\n        R_SDS1 = (IJV_small_SDS1/IJV_large_SDS1).mean()\n        R_SDS2 = (IJV_small_SDS2/IJV_large_SDS2).mean()\n        if (R_SDS2 &gt; R_SDS1) &amp; (R_SDS1&gt;1) &amp; (R_SDS2&gt;1):\n            R_SDS1_set.append((IJV_small_SDS1/IJV_large_SDS1).mean())\n            R_SDS2_set.append((IJV_small_SDS2/IJV_large_SDS2).mean())\n        else:\n            count += 1\n# surrogate_result\n</pre> R_SDS1_set = [] R_SDS2_set = [] count = 0 for blc in [138, 151, 174]:     for i in range(10000):         surrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', f'bloodConc_{blc}', 'SO2_0.7', f'{i}_train.csv'))         IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy()         IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy()         IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy()         IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()         R_SDS1 = (IJV_small_SDS1/IJV_large_SDS1).mean()         R_SDS2 = (IJV_small_SDS2/IJV_large_SDS2).mean()         if (R_SDS2 &gt; R_SDS1) &amp; (R_SDS1&gt;1) &amp; (R_SDS2&gt;1):             R_SDS1_set.append((IJV_small_SDS1/IJV_large_SDS1).mean())             R_SDS2_set.append((IJV_small_SDS2/IJV_large_SDS2).mean())         else:             count += 1 # surrogate_result In\u00a0[93]: Copied! <pre># R_set = [R_SDS1_set] + [R_SDS2_set]\n# R_set = np.array(R_set)\n# idx1 = np.argsort(R_set)[0][::-1]\n# idx1\n</pre> # R_set = [R_SDS1_set] + [R_SDS2_set] # R_set = np.array(R_set) # idx1 = np.argsort(R_set)[0][::-1] # idx1 Out[93]: <pre>array([ 873, 9084, 7960, ...,  292, 2847, 2861], dtype=int64)</pre> In\u00a0[100]: Copied! <pre>plt.plot(R_SDS1_set[5000:5200], label='SDS10mm Rmax/Rmin')\nplt.plot(R_SDS2_set[5000:5200], label='SDS20mm Rmax/Rmin')\nplt.xlabel('simulation set #')\nplt.ylabel('Rmax/Rmin')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.plot(R_SDS1_set[5000:5200], label='SDS10mm Rmax/Rmin') plt.plot(R_SDS2_set[5000:5200], label='SDS20mm Rmax/Rmin') plt.xlabel('simulation set #') plt.ylabel('Rmax/Rmin') plt.legend() plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[102]: Copied! <pre>R = (IJV_small_SDS1/IJV_large_SDS1).mean()\nplt.title(f'R_max/R_min= {R:.2f}')\nplt.plot(IJV_large_SDS1, label='IJV_large')\nplt.plot(IJV_small_SDS1, label='IJV_small')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS1_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> R = (IJV_small_SDS1/IJV_large_SDS1).mean() plt.title(f'R_max/R_min= {R:.2f}') plt.plot(IJV_large_SDS1, label='IJV_large') plt.plot(IJV_small_SDS1, label='IJV_small') plt.legend() plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS1_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[101]: Copied! <pre>R = (IJV_small_SDS2/IJV_large_SDS2).mean()\nplt.title(f'R_max/R_min= {R:.2f}')\nplt.plot(IJV_large_SDS2, label='IJV_large')\nplt.plot(IJV_small_SDS2, label='IJV_small')\nplt.legend()\nplt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS2_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> R = (IJV_small_SDS2/IJV_large_SDS2).mean() plt.title(f'R_max/R_min= {R:.2f}') plt.plot(IJV_large_SDS2, label='IJV_large') plt.plot(IJV_small_SDS2, label='IJV_small') plt.legend() plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS2_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[113]: Copied! <pre>surrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', 'bloodConc_138', 'SO2_0.7', f'5000_train.csv'))\nSO2_70_IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy()\nSO2_70_IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy()\nSO2_70_IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy()\nSO2_70_IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()\n\nsurrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', 'bloodConc_138', 'SO2_0.8', f'5000_train.csv'))\nSO2_80_IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy()\nSO2_80_IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy()\nSO2_80_IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy()\nSO2_80_IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()\n\nsurrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', 'bloodConc_138', 'SO2_0.6', f'5000_train.csv'))\nSO2_60_IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy()\nSO2_60_IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy()\nSO2_60_IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy()\nSO2_60_IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()\n</pre> surrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', 'bloodConc_138', 'SO2_0.7', f'5000_train.csv')) SO2_70_IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy() SO2_70_IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy() SO2_70_IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy() SO2_70_IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()  surrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', 'bloodConc_138', 'SO2_0.8', f'5000_train.csv')) SO2_80_IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy() SO2_80_IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy() SO2_80_IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy() SO2_80_IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy()  surrogate_result = pd.read_csv(os.path.join('dataset', 'surrogate_result', 'train', 'bloodConc_138', 'SO2_0.6', f'5000_train.csv')) SO2_60_IJV_large_SDS1 = surrogate_result['largeIJV_SDS1'].to_numpy() SO2_60_IJV_large_SDS2 = surrogate_result['largeIJV_SDS2'].to_numpy() SO2_60_IJV_small_SDS1 = surrogate_result['smallIJV_SDS1'].to_numpy() SO2_60_IJV_small_SDS2 = surrogate_result['smallIJV_SDS2'].to_numpy() In\u00a0[114]: Copied! <pre>plt.plot(SO2_70_IJV_large_SDS1, label='SO2=70%, IJV_large')\nplt.plot(SO2_70_IJV_small_SDS1, label='SO2=70%, IJV_small')\nplt.plot(SO2_60_IJV_large_SDS1, label='SO2=60%, IJV_large')\nplt.plot(SO2_60_IJV_small_SDS1, label='SO2=60%, IJV_small')\nplt.plot(SO2_80_IJV_large_SDS1, label='SO2=80%, IJV_large')\nplt.plot(SO2_80_IJV_small_SDS1, label='SO2=80%, IJV_small')\nplt.legend()\n# plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS1_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.plot(SO2_70_IJV_large_SDS1, label='SO2=70%, IJV_large') plt.plot(SO2_70_IJV_small_SDS1, label='SO2=70%, IJV_small') plt.plot(SO2_60_IJV_large_SDS1, label='SO2=60%, IJV_large') plt.plot(SO2_60_IJV_small_SDS1, label='SO2=60%, IJV_small') plt.plot(SO2_80_IJV_large_SDS1, label='SO2=80%, IJV_large') plt.plot(SO2_80_IJV_small_SDS1, label='SO2=80%, IJV_small') plt.legend() # plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS1_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[115]: Copied! <pre>plt.plot(SO2_70_IJV_large_SDS2, label='SO2=70%, IJV_large')\nplt.plot(SO2_70_IJV_small_SDS2, label='SO2=70%, IJV_small')\nplt.plot(SO2_60_IJV_large_SDS2, label='SO2=60%, IJV_large')\nplt.plot(SO2_60_IJV_small_SDS2, label='SO2=60%, IJV_small')\nplt.plot(SO2_80_IJV_large_SDS2, label='SO2=80%, IJV_large')\nplt.plot(SO2_80_IJV_small_SDS2, label='SO2=80%, IJV_small')\nplt.legend()\n# plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS1_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.plot(SO2_70_IJV_large_SDS2, label='SO2=70%, IJV_large') plt.plot(SO2_70_IJV_small_SDS2, label='SO2=70%, IJV_small') plt.plot(SO2_60_IJV_large_SDS2, label='SO2=60%, IJV_large') plt.plot(SO2_60_IJV_small_SDS2, label='SO2=60%, IJV_small') plt.plot(SO2_80_IJV_large_SDS2, label='SO2=80%, IJV_large') plt.plot(SO2_80_IJV_small_SDS2, label='SO2=80%, IJV_small') plt.legend() # plt.savefig(os.path.join(\"pic\", 'test_Rmax_Rmin', \"SDS1_Rmax_Rmin.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"prediction_model/plot_spectrum/","title":"plot similarity","text":"In\u00a0[1]: Copied! <pre>import pandas as pd\nimport numpy as np\nimport os\nimport json\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom sklearn.manifold import TSNE\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\n</pre> import pandas as pd import numpy as np import os import json import matplotlib.pyplot as plt import matplotlib as mpl from sklearn.manifold import TSNE # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_32296\\273221587.py:10: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[2]: Copied! <pre>def plot_formula3_delta_OD(dataset, wavelength, subject, used_idx, result_folder):\n    data = dataset.iloc[used_idx]\n    now_ijv_SO2 = round(data['true'])\n    fig, ax = plt.subplots(5,4,figsize=(16,12))\n    fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.1f}%', fontsize=20)\n    for i in range(20):\n        ijv_large_spec = []\n        ijv_small_spec = []\n        for j in range(20):\n            ijv_large_spec.append(data[f'data_value_{j+i*20}'])\n            ijv_small_spec.append(data[f'data_value_{400+j+i*20}'])\n        ijv_large_spec = np.array(ijv_large_spec)\n        # if (ijv_large_spec.max() - ijv_large_spec.min()) != 0:\n        #     ijv_large_spec = (ijv_large_spec - ijv_large_spec.min())/ (ijv_large_spec.max() - ijv_large_spec.min())\n        \n        ijv_small_spec = np.array(ijv_small_spec)\n        # if (ijv_small_spec.max() - ijv_small_spec.min()) != 0:\n        #     ijv_small_spec = (ijv_small_spec - ijv_small_spec.min())/ (ijv_small_spec.max() - ijv_small_spec.min())\n        \n        ax[i//4][i%4].plot(wavelength, ijv_large_spec, label=r'$IJV_{large}$')\n        ax[i//4][i%4].plot(wavelength, ijv_small_spec, label=r'$IJV_{small}$')\n        ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")\n        ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")\n        ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')\n        ax[i//4][i%4].legend()\n    fig.tight_layout()\n    fig.savefig(os.path.join(\"pic\", subject, result_folder, \"delta_OD\", f\"{now_ijv_SO2}_delta_OD.png\"), dpi=300, format='png', bbox_inches='tight')\n    plt.show()\n</pre> def plot_formula3_delta_OD(dataset, wavelength, subject, used_idx, result_folder):     data = dataset.iloc[used_idx]     now_ijv_SO2 = round(data['true'])     fig, ax = plt.subplots(5,4,figsize=(16,12))     fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.1f}%', fontsize=20)     for i in range(20):         ijv_large_spec = []         ijv_small_spec = []         for j in range(20):             ijv_large_spec.append(data[f'data_value_{j+i*20}'])             ijv_small_spec.append(data[f'data_value_{400+j+i*20}'])         ijv_large_spec = np.array(ijv_large_spec)         # if (ijv_large_spec.max() - ijv_large_spec.min()) != 0:         #     ijv_large_spec = (ijv_large_spec - ijv_large_spec.min())/ (ijv_large_spec.max() - ijv_large_spec.min())                  ijv_small_spec = np.array(ijv_small_spec)         # if (ijv_small_spec.max() - ijv_small_spec.min()) != 0:         #     ijv_small_spec = (ijv_small_spec - ijv_small_spec.min())/ (ijv_small_spec.max() - ijv_small_spec.min())                  ax[i//4][i%4].plot(wavelength, ijv_large_spec, label=r'$IJV_{large}$')         ax[i//4][i%4].plot(wavelength, ijv_small_spec, label=r'$IJV_{small}$')         ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")         ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")         ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')         ax[i//4][i%4].legend()     fig.tight_layout()     fig.savefig(os.path.join(\"pic\", subject, result_folder, \"delta_OD\", f\"{now_ijv_SO2}_delta_OD.png\"), dpi=300, format='png', bbox_inches='tight')     plt.show() In\u00a0[3]: Copied! <pre>def plot_formula2_delta_OD(dataset, wavelength, subject, used_idx, result_folder):\n    data = dataset.iloc[used_idx]\n    now_ijv_SO2 = round(data['true'])\n    # fig, ax = plt.subplots(5,4,figsize=(16,12))\n    # fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.1f}%', fontsize=20)\n    ijv_large_spec = []\n    ijv_small_spec = []\n    for i in range(20):\n        ijv_large_spec.append(data[f'data_value_{i}'])\n        ijv_small_spec.append(data[f'data_value_{20 + i}'])\n    plt.plot(wavelength, ijv_large_spec, label=r'$IJV_{large}$')\n    plt.plot(wavelength, ijv_small_spec, label=r'$IJV_{small}$')\n    plt.xlabel(\"wavelength(nm)\")\n    plt.ylabel(f\"$\\Delta$OD\")\n    plt.title(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.1f}%')\n    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                            fancybox=True, shadow=True)\n    plt.tight_layout()\n    plt.savefig(os.path.join(\"pic\", subject, result_folder, \"delta_OD\", f\"{now_ijv_SO2}_delta_OD.png\"), dpi=300, format='png', bbox_inches='tight')\n    plt.show()\n</pre> def plot_formula2_delta_OD(dataset, wavelength, subject, used_idx, result_folder):     data = dataset.iloc[used_idx]     now_ijv_SO2 = round(data['true'])     # fig, ax = plt.subplots(5,4,figsize=(16,12))     # fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.1f}%', fontsize=20)     ijv_large_spec = []     ijv_small_spec = []     for i in range(20):         ijv_large_spec.append(data[f'data_value_{i}'])         ijv_small_spec.append(data[f'data_value_{20 + i}'])     plt.plot(wavelength, ijv_large_spec, label=r'$IJV_{large}$')     plt.plot(wavelength, ijv_small_spec, label=r'$IJV_{small}$')     plt.xlabel(\"wavelength(nm)\")     plt.ylabel(f\"$\\Delta$OD\")     plt.title(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.1f}%')     plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                             fancybox=True, shadow=True)     plt.tight_layout()     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"delta_OD\", f\"{now_ijv_SO2}_delta_OD.png\"), dpi=300, format='png', bbox_inches='tight')     plt.show() In\u00a0[4]: Copied! <pre>def plot_2D_tsne(dataset, num_input, get_portion, perplexity):\n    # get dataset and label\n    dataset = dataset.sort_values('true')\n    use_col = []\n    for i in range(num_input):\n        use_col += [f'data_value_{i}']\n    spec_data = dataset[use_col].to_numpy()\n    label = dataset['true'].to_numpy()\n\n    get_portion = get_portion\n    used_index = []\n    for SO2_idx in range(51):\n        used_index += [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]\n    spec_data = spec_data[used_index]\n    label = np.round(label[used_index],2)\n    \n    # TSNE\n    X_embedded = TSNE(n_components=2, perplexity=perplexity, n_iter=5000).fit_transform(spec_data)\n    \n    # plot\n    plot_data = X_embedded\n    x_min, x_max = np.min(plot_data, axis=0), np.max(plot_data, axis=0)\n    plot_data = (plot_data-x_min) / (x_max - x_min)\n    fig = plt.figure()\n    for i in range(plot_data.shape[0]):\n        color = (label[i]+30)/50\n        if i% get_portion == 0:\n            plt.scatter(plot_data[i,0], plot_data[i,1], s=5, color=plt.colormaps['rainbow'](color), label=f'{label[i]:2.0f}%')\n        else:\n            plt.scatter(plot_data[i,0], plot_data[i,1], s=5, color=plt.colormaps['rainbow'](color))\n        # plt.text(plot_data[i,0], plot_data[i,1], str(label[i]), color=plt.cm.Set1(label[i]), fontdict={'weight': 'bold', 'size' : 9})\n    plt.title(\"TSNE\")\n    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                            fancybox=True, shadow=True, ncol=3)\n    plt.xticks([])\n    plt.yticks([])\n    \n    return fig\n\ndef plot_3D_tsne(dataset, num_input, get_portion, perplexity):\n    # get dataset and label\n    dataset = dataset.sort_values('true')\n    use_col = []\n    for i in range(num_input):\n        use_col += [f'data_value_{i}']\n    spec_data = dataset[use_col].to_numpy()\n    label = dataset['true'].to_numpy()\n\n    get_portion = get_portion\n    used_index = []\n    for SO2_idx in range(51):\n        used_index += [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]\n    spec_data = spec_data[used_index]\n    label = np.round(label[used_index],2)\n    \n    # TSNE\n    X_embedded = TSNE(n_components=3, perplexity=perplexity, n_iter=5000).fit_transform(spec_data)\n    \n    # plot\n    plot_data = X_embedded\n    x_min, x_max = np.min(plot_data, axis=0), np.max(plot_data, axis=0)\n    plot_data = (plot_data-x_min) / (x_max - x_min)\n\n    fig = plt.figure()\n    ax = fig.add_subplot(projection='3d')\n    for i in range(plot_data.shape[0]):\n        color = (label[i]+30)/50\n        if i% get_portion == 0:\n            ax.scatter(plot_data[i,0], plot_data[i,1], plot_data[i,2], s=5, color=plt.colormaps['rainbow'](color), label=f'{label[i]:2.0f}%')\n        else:\n            ax.scatter(plot_data[i,0], plot_data[i,1], plot_data[i,2], s=5, color=plt.colormaps['rainbow'](color))\n        # plt.text(plot_data[i,0], plot_data[i,1], str(label[i]), color=plt.cm.Set1(label[i]), fontdict={'weight': 'bold', 'size' : 9})\n    plt.title(\"TSNE\")\n    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                            fancybox=True, shadow=True, ncol=3)\n    plt.xticks([])\n    plt.yticks([])\n    ax.set_zticks([])\n    \n    return fig\n    \n</pre> def plot_2D_tsne(dataset, num_input, get_portion, perplexity):     # get dataset and label     dataset = dataset.sort_values('true')     use_col = []     for i in range(num_input):         use_col += [f'data_value_{i}']     spec_data = dataset[use_col].to_numpy()     label = dataset['true'].to_numpy()      get_portion = get_portion     used_index = []     for SO2_idx in range(51):         used_index += [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]     spec_data = spec_data[used_index]     label = np.round(label[used_index],2)          # TSNE     X_embedded = TSNE(n_components=2, perplexity=perplexity, n_iter=5000).fit_transform(spec_data)          # plot     plot_data = X_embedded     x_min, x_max = np.min(plot_data, axis=0), np.max(plot_data, axis=0)     plot_data = (plot_data-x_min) / (x_max - x_min)     fig = plt.figure()     for i in range(plot_data.shape[0]):         color = (label[i]+30)/50         if i% get_portion == 0:             plt.scatter(plot_data[i,0], plot_data[i,1], s=5, color=plt.colormaps['rainbow'](color), label=f'{label[i]:2.0f}%')         else:             plt.scatter(plot_data[i,0], plot_data[i,1], s=5, color=plt.colormaps['rainbow'](color))         # plt.text(plot_data[i,0], plot_data[i,1], str(label[i]), color=plt.cm.Set1(label[i]), fontdict={'weight': 'bold', 'size' : 9})     plt.title(\"TSNE\")     plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                             fancybox=True, shadow=True, ncol=3)     plt.xticks([])     plt.yticks([])          return fig  def plot_3D_tsne(dataset, num_input, get_portion, perplexity):     # get dataset and label     dataset = dataset.sort_values('true')     use_col = []     for i in range(num_input):         use_col += [f'data_value_{i}']     spec_data = dataset[use_col].to_numpy()     label = dataset['true'].to_numpy()      get_portion = get_portion     used_index = []     for SO2_idx in range(51):         used_index += [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]     spec_data = spec_data[used_index]     label = np.round(label[used_index],2)          # TSNE     X_embedded = TSNE(n_components=3, perplexity=perplexity, n_iter=5000).fit_transform(spec_data)          # plot     plot_data = X_embedded     x_min, x_max = np.min(plot_data, axis=0), np.max(plot_data, axis=0)     plot_data = (plot_data-x_min) / (x_max - x_min)      fig = plt.figure()     ax = fig.add_subplot(projection='3d')     for i in range(plot_data.shape[0]):         color = (label[i]+30)/50         if i% get_portion == 0:             ax.scatter(plot_data[i,0], plot_data[i,1], plot_data[i,2], s=5, color=plt.colormaps['rainbow'](color), label=f'{label[i]:2.0f}%')         else:             ax.scatter(plot_data[i,0], plot_data[i,1], plot_data[i,2], s=5, color=plt.colormaps['rainbow'](color))         # plt.text(plot_data[i,0], plot_data[i,1], str(label[i]), color=plt.cm.Set1(label[i]), fontdict={'weight': 'bold', 'size' : 9})     plt.title(\"TSNE\")     plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                             fancybox=True, shadow=True, ncol=3)     plt.xticks([])     plt.yticks([])     ax.set_zticks([])          return fig      In\u00a0[9]: Copied! <pre>a = np.array([[0,1],[2,3]])\nb = []\nb += a.tolist()\nb\n</pre> a = np.array([[0,1],[2,3]]) b = [] b += a.tolist() b Out[9]: <pre>[[0, 1], [2, 3]]</pre> In\u00a0[5]: Copied! <pre>subject = \"ctchen\"\nresult_folder = \"prediction_model_formula24\"\n\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"delta_OD\"), exist_ok=True)\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\"), exist_ok=True)\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\"), exist_ok=True)\nfile_path = os.path.join(\"dataset\", result_folder, \"RMSE.csv\")\ndataset_formula3 = pd.read_csv(file_path)\nwith open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\n\nfor find_SO2 in range(-30,20):\n# for find_SO2 in [-30,20]:\n    used_idx = np.where(abs(dataset_formula3['true'].to_numpy()-find_SO2) &lt; 0.01)[0][5]\n    plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=used_idx, result_folder=result_folder)\n# plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=988, result_folder=result_folder)\n# plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=0, result_folder=result_folder)\n\n# plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=494, result_folder=result_folder)\n\n\n# for perplexity in [2,5,30,50,100]:\n#     fig = plot_2D_tsne(dataset=dataset_formula3, num_input=800, get_portion=100, perplexity=perplexity)\n#     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\", f\"p_{perplexity}_tsne_2d.png\"), dpi=300, format='png', bbox_inches='tight')\n#     plt.show(fig)\n#     fig = plot_3D_tsne(dataset=dataset_formula3, num_input=800, get_portion=100, perplexity=perplexity)\n#     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\", f\"p_{perplexity}_tsne_3d.png\"), dpi=300, format='png', bbox_inches='tight')\n#     plt.show(fig)\n</pre>  subject = \"ctchen\" result_folder = \"prediction_model_formula24\"  os.makedirs(os.path.join(\"pic\", subject, result_folder, \"delta_OD\"), exist_ok=True) os.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\"), exist_ok=True) os.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\"), exist_ok=True) file_path = os.path.join(\"dataset\", result_folder, \"RMSE.csv\") dataset_formula3 = pd.read_csv(file_path) with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength']  for find_SO2 in range(-30,20): # for find_SO2 in [-30,20]:     used_idx = np.where(abs(dataset_formula3['true'].to_numpy()-find_SO2) &lt; 0.01)[0][5]     plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=used_idx, result_folder=result_folder) # plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=988, result_folder=result_folder) # plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=0, result_folder=result_folder)  # plot_formula3_delta_OD(dataset=dataset_formula3, wavelength=wavelength, subject=subject, used_idx=494, result_folder=result_folder)   # for perplexity in [2,5,30,50,100]: #     fig = plot_2D_tsne(dataset=dataset_formula3, num_input=800, get_portion=100, perplexity=perplexity) #     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\", f\"p_{perplexity}_tsne_2d.png\"), dpi=300, format='png', bbox_inches='tight') #     plt.show(fig) #     fig = plot_3D_tsne(dataset=dataset_formula3, num_input=800, get_portion=100, perplexity=perplexity) #     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\", f\"p_{perplexity}_tsne_3d.png\"), dpi=300, format='png', bbox_inches='tight') #     plt.show(fig) In\u00a0[106]: Copied! <pre>subject = \"ctchen\"\nresult_folder = \"prediction_model_formula2\"\n\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"delta_OD\"), exist_ok=True)\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\"), exist_ok=True)\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\"), exist_ok=True)\nfile_path = os.path.join(\"dataset\", result_folder, \"RMSE.csv\")\ndataset_formula2 = pd.read_csv(file_path)\nwith open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\n\nplot_formula2_delta_OD(dataset=dataset_formula2, wavelength=wavelength, subject=subject, used_idx=203959, result_folder=result_folder)\nplot_formula2_delta_OD(dataset=dataset_formula2, wavelength=wavelength, subject=subject, used_idx=0, result_folder=result_folder)\nfor perplexity in [2,5,30,50,100]:\n    fig = plot_2D_tsne(dataset=dataset_formula2, num_input=40, get_portion=100, perplexity=perplexity)\n    plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\", f\"p_{perplexity}_tsne_2d.png\"), dpi=300, format='png', bbox_inches='tight')\n    plt.show(fig)\n    fig = plot_3D_tsne(dataset=dataset_formula2, num_input=40, get_portion=100, perplexity=perplexity)\n    plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\", f\"p_{perplexity}_tsne_3d.png\"), dpi=300, format='png', bbox_inches='tight')\n    plt.show(fig)\n</pre> subject = \"ctchen\" result_folder = \"prediction_model_formula2\"  os.makedirs(os.path.join(\"pic\", subject, result_folder, \"delta_OD\"), exist_ok=True) os.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\"), exist_ok=True) os.makedirs(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\"), exist_ok=True) file_path = os.path.join(\"dataset\", result_folder, \"RMSE.csv\") dataset_formula2 = pd.read_csv(file_path) with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength']  plot_formula2_delta_OD(dataset=dataset_formula2, wavelength=wavelength, subject=subject, used_idx=203959, result_folder=result_folder) plot_formula2_delta_OD(dataset=dataset_formula2, wavelength=wavelength, subject=subject, used_idx=0, result_folder=result_folder) for perplexity in [2,5,30,50,100]:     fig = plot_2D_tsne(dataset=dataset_formula2, num_input=40, get_portion=100, perplexity=perplexity)     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_2d\", f\"p_{perplexity}_tsne_2d.png\"), dpi=300, format='png', bbox_inches='tight')     plt.show(fig)     fig = plot_3D_tsne(dataset=dataset_formula2, num_input=40, get_portion=100, perplexity=perplexity)     plt.savefig(os.path.join(\"pic\", subject, result_folder, \"tsne_3d\", f\"p_{perplexity}_tsne_3d.png\"), dpi=300, format='png', bbox_inches='tight')     plt.show(fig) In\u00a0[57]: Copied! <pre>subject = \"ctchen\"\nresult_folder = \"prediction_model_formula3\"\n\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"similarity\"), exist_ok=True)\nfile_path = os.path.join(\"dataset\", result_folder, \"RMSE.csv\")\ndataset_formula3 = pd.read_csv(file_path)\n</pre> subject = \"ctchen\" result_folder = \"prediction_model_formula3\"  os.makedirs(os.path.join(\"pic\", subject, result_folder, \"similarity\"), exist_ok=True) file_path = os.path.join(\"dataset\", result_folder, \"RMSE.csv\") dataset_formula3 = pd.read_csv(file_path) In\u00a0[58]: Copied! <pre>SO2_used = [i for i in range(-30,21)]\n# get dataset and label\ndataset = dataset_formula3.sort_values('true')\nuse_col = []\nuse_col = ['error']\nsimilarity_set = dataset[use_col].to_numpy()\nlabel = dataset['true'].to_numpy()\n\nget_portion = 4000\nsimilarity_data = {}\nlabel_dict = {}\nfor SO2_idx in range(len(SO2_used)):\n    used_index = [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]\n    similarity_data[SO2_used[SO2_idx]] = similarity_set[used_index]\n    label_dict[SO2_used[SO2_idx]] = np.round(label[used_index],2)\n    # label = np.round(label[used_index],2)\n\naccumulate_mean = []\naccumulate_std = []\nfor using_SO2 in similarity_data.keys():\n    # similarity_set = dataset_formula2[dataset_formula2['true'] == using_SO2]['similarity_0'].to_numpy()\n    similarity_mean = np.mean(similarity_data[using_SO2])\n    # print(f'{similarity_mean}')\n    similarity_std = np.std(similarity_data[using_SO2])\n    accumulate_mean += [similarity_mean]\n    accumulate_std += [similarity_std]\nplt.figure()\nplt.plot(similarity_data.keys(), accumulate_mean, label=r'$\\mu$')\nplt.fill_between(similarity_data.keys(), np.array(accumulate_mean)+2*np.array(accumulate_std), np.array(accumulate_mean)-2*np.array(accumulate_std), label=r'$\\mu$$\\pm$$\\sigma$', alpha=0.5)\nplt.title(\"analyze the similarity of $\\u0394$OD spectrum\")\nplt.xlabel(\"$\\u0394$SO2(%)\")\nplt.ylabel(\"relative error (%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability_error.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> SO2_used = [i for i in range(-30,21)] # get dataset and label dataset = dataset_formula3.sort_values('true') use_col = [] use_col = ['error'] similarity_set = dataset[use_col].to_numpy() label = dataset['true'].to_numpy()  get_portion = 4000 similarity_data = {} label_dict = {} for SO2_idx in range(len(SO2_used)):     used_index = [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]     similarity_data[SO2_used[SO2_idx]] = similarity_set[used_index]     label_dict[SO2_used[SO2_idx]] = np.round(label[used_index],2)     # label = np.round(label[used_index],2)  accumulate_mean = [] accumulate_std = [] for using_SO2 in similarity_data.keys():     # similarity_set = dataset_formula2[dataset_formula2['true'] == using_SO2]['similarity_0'].to_numpy()     similarity_mean = np.mean(similarity_data[using_SO2])     # print(f'{similarity_mean}')     similarity_std = np.std(similarity_data[using_SO2])     accumulate_mean += [similarity_mean]     accumulate_std += [similarity_std] plt.figure() plt.plot(similarity_data.keys(), accumulate_mean, label=r'$\\mu$') plt.fill_between(similarity_data.keys(), np.array(accumulate_mean)+2*np.array(accumulate_std), np.array(accumulate_mean)-2*np.array(accumulate_std), label=r'$\\mu$$\\pm$$\\sigma$', alpha=0.5) plt.title(\"analyze the similarity of $\\u0394$OD spectrum\") plt.xlabel(\"$\\u0394$SO2(%)\") plt.ylabel(\"relative error (%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability_error.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[3]: Copied! <pre>subject = \"ctchen\"\nresult_folder = \"prediction_model_formula2\"\n\nos.makedirs(os.path.join(\"pic\", subject, result_folder, \"similarity\"), exist_ok=True)\nfile_path = os.path.join(\"dataset\", result_folder, \"similar_analysis.csv\")\ndataset_formula2 = pd.read_csv(file_path)\n</pre> subject = \"ctchen\" result_folder = \"prediction_model_formula2\"  os.makedirs(os.path.join(\"pic\", subject, result_folder, \"similarity\"), exist_ok=True) file_path = os.path.join(\"dataset\", result_folder, \"similar_analysis.csv\") dataset_formula2 = pd.read_csv(file_path) In\u00a0[33]: Copied! <pre>error = dataset_formula2['abs_error'].to_numpy()\nusing_col = [f'similarity_0']\nsimilarity_mean = dataset_formula2[using_col].to_numpy()\nsimilarity_mean = np.mean(similarity_mean, axis=1)\nplt.scatter(error, similarity_mean, label=r'$\\mu$', s=0.1)\nplt.show()\n</pre> error = dataset_formula2['abs_error'].to_numpy() using_col = [f'similarity_0'] similarity_mean = dataset_formula2[using_col].to_numpy() similarity_mean = np.mean(similarity_mean, axis=1) plt.scatter(error, similarity_mean, label=r'$\\mu$', s=0.1) plt.show() In\u00a0[30]: Copied! <pre>error = dataset_formula2['abs_error'].to_numpy()\nusing_col = []\nfor i in range(10):\n    using_col += [f'similarity_{i}']\nsimilarity_mean = dataset_formula2[using_col].to_numpy()\nsimilarity_mean = np.mean(similarity_mean, axis=1)\nplt.scatter(error, similarity_mean, label=r'$\\mu$', s=0.1)\nplt.show()\n</pre> error = dataset_formula2['abs_error'].to_numpy() using_col = [] for i in range(10):     using_col += [f'similarity_{i}'] similarity_mean = dataset_formula2[using_col].to_numpy() similarity_mean = np.mean(similarity_mean, axis=1) plt.scatter(error, similarity_mean, label=r'$\\mu$', s=0.1) plt.show() In\u00a0[6]: Copied! <pre>error = dataset_formula2['error'].to_numpy()\nsimilarity_mean = dataset_formula2['similarity_mean'].to_numpy()\nplt.scatter(error, similarity_mean, label=r'$\\mu$', s=0.5)\nplt.title(\"relative error(%)\")\nplt.xlabel(\"relative error (%)\")\nplt.ylabel('$\\u0394$OD spectrum variability(%)')\nplt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability_scatter_error.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> error = dataset_formula2['error'].to_numpy() similarity_mean = dataset_formula2['similarity_mean'].to_numpy() plt.scatter(error, similarity_mean, label=r'$\\mu$', s=0.5) plt.title(\"relative error(%)\") plt.xlabel(\"relative error (%)\") plt.ylabel('$\\u0394$OD spectrum variability(%)') plt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability_scatter_error.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[55]: Copied! <pre>SO2_used = [i for i in range(-30,21)]\n# get dataset and label\ndataset = dataset_formula2.sort_values('true')\nuse_col = []\nuse_col = ['error']\nsimilarity_set = dataset[use_col].to_numpy()\nlabel = dataset['true'].to_numpy()\n\nget_portion = 4000\nsimilarity_data = {}\nlabel_dict = {}\nfor SO2_idx in range(len(SO2_used)):\n    used_index = [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]\n    similarity_data[SO2_used[SO2_idx]] = similarity_set[used_index]\n    label_dict[SO2_used[SO2_idx]] = np.round(label[used_index],2)\n    # label = np.round(label[used_index],2)\n\naccumulate_mean = []\naccumulate_std = []\nfor using_SO2 in similarity_data.keys():\n    # similarity_set = dataset_formula2[dataset_formula2['true'] == using_SO2]['similarity_0'].to_numpy()\n    similarity_mean = np.mean(similarity_data[using_SO2])\n    # print(f'{similarity_mean}')\n    similarity_std = np.std(similarity_data[using_SO2])\n    accumulate_mean += [similarity_mean]\n    accumulate_std += [similarity_std]\nplt.figure()\nplt.plot(similarity_data.keys(), accumulate_mean, label=r'$\\mu$')\nplt.fill_between(similarity_data.keys(), np.array(accumulate_mean)+2*np.array(accumulate_std), np.array(accumulate_mean)-2*np.array(accumulate_std), label=r'$\\mu$$\\pm$$\\sigma$', alpha=0.5)\nplt.title(\"analyze the similarity of $\\u0394$OD spectrum\")\nplt.xlabel(\"$\\u0394$SO2(%)\")\nplt.ylabel(\"relative error (%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability_error.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> SO2_used = [i for i in range(-30,21)] # get dataset and label dataset = dataset_formula2.sort_values('true') use_col = [] use_col = ['error'] similarity_set = dataset[use_col].to_numpy() label = dataset['true'].to_numpy()  get_portion = 4000 similarity_data = {} label_dict = {} for SO2_idx in range(len(SO2_used)):     used_index = [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]     similarity_data[SO2_used[SO2_idx]] = similarity_set[used_index]     label_dict[SO2_used[SO2_idx]] = np.round(label[used_index],2)     # label = np.round(label[used_index],2)  accumulate_mean = [] accumulate_std = [] for using_SO2 in similarity_data.keys():     # similarity_set = dataset_formula2[dataset_formula2['true'] == using_SO2]['similarity_0'].to_numpy()     similarity_mean = np.mean(similarity_data[using_SO2])     # print(f'{similarity_mean}')     similarity_std = np.std(similarity_data[using_SO2])     accumulate_mean += [similarity_mean]     accumulate_std += [similarity_std] plt.figure() plt.plot(similarity_data.keys(), accumulate_mean, label=r'$\\mu$') plt.fill_between(similarity_data.keys(), np.array(accumulate_mean)+2*np.array(accumulate_std), np.array(accumulate_mean)-2*np.array(accumulate_std), label=r'$\\mu$$\\pm$$\\sigma$', alpha=0.5) plt.title(\"analyze the similarity of $\\u0394$OD spectrum\") plt.xlabel(\"$\\u0394$SO2(%)\") plt.ylabel(\"relative error (%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability_error.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[41]: Copied! <pre>SO2_used = [i for i in range(-30,21)]\n# get dataset and label\ndataset = dataset_formula2.sort_values('true')\nuse_col = []\nuse_col = ['similarity_mean']\nsimilarity_set = dataset[use_col].to_numpy()\nlabel = dataset['true'].to_numpy()\n\nget_portion = 4000\nsimilarity_data = {}\nlabel_dict = {}\nfor SO2_idx in range(len(SO2_used)):\n    used_index = [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]\n    similarity_data[SO2_used[SO2_idx]] = similarity_set[used_index]\n    label_dict[SO2_used[SO2_idx]] = np.round(label[used_index],2)\n    # label = np.round(label[used_index],2)\n\naccumulate_mean = []\naccumulate_std = []\nfor using_SO2 in similarity_data.keys():\n    # similarity_set = dataset_formula2[dataset_formula2['true'] == using_SO2]['similarity_0'].to_numpy()\n    similarity_mean = np.mean(similarity_data[using_SO2])\n    # print(f'{similarity_mean}')\n    similarity_std = np.std(similarity_data[using_SO2])\n    accumulate_mean += [similarity_mean]\n    accumulate_std += [similarity_std]\nplt.figure()\nplt.plot(similarity_data.keys(), accumulate_mean, label=r'$\\mu$')\nplt.fill_between(similarity_data.keys(), np.array(accumulate_mean)+2*np.array(accumulate_std), np.array(accumulate_mean)-2*np.array(accumulate_std), label=r'$\\mu$$\\pm$$\\sigma$', alpha=0.5)\nplt.title(\"analyze the similarity of $\\u0394$OD spectrum\")\nplt.xlabel(\"$\\u0394$SO2(%)\")\nplt.ylabel(\"$\\u0394$OD spectrum variability(%)\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> SO2_used = [i for i in range(-30,21)] # get dataset and label dataset = dataset_formula2.sort_values('true') use_col = [] use_col = ['similarity_mean'] similarity_set = dataset[use_col].to_numpy() label = dataset['true'].to_numpy()  get_portion = 4000 similarity_data = {} label_dict = {} for SO2_idx in range(len(SO2_used)):     used_index = [i for i in range(SO2_idx*4000,SO2_idx*4000+get_portion)]     similarity_data[SO2_used[SO2_idx]] = similarity_set[used_index]     label_dict[SO2_used[SO2_idx]] = np.round(label[used_index],2)     # label = np.round(label[used_index],2)  accumulate_mean = [] accumulate_std = [] for using_SO2 in similarity_data.keys():     # similarity_set = dataset_formula2[dataset_formula2['true'] == using_SO2]['similarity_0'].to_numpy()     similarity_mean = np.mean(similarity_data[using_SO2])     # print(f'{similarity_mean}')     similarity_std = np.std(similarity_data[using_SO2])     accumulate_mean += [similarity_mean]     accumulate_std += [similarity_std] plt.figure() plt.plot(similarity_data.keys(), accumulate_mean, label=r'$\\mu$') plt.fill_between(similarity_data.keys(), np.array(accumulate_mean)+2*np.array(accumulate_std), np.array(accumulate_mean)-2*np.array(accumulate_std), label=r'$\\mu$$\\pm$$\\sigma$', alpha=0.5) plt.title(\"analyze the similarity of $\\u0394$OD spectrum\") plt.xlabel(\"$\\u0394$SO2(%)\") plt.ylabel(\"$\\u0394$OD spectrum variability(%)\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(\"pic\", subject, result_folder, \"similarity\", \"variability.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()"},{"location":"prediction_model/plot_spectrum/#plot-similarity","title":"plot similarity\u00b6","text":""},{"location":"prediction_model/plot_spectrum2/","title":"Plot Spectrum2","text":"In\u00a0[1]: Copied! <pre>import numpy as np\nimport os \nimport pandas as pd\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport json\n</pre> import numpy as np import os  import pandas as pd from glob import glob import matplotlib.pyplot as plt import json In\u00a0[2]: Copied! <pre>with open(os.path.join(\"OPs_used\", \"bloodConc.json\"), \"r\") as f:\n    bloodConc = json.load(f)\n    bloodConc = bloodConc['bloodConc']\nwith open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\nwith open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:\n    SO2 = json.load(f)\n    train_SO2 = SO2['train_SO2']\n    test_SO2 = SO2['test_SO2']\n</pre> with open(os.path.join(\"OPs_used\", \"bloodConc.json\"), \"r\") as f:     bloodConc = json.load(f)     bloodConc = bloodConc['bloodConc'] with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength'] with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:     SO2 = json.load(f)     train_SO2 = SO2['train_SO2']     test_SO2 = SO2['test_SO2'] In\u00a0[3]: Copied! <pre>file_set = glob(os.path.join('dataset', 'prediction_model_formula3', 'test','*'))\nlen(file_set)\n</pre> file_set = glob(os.path.join('dataset', 'prediction_model_formula3', 'test','*')) len(file_set) Out[3]: <pre>4000</pre> In\u00a0[4]: Copied! <pre>file_set[0]\n</pre> file_set[0] Out[4]: <pre>'dataset\\\\prediction_model_formula3\\\\test\\\\0_blc_138.npy'</pre> In\u00a0[5]: Copied! <pre>data = np.load(file_set[2000])\ndata.shape\n</pre> data = np.load(file_set[2000]) data.shape Out[5]: <pre>(51, 805)</pre> In\u00a0[6]: Copied! <pre>OD_spec = data[25,:800]\nfor i in range(20):\n    plt.plot(OD_spec[i*20:i*20+20])\nplt.show()\n</pre> OD_spec = data[25,:800] for i in range(20):     plt.plot(OD_spec[i*20:i*20+20]) plt.show() In\u00a0[7]: Copied! <pre>file_set = glob(os.path.join('dataset', 'surrogate_result', 'test','bloodConc_138', 'SO2_0.7', '*'))\nlen(file_set)\n</pre> file_set = glob(os.path.join('dataset', 'surrogate_result', 'test','bloodConc_138', 'SO2_0.7', '*')) len(file_set) Out[7]: <pre>200</pre> In\u00a0[77]: Copied! <pre>file_set[0]\n</pre> file_set[0] Out[77]: <pre>'dataset\\\\surrogate_result\\\\test\\\\bloodConc_138\\\\SO2_0.7\\\\0_test.csv'</pre> In\u00a0[\u00a0]: Copied! <pre>baseline = \n</pre> baseline =  In\u00a0[75]: Copied! <pre>for filename in file_set:\n    data = pd.read_csv(filename)\n    plt.plot(wavelength, data['largeIJV_SDS2'])\nplt.xlabel('wavelength (nm)')\nplt.ylabel('reflectance')\nplt.show()\n</pre> for filename in file_set:     data = pd.read_csv(filename)     plt.plot(wavelength, data['largeIJV_SDS2']) plt.xlabel('wavelength (nm)') plt.ylabel('reflectance') plt.show() In\u00a0[21]: Copied! <pre>plt.plot(wavelength, data['smallIJV_SDS2'])\n\nplt.show()\n</pre> plt.plot(wavelength, data['smallIJV_SDS2'])  plt.show() In\u00a0[4]: Copied! <pre>with open(os.path.join(\"OPs_used\", \"bloodConc.json\"), \"r\") as f:\n    bloodConc = json.load(f)\n    bloodConc = bloodConc['bloodConc']\nwith open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:\n    wavelength = json.load(f)\n    wavelength = wavelength['wavelength']\nwith open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:\n    SO2 = json.load(f)\n    train_SO2 = SO2['train_SO2']\n    test_SO2 = SO2['test_SO2']\n</pre> with open(os.path.join(\"OPs_used\", \"bloodConc.json\"), \"r\") as f:     bloodConc = json.load(f)     bloodConc = bloodConc['bloodConc'] with open(os.path.join(\"OPs_used\", \"wavelength.json\"), 'r') as f:     wavelength = json.load(f)     wavelength = wavelength['wavelength'] with open(os.path.join(\"OPs_used\", \"SO2.json\"), 'r') as f:     SO2 = json.load(f)     train_SO2 = SO2['train_SO2']     test_SO2 = SO2['test_SO2'] In\u00a0[5]: Copied! <pre>result_OD1_spec = np.load(os.path.join('dataset', 'result_OD1_spec.npy'))\nresult_OD2_spec = np.load(os.path.join('dataset', 'result_OD2_spec.npy'))\nresult_diff_spec = result_OD2_spec - result_OD1_spec\n</pre> result_OD1_spec = np.load(os.path.join('dataset', 'result_OD1_spec.npy')) result_OD2_spec = np.load(os.path.join('dataset', 'result_OD2_spec.npy')) result_diff_spec = result_OD2_spec - result_OD1_spec In\u00a0[6]: Copied! <pre>blc = 138\nid = 199\nfor id in range(1):\n    # for blc in bloodConc:\n    for blc in [138]:\n        print(f'now processing test_{id}...')\n        prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id\n        for i, s in enumerate(test_SO2):\n            \n            surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n                                                        f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))\n            surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n                                                        f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))\n            for wl_idx in range(len(wavelength)):\n                T2_large_SDS2 = surrogate_result_T2['largeIJV_SDS2'].to_numpy()\n                T2_large_SDS1 = surrogate_result_T2['largeIJV_SDS1'][wl_idx]\n                T1_large_SDS2 = surrogate_result_T1['largeIJV_SDS2'].to_numpy()\n                T1_large_SDS1 = surrogate_result_T1['largeIJV_SDS1'][wl_idx]\n\n                T2_small_SDS2 = surrogate_result_T2['smallIJV_SDS2'].to_numpy()\n                T2_small_SDS1 = surrogate_result_T2['smallIJV_SDS1'][wl_idx]\n                T1_small_SDS2 = surrogate_result_T1['smallIJV_SDS2'].to_numpy()\n                T1_small_SDS1 = surrogate_result_T1['smallIJV_SDS1'][wl_idx]\n\n                prediction_input[i][wl_idx*20 : wl_idx*20+20] = T2_large_SDS1 /T2_large_SDS2 - T1_large_SDS1 /T1_large_SDS2\n                prediction_input[i][400+wl_idx*20 : 400+wl_idx*20+20] = T2_small_SDS1 / T2_small_SDS2  - T1_small_SDS1 / T1_small_SDS2\n\n#         invivo_OD2_spec_set = result_OD2_spec\n#         for SO2_idx in range(51):\n#             OD_spec = prediction_input[SO2_idx,:800]\n#             for i in range(20):\n#                 plt.plot(OD_spec[i*20:i*20+20], 'b--')\n                    \n        \n# for invivo_spec in result_diff_spec:\n#     for i in range(20):\n#         plt.plot(invivo_spec[i*20:i*20+20], 'r')\n# plt.show()\n        \n        \n</pre> blc = 138 id = 199 for id in range(1):     # for blc in bloodConc:     for blc in [138]:         print(f'now processing test_{id}...')         prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id         for i, s in enumerate(test_SO2):                          surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test',                                                          f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))             surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test',                                                          f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))             for wl_idx in range(len(wavelength)):                 T2_large_SDS2 = surrogate_result_T2['largeIJV_SDS2'].to_numpy()                 T2_large_SDS1 = surrogate_result_T2['largeIJV_SDS1'][wl_idx]                 T1_large_SDS2 = surrogate_result_T1['largeIJV_SDS2'].to_numpy()                 T1_large_SDS1 = surrogate_result_T1['largeIJV_SDS1'][wl_idx]                  T2_small_SDS2 = surrogate_result_T2['smallIJV_SDS2'].to_numpy()                 T2_small_SDS1 = surrogate_result_T2['smallIJV_SDS1'][wl_idx]                 T1_small_SDS2 = surrogate_result_T1['smallIJV_SDS2'].to_numpy()                 T1_small_SDS1 = surrogate_result_T1['smallIJV_SDS1'][wl_idx]                  prediction_input[i][wl_idx*20 : wl_idx*20+20] = T2_large_SDS1 /T2_large_SDS2 - T1_large_SDS1 /T1_large_SDS2                 prediction_input[i][400+wl_idx*20 : 400+wl_idx*20+20] = T2_small_SDS1 / T2_small_SDS2  - T1_small_SDS1 / T1_small_SDS2  #         invivo_OD2_spec_set = result_OD2_spec #         for SO2_idx in range(51): #             OD_spec = prediction_input[SO2_idx,:800] #             for i in range(20): #                 plt.plot(OD_spec[i*20:i*20+20], 'b--')                               # for invivo_spec in result_diff_spec: #     for i in range(20): #         plt.plot(invivo_spec[i*20:i*20+20], 'r') # plt.show()                   <pre>now processing test_0...\n</pre> In\u00a0[7]: Copied! <pre>invivo_OD2_spec_set = result_OD2_spec\nfor SO2_idx in range(51):\n    OD_spec = prediction_input[SO2_idx,:800]\n    for i in range(20):\n        plt.plot(OD_spec[i*20:i*20+20], 'b')\nfor invivo_spec in result_diff_spec:\n    for i in range(20):\n        plt.plot(invivo_spec[i*20:i*20+20], 'r')\nplt.show()\n</pre> invivo_OD2_spec_set = result_OD2_spec for SO2_idx in range(51):     OD_spec = prediction_input[SO2_idx,:800]     for i in range(20):         plt.plot(OD_spec[i*20:i*20+20], 'b') for invivo_spec in result_diff_spec:     for i in range(20):         plt.plot(invivo_spec[i*20:i*20+20], 'r') plt.show() In\u00a0[8]: Copied! <pre>for invivo_OD2_spec in invivo_OD2_spec_set:\n    for i in range(20):\n        plt.plot(invivo_OD2_spec[i*20:i*20+20], 'r')\nplt.show()\n</pre> for invivo_OD2_spec in invivo_OD2_spec_set:     for i in range(20):         plt.plot(invivo_OD2_spec[i*20:i*20+20], 'r') plt.show() In\u00a0[9]: Copied! <pre>result_diff_spec.shape\n</pre> result_diff_spec.shape Out[9]: <pre>(84, 800)</pre> In\u00a0[10]: Copied! <pre>for invivo_spec in result_diff_spec[:1]:\n    for i in range(20):\n        plt.plot(invivo_spec[i*20:i*20+20], 'r')\nplt.show()\n</pre> for invivo_spec in result_diff_spec[:1]:     for i in range(20):         plt.plot(invivo_spec[i*20:i*20+20], 'r') plt.show() In\u00a0[11]: Copied! <pre>result_diff_spec.shape\n</pre> result_diff_spec.shape Out[11]: <pre>(84, 800)</pre> In\u00a0[12]: Copied! <pre>prediction_input.shape\n</pre> prediction_input.shape Out[12]: <pre>(51, 805)</pre> In\u00a0[13]: Copied! <pre>os.makedirs(os.path.join('pic', 'ctchen', 'invivo'), exist_ok=True)\n</pre> os.makedirs(os.path.join('pic', 'ctchen', 'invivo'), exist_ok=True) In\u00a0[14]: Copied! <pre>def find_best_sol(invivo_diff_spec):\n    min_RMSPE = 1000000000\n    for id in range(1):\n        print(f'now processing test_{id}...')\n        for blc in bloodConc:\n        # for blc in [138]:\n            prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id\n            for i, s in enumerate(test_SO2):\n                \n                surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n                                                            f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))\n                surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n                                                            f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))\n                for wl_idx in range(len(wavelength)):\n                    T2_large_SDS2 = surrogate_result_T2['largeIJV_SDS2'].to_numpy()\n                    T2_large_SDS1 = surrogate_result_T2['largeIJV_SDS1'][wl_idx]\n                    T1_large_SDS2 = surrogate_result_T1['largeIJV_SDS2'].to_numpy()\n                    T1_large_SDS1 = surrogate_result_T1['largeIJV_SDS1'][wl_idx]\n\n                    T2_small_SDS2 = surrogate_result_T2['smallIJV_SDS2'].to_numpy()\n                    T2_small_SDS1 = surrogate_result_T2['smallIJV_SDS1'][wl_idx]\n                    T1_small_SDS2 = surrogate_result_T1['smallIJV_SDS2'].to_numpy()\n                    T1_small_SDS1 = surrogate_result_T1['smallIJV_SDS1'][wl_idx]\n\n                    prediction_input[i][wl_idx*20 : wl_idx*20+20] = T2_large_SDS1 /T2_large_SDS2 - T1_large_SDS1 /T1_large_SDS2\n                    prediction_input[i][400+wl_idx*20 : 400+wl_idx*20+20] = T2_small_SDS1 / T2_small_SDS2  - T1_small_SDS1 / T1_small_SDS2\n                    \n            rmspe = np.sqrt(np.mean(np.square((prediction_input[:, :800] - invivo_diff_spec)/invivo_diff_spec), axis=1))\n            rmspe_second_small = rmspe[np.argsort(rmspe)[1]]\n            if rmspe_second_small &lt; min_RMSPE:\n                min_RMSPE = rmspe.min()\n                used_rmspe = rmspe_second_small\n                closest_idx = np.argsort(rmspe)[1]\n                now_ijv_SO2 = (test_SO2[closest_idx] - 0.7)*100\n                sim_spec = prediction_input[closest_idx][:800]\n                invivo_spec = invivo_diff_spec\n    \n    return used_rmspe, closest_idx, now_ijv_SO2, sim_spec, invivo_spec\n</pre> def find_best_sol(invivo_diff_spec):     min_RMSPE = 1000000000     for id in range(1):         print(f'now processing test_{id}...')         for blc in bloodConc:         # for blc in [138]:             prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id             for i, s in enumerate(test_SO2):                                  surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test',                                                              f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))                 surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test',                                                              f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))                 for wl_idx in range(len(wavelength)):                     T2_large_SDS2 = surrogate_result_T2['largeIJV_SDS2'].to_numpy()                     T2_large_SDS1 = surrogate_result_T2['largeIJV_SDS1'][wl_idx]                     T1_large_SDS2 = surrogate_result_T1['largeIJV_SDS2'].to_numpy()                     T1_large_SDS1 = surrogate_result_T1['largeIJV_SDS1'][wl_idx]                      T2_small_SDS2 = surrogate_result_T2['smallIJV_SDS2'].to_numpy()                     T2_small_SDS1 = surrogate_result_T2['smallIJV_SDS1'][wl_idx]                     T1_small_SDS2 = surrogate_result_T1['smallIJV_SDS2'].to_numpy()                     T1_small_SDS1 = surrogate_result_T1['smallIJV_SDS1'][wl_idx]                      prediction_input[i][wl_idx*20 : wl_idx*20+20] = T2_large_SDS1 /T2_large_SDS2 - T1_large_SDS1 /T1_large_SDS2                     prediction_input[i][400+wl_idx*20 : 400+wl_idx*20+20] = T2_small_SDS1 / T2_small_SDS2  - T1_small_SDS1 / T1_small_SDS2                                  rmspe = np.sqrt(np.mean(np.square((prediction_input[:, :800] - invivo_diff_spec)/invivo_diff_spec), axis=1))             rmspe_second_small = rmspe[np.argsort(rmspe)[1]]             if rmspe_second_small &lt; min_RMSPE:                 min_RMSPE = rmspe.min()                 used_rmspe = rmspe_second_small                 closest_idx = np.argsort(rmspe)[1]                 now_ijv_SO2 = (test_SO2[closest_idx] - 0.7)*100                 sim_spec = prediction_input[closest_idx][:800]                 invivo_spec = invivo_diff_spec          return used_rmspe, closest_idx, now_ijv_SO2, sim_spec, invivo_spec In\u00a0[15]: Copied! <pre>invivo_spec\n</pre> invivo_spec Out[15]: <pre>array([-4.34866232e-03,  4.44104607e-03, -3.71014418e-03, -4.07704647e-03,\n        2.40704735e-04,  2.59785130e-04, -1.97301559e-03, -2.58951565e-03,\n        1.17146121e-03, -1.73363479e-03, -2.75308140e-03,  3.06206924e-03,\n       -1.90739940e-03,  2.45120717e-03, -7.37323720e-04,  1.36217378e-03,\n       -2.79570509e-03,  4.29579797e-04,  3.65923785e-03,  1.67253261e-03,\n       -2.46066213e-03,  6.32904626e-03, -1.82214399e-03, -2.18904628e-03,\n        2.12870492e-03,  2.14778532e-03, -8.50153992e-05, -7.01515465e-04,\n        3.05946140e-03,  1.54365399e-04, -8.65081211e-04,  4.95006943e-03,\n       -1.93992131e-05,  4.33920736e-03,  1.15067647e-03,  3.25017397e-03,\n       -9.07704906e-04,  2.31757999e-03,  5.54723804e-03,  3.56053280e-03,\n       -2.02245674e-03,  6.76725165e-03, -1.38393860e-03, -1.75084090e-03,\n        2.56691031e-03,  2.58599070e-03,  3.53189985e-04, -2.63310081e-04,\n        3.49766679e-03,  5.92570783e-04, -4.26875827e-04,  5.38827482e-03,\n        4.18806171e-04,  4.77741274e-03,  1.58888185e-03,  3.68837935e-03,\n       -4.69499522e-04,  2.75578537e-03,  5.98544342e-03,  3.99873818e-03,\n       -1.18393309e-03,  7.60577530e-03, -5.45414948e-04, -9.12317240e-04,\n        3.40543396e-03,  3.42451436e-03,  1.19171364e-03,  5.75213576e-04,\n        4.33619044e-03,  1.43109444e-03,  4.11647830e-04,  6.22679847e-03,\n        1.25732983e-03,  5.61593640e-03,  2.42740551e-03,  4.52690301e-03,\n        3.69024135e-04,  3.59430903e-03,  6.82396708e-03,  4.83726184e-03,\n       -1.99341403e-03,  6.79629436e-03, -1.35489589e-03, -1.72179818e-03,\n        2.59595302e-03,  2.61503342e-03,  3.82232698e-04, -2.34267368e-04,\n        3.52670950e-03,  6.21613496e-04, -3.97833114e-04,  5.41731753e-03,\n        4.47848884e-04,  4.80645546e-03,  1.61792457e-03,  3.71742207e-03,\n       -4.40456809e-04,  2.78482808e-03,  6.01448613e-03,  4.02778089e-03,\n       -2.50218071e-03,  6.28752768e-03, -1.86366257e-03, -2.23056487e-03,\n        2.08718634e-03,  2.10626673e-03, -1.26533984e-04, -7.43034049e-04,\n        3.01794282e-03,  1.12846815e-04, -9.06599796e-04,  4.90855085e-03,\n       -6.09177977e-05,  4.29768877e-03,  1.10915788e-03,  3.20865538e-03,\n       -9.49223490e-04,  2.27606140e-03,  5.50571945e-03,  3.51901421e-03,\n       -2.40002126e-03,  6.38968713e-03, -1.76150312e-03, -2.12840541e-03,\n        2.18934579e-03,  2.20842618e-03, -2.43745329e-05, -6.40874598e-04,\n        3.12010227e-03,  2.15006266e-04, -8.04440345e-04,  5.01071030e-03,\n        4.12416533e-05,  4.39984822e-03,  1.21131734e-03,  3.31081484e-03,\n       -8.47064039e-04,  2.37822085e-03,  5.60787890e-03,  3.62117366e-03,\n       -3.26457225e-03,  5.52513614e-03, -2.62605411e-03, -2.99295640e-03,\n        1.32479480e-03,  1.34387520e-03, -8.88925519e-04, -1.50542558e-03,\n        2.25555128e-03, -6.49544720e-04, -1.66899133e-03,  4.14615931e-03,\n       -8.23309333e-04,  3.53529724e-03,  3.46766349e-04,  2.44626385e-03,\n       -1.71161503e-03,  1.51366987e-03,  4.74332792e-03,  2.75662268e-03,\n       -3.07209410e-03,  5.71761429e-03, -2.43357596e-03, -2.80047825e-03,\n        1.51727295e-03,  1.53635335e-03, -6.96447370e-04, -1.31294744e-03,\n        2.44802943e-03, -4.57066572e-04, -1.47651318e-03,  4.33863746e-03,\n       -6.30831184e-04,  3.72777539e-03,  5.39244498e-04,  2.63874200e-03,\n       -1.51913688e-03,  1.70614801e-03,  4.93580606e-03,  2.94910083e-03,\n       -3.26271858e-03,  5.52698981e-03, -2.62420044e-03, -2.99110273e-03,\n        1.32664847e-03,  1.34572887e-03, -8.87071852e-04, -1.50357192e-03,\n        2.25740495e-03, -6.47691053e-04, -1.66713766e-03,  4.14801298e-03,\n       -8.21455666e-04,  3.53715091e-03,  3.48620016e-04,  2.44811752e-03,\n       -1.70976136e-03,  1.51552353e-03,  4.74518158e-03,  2.75847634e-03,\n       -2.91958763e-03,  5.87012076e-03, -2.28106949e-03, -2.64797178e-03,\n        1.66977942e-03,  1.68885982e-03, -5.43940899e-04, -1.16044096e-03,\n        2.60053590e-03, -3.04560101e-04, -1.32400671e-03,  4.49114393e-03,\n       -4.78324713e-04,  3.88028186e-03,  6.91750969e-04,  2.79124847e-03,\n       -1.36663041e-03,  1.85865449e-03,  5.08831254e-03,  3.10160730e-03,\n       -3.70214319e-03,  5.08756520e-03, -3.06362505e-03, -3.43052734e-03,\n        8.87223866e-04,  9.06304261e-04, -1.32649646e-03, -1.94299652e-03,\n        1.81798034e-03, -1.08711566e-03, -2.10656227e-03,  3.70858837e-03,\n       -1.26088027e-03,  3.09772630e-03, -9.08045888e-05,  2.00869291e-03,\n       -2.14918596e-03,  1.07609893e-03,  4.30575698e-03,  2.31905174e-03,\n       -4.20634415e-03,  4.58336424e-03, -3.56782601e-03, -3.93472830e-03,\n        3.83022906e-04,  4.02103300e-04, -1.83069742e-03, -2.44719748e-03,\n        1.31377938e-03, -1.59131662e-03, -2.61076323e-03,  3.20438741e-03,\n       -1.76508123e-03,  2.59352534e-03, -5.95005549e-04,  1.50449195e-03,\n       -2.65338692e-03,  5.71897968e-04,  3.80155602e-03,  1.81485078e-03,\n       -4.28705190e-03,  4.50265649e-03, -3.64853376e-03, -4.01543606e-03,\n        3.02315148e-04,  3.21395543e-04, -1.91140517e-03, -2.52790524e-03,\n        1.23307163e-03, -1.67202438e-03, -2.69147099e-03,  3.12367966e-03,\n       -1.84578899e-03,  2.51281758e-03, -6.75713307e-04,  1.42378419e-03,\n       -2.73409468e-03,  4.91190210e-04,  3.72084826e-03,  1.73414302e-03,\n       -3.36048305e-03,  5.42922534e-03, -2.72196491e-03, -3.08886720e-03,\n        1.22888400e-03,  1.24796440e-03, -9.84836322e-04, -1.60133639e-03,\n        2.15964048e-03, -7.45455523e-04, -1.76490213e-03,  4.05024851e-03,\n       -9.19220135e-04,  3.43938644e-03,  2.50855547e-04,  2.35035305e-03,\n       -1.80752583e-03,  1.41775906e-03,  4.64741711e-03,  2.66071188e-03,\n       -4.35554544e-03,  4.43416295e-03, -3.71702730e-03, -4.08392959e-03,\n        2.33821612e-04,  2.52902007e-04, -1.97989871e-03, -2.59639878e-03,\n        1.16457809e-03, -1.74051791e-03, -2.75996452e-03,  3.05518612e-03,\n       -1.91428252e-03,  2.44432405e-03, -7.44206843e-04,  1.35529066e-03,\n       -2.80258822e-03,  4.22696674e-04,  3.65235472e-03,  1.66564949e-03,\n       -5.73380002e-03,  3.05590837e-03, -5.09528188e-03, -5.46218417e-03,\n       -1.14443297e-03, -1.12535257e-03, -3.35815329e-03, -3.97465336e-03,\n       -2.13676489e-04, -3.11877249e-03, -4.13821910e-03,  1.67693154e-03,\n       -3.29253710e-03,  1.06606947e-03, -2.12246142e-03, -2.29639216e-05,\n       -4.18084280e-03, -9.55557905e-04,  2.27410014e-03,  2.87394907e-04,\n       -6.01879469e-03,  2.77091370e-03, -5.38027655e-03, -5.74717884e-03,\n       -1.42942764e-03, -1.41034724e-03, -3.64314796e-03, -4.25964803e-03,\n       -4.98671161e-04, -3.40376716e-03, -4.42321377e-03,  1.39193687e-03,\n       -3.57753178e-03,  7.81074795e-04, -2.40745609e-03, -3.07958594e-04,\n       -4.46583747e-03, -1.24055258e-03,  1.98910547e-03,  2.40023442e-06,\n       -4.64198401e-03,  4.14772438e-03, -4.00346587e-03, -4.37036816e-03,\n       -5.26169531e-05, -3.35365585e-05, -2.26633728e-03, -2.88283734e-03,\n        8.78139525e-04, -2.02695648e-03, -3.04640309e-03,  2.76874755e-03,\n       -2.20072109e-03,  2.15788548e-03, -1.03064541e-03,  1.06885209e-03,\n       -3.08902678e-03,  1.36258109e-04,  3.36591616e-03,  1.37921092e-03,\n       -4.34652412e-03,  4.44318427e-03, -3.70800598e-03, -4.07490827e-03,\n        2.42842931e-04,  2.61923326e-04, -1.97087739e-03, -2.58737746e-03,\n        1.17359941e-03, -1.73149659e-03, -2.75094320e-03,  3.06420744e-03,\n       -1.90526121e-03,  2.45334537e-03, -7.35185524e-04,  1.36431198e-03,\n       -2.79356690e-03,  4.31717993e-04,  3.66137604e-03,  1.67467080e-03,\n       -2.85654121e-03,  6.03985868e-03,  3.17827249e-05,  2.36331217e-03,\n        1.32256512e-03,  2.97415804e-03,  1.83038309e-03,  1.90622865e-03,\n        1.67036487e-03, -1.23197709e-03,  6.29734151e-03,  4.13714069e-03,\n        1.33449405e-03,  2.12263255e-05, -1.60613543e-03,  7.89558750e-04,\n       -3.65257352e-04, -1.53186226e-03,  3.31795581e-03,  1.80975550e-03,\n       -7.90116414e-04,  8.10628348e-03,  2.09820752e-03,  4.42973696e-03,\n        3.38898991e-03,  5.04058283e-03,  3.89680788e-03,  3.97265344e-03,\n        3.73678966e-03,  8.34447708e-04,  8.36376630e-03,  6.20356548e-03,\n        3.40091884e-03,  2.08765112e-03,  4.60289366e-04,  2.85598354e-03,\n        1.70116744e-03,  5.34562536e-04,  5.38438061e-03,  3.87618030e-03,\n       -1.10977169e-03,  7.78662821e-03,  1.77855225e-03,  4.11008169e-03,\n        3.06933464e-03,  4.72092756e-03,  3.57715261e-03,  3.65299817e-03,\n        3.41713439e-03,  5.14792437e-04,  8.04411103e-03,  5.88391021e-03,\n        3.08126357e-03,  1.76799585e-03,  1.40634095e-04,  2.53632827e-03,\n        1.38151217e-03,  2.14907264e-04,  5.06472533e-03,  3.55652503e-03,\n       -1.71518356e-03,  7.18121634e-03,  1.17314038e-03,  3.50466982e-03,\n        2.46392277e-03,  4.11551569e-03,  2.97174074e-03,  3.04758630e-03,\n        2.81172252e-03, -9.06194332e-05,  7.43869916e-03,  5.27849834e-03,\n        2.47585170e-03,  1.16258398e-03, -4.64777775e-04,  1.93091640e-03,\n        7.76100300e-04, -3.90504606e-04,  4.45931346e-03,  2.95111316e-03,\n       -5.47233235e-04,  8.34916666e-03,  2.34109070e-03,  4.67262014e-03,\n        3.63187309e-03,  5.28346601e-03,  4.13969106e-03,  4.21553662e-03,\n        3.97967284e-03,  1.07733089e-03,  8.60664948e-03,  6.44644866e-03,\n        3.64380202e-03,  2.33053430e-03,  7.03172545e-04,  3.09886672e-03,\n        1.94405062e-03,  7.77445714e-04,  5.62726378e-03,  4.11906348e-03,\n       -1.73306408e-03,  7.16333582e-03,  1.15525986e-03,  3.48678930e-03,\n        2.44604225e-03,  4.09763517e-03,  2.95386022e-03,  3.02970578e-03,\n        2.79384200e-03, -1.08499953e-04,  7.42081864e-03,  5.26061782e-03,\n        2.45797118e-03,  1.14470346e-03, -4.82658295e-04,  1.91303588e-03,\n        7.58219780e-04, -4.08385126e-04,  4.44143294e-03,  2.93323264e-03,\n       -1.34592565e-03,  7.55047424e-03,  1.54239828e-03,  3.87392773e-03,\n        2.83318067e-03,  4.48477359e-03,  3.34099864e-03,  3.41684421e-03,\n        3.18098043e-03,  2.78638472e-04,  7.80795707e-03,  5.64775625e-03,\n        2.84510961e-03,  1.53184188e-03, -9.55198702e-05,  2.30017431e-03,\n        1.14535820e-03, -2.12467010e-05,  4.82857137e-03,  3.32037106e-03,\n       -1.21000765e-03,  7.68639225e-03,  1.67831629e-03,  4.00984573e-03,\n        2.96909868e-03,  4.62069160e-03,  3.47691665e-03,  3.55276221e-03,\n        3.31689843e-03,  4.14556476e-04,  7.94387507e-03,  5.78367425e-03,\n        2.98102761e-03,  1.66775989e-03,  4.03981339e-05,  2.43609231e-03,\n        1.28127621e-03,  1.14671303e-04,  4.96448937e-03,  3.45628907e-03,\n       -1.82440819e-03,  7.07199171e-03,  1.06391575e-03,  3.39544519e-03,\n        2.35469814e-03,  4.00629106e-03,  2.86251611e-03,  2.93836167e-03,\n        2.70249789e-03, -1.99844064e-04,  7.32947453e-03,  5.16927371e-03,\n        2.36662707e-03,  1.05335935e-03, -5.74002406e-04,  1.82169177e-03,\n        6.66875669e-04, -4.99729237e-04,  4.35008883e-03,  2.84188852e-03,\n       -2.59397362e-03,  6.30242628e-03,  2.94350315e-04,  2.62587976e-03,\n        1.58513271e-03,  3.23672563e-03,  2.09295068e-03,  2.16879624e-03,\n        1.93293246e-03, -9.69409495e-04,  6.55990910e-03,  4.39970828e-03,\n        1.59706164e-03,  2.83793916e-04, -1.34356784e-03,  1.05212634e-03,\n       -1.02689762e-04, -1.26929467e-03,  3.58052340e-03,  2.07232309e-03,\n       -2.66333282e-03,  6.23306708e-03,  2.24991116e-04,  2.55652056e-03,\n        1.51577351e-03,  3.16736643e-03,  2.02359148e-03,  2.09943704e-03,\n        1.86357326e-03, -1.03876869e-03,  6.49054990e-03,  4.33034908e-03,\n        1.52770244e-03,  2.14434717e-04, -1.41292704e-03,  9.82767141e-04,\n       -1.72048961e-04, -1.33865387e-03,  3.51116420e-03,  2.00296390e-03,\n       -1.14511217e-03,  7.75128772e-03,  1.74321176e-03,  4.07474121e-03,\n        3.03399415e-03,  4.68558707e-03,  3.54181212e-03,  3.61765768e-03,\n        3.38179390e-03,  4.79451949e-04,  8.00877054e-03,  5.84856972e-03,\n        3.04592308e-03,  1.73265536e-03,  1.05293607e-04,  2.50098778e-03,\n        1.34617168e-03,  1.79566777e-04,  5.02938485e-03,  3.52118454e-03,\n       -1.82455589e-03,  7.07184400e-03,  1.06376804e-03,  3.39529749e-03,\n        2.35455043e-03,  4.00614335e-03,  2.86236840e-03,  2.93821397e-03,\n        2.70235019e-03, -1.99991767e-04,  7.32932683e-03,  5.16912601e-03,\n        2.36647937e-03,  1.05321164e-03, -5.74150109e-04,  1.82154407e-03,\n        6.66727966e-04, -4.99876940e-04,  4.34994113e-03,  2.84174082e-03,\n       -3.23011902e-03,  5.66628088e-03, -3.41795084e-04,  1.98973436e-03,\n        9.48987307e-04,  2.60058023e-03,  1.45680528e-03,  1.53265084e-03,\n        1.29678706e-03, -1.60555489e-03,  5.92376370e-03,  3.76356288e-03,\n        9.60916240e-04, -3.52351484e-04, -1.97971324e-03,  4.15980941e-04,\n       -7.38835161e-04, -1.90544007e-03,  2.94437800e-03,  1.43617770e-03,\n       -3.72321188e-03,  5.17318801e-03, -8.34887950e-04,  1.49664150e-03,\n        4.55894441e-04,  2.10748736e-03,  9.63712410e-04,  1.03955797e-03,\n        8.03694195e-04, -2.09864776e-03,  5.43067083e-03,  3.27047002e-03,\n        4.67823374e-04, -8.45444350e-04, -2.47280610e-03, -7.71119253e-05,\n       -1.23192803e-03, -2.39853293e-03,  2.45128514e-03,  9.43084829e-04,\n       -2.99357266e-03,  5.90282723e-03, -1.05248729e-04,  2.22628072e-03,\n        1.18553366e-03,  2.83712658e-03,  1.69335163e-03,  1.76919720e-03,\n        1.53333342e-03, -1.36900854e-03,  6.16031006e-03,  4.00010924e-03,\n        1.19746260e-03, -1.15805128e-04, -1.74316688e-03,  6.52527296e-04,\n       -5.02288806e-04, -1.66889371e-03,  3.18092436e-03,  1.67272405e-03,\n       -2.72958176e-03,  6.16681813e-03,  1.58742174e-04,  2.49027162e-03,\n        1.44952457e-03,  3.10111749e-03,  1.95734253e-03,  2.03318810e-03,\n        1.79732432e-03, -1.10501764e-03,  6.42430096e-03,  4.26410014e-03,\n        1.46145350e-03,  1.48185775e-04, -1.47917598e-03,  9.16518199e-04,\n       -2.38297903e-04, -1.40490281e-03,  3.44491526e-03,  1.93671495e-03,\n       -4.28015498e-03,  4.61624491e-03, -1.39183105e-03,  9.39698397e-04,\n       -1.01048658e-04,  1.55054426e-03,  4.06769311e-04,  4.82614875e-04,\n        2.46751096e-04, -2.65559086e-03,  4.87372774e-03,  2.71352692e-03,\n       -8.91197248e-05, -1.40238745e-03, -3.02974920e-03, -6.34055024e-04,\n       -1.78887113e-03, -2.95547603e-03,  1.89434204e-03,  3.86141730e-04,\n       -4.24135065e-03,  4.65504924e-03, -1.35302672e-03,  9.78502727e-04,\n       -6.22443281e-05,  1.58934859e-03,  4.45573641e-04,  5.21419205e-04,\n        2.85555425e-04, -2.61678653e-03,  4.91253207e-03,  2.75233125e-03,\n       -5.03153951e-05, -1.36358312e-03, -2.99094487e-03, -5.95250695e-04,\n       -1.75006680e-03, -2.91667170e-03,  1.93314637e-03,  4.24946060e-04,\n       -3.78239745e-03,  5.11400245e-03, -8.94073514e-04,  1.43745593e-03,\n        3.96708878e-04,  2.04830180e-03,  9.04526846e-04,  9.80372410e-04,\n        7.44508631e-04, -2.15783332e-03,  5.37148527e-03,  3.21128445e-03,\n        4.08637811e-04, -9.04629913e-04, -2.53199167e-03, -1.36297489e-04,\n       -1.29111359e-03, -2.45771850e-03,  2.39209957e-03,  8.83899266e-04])</pre> In\u00a0[16]: Copied! <pre>for invivo_idx in range(result_diff_spec.shape[0]):\n    # rmspe = np.sqrt(np.mean(np.square((prediction_input[:, :800] - result_diff_spec[invivo_idx])/result_diff_spec[invivo_idx]), axis=1))\n    # closest_idx = np.argsort(rmspe)[1]\n    # now_ijv_SO2 = (test_SO2[closest_idx] - 0.7)*100\n\n    # sim_spec = prediction_input[closest_idx][:800]\n    # invivo_spec = result_diff_spec[invivo_idx]\n    \n    used_rmspe, closest_idx, now_ijv_SO2, sim_spec, invivo_spec = find_best_sol(invivo_diff_spec=result_diff_spec[invivo_idx])\n    fig, ax = plt.subplots(5,4,figsize=(16,12))\n    fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.2f}%, RMSPE : {used_rmspe:.2f}%', fontsize=20)\n    sim_ijv_large_spec = []\n    sim_ijv_small_spec = []\n    ijv_large_spec = []\n    ijv_small_spec = []\n    for i in range(20):\n        ijv_large_spec = invivo_spec[i*20:i*20+20]\n        ijv_small_spec = invivo_spec[i*20+400:i*20+20+400]\n        sim_ijv_large_spec = sim_spec[i*20:i*20+20]\n        sim_ijv_small_spec = sim_spec[i*20+400:i*20+20+400]\n        \n        ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue', label=r'$IJV_{large}$ sim')\n        ax[i//4][i%4].plot(wavelength, sim_ijv_small_spec, linestyle='--', color = 'orange', label=r'$IJV_{small}$ sim')\n        ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'blue', label=r'$IJV_{large}$ in vivo')\n        ax[i//4][i%4].plot(wavelength, ijv_small_spec, color = 'orange', label=r'$IJV_{small}$ in vivo')\n        ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")\n        ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")\n        ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')\n        # ax[i//4][i%4].legend()\n    fig.tight_layout()\n    plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                                fancybox=True, shadow=True)\n    plt.savefig(os.path.join(\"pic\", 'ctchen', 'invivo', f\"invivo_{invivo_idx+1}.png\"), dpi=300, format='png', bbox_inches='tight')\n    \n    # plt.show()\n    plt.close()\n</pre> for invivo_idx in range(result_diff_spec.shape[0]):     # rmspe = np.sqrt(np.mean(np.square((prediction_input[:, :800] - result_diff_spec[invivo_idx])/result_diff_spec[invivo_idx]), axis=1))     # closest_idx = np.argsort(rmspe)[1]     # now_ijv_SO2 = (test_SO2[closest_idx] - 0.7)*100      # sim_spec = prediction_input[closest_idx][:800]     # invivo_spec = result_diff_spec[invivo_idx]          used_rmspe, closest_idx, now_ijv_SO2, sim_spec, invivo_spec = find_best_sol(invivo_diff_spec=result_diff_spec[invivo_idx])     fig, ax = plt.subplots(5,4,figsize=(16,12))     fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.2f}%, RMSPE : {used_rmspe:.2f}%', fontsize=20)     sim_ijv_large_spec = []     sim_ijv_small_spec = []     ijv_large_spec = []     ijv_small_spec = []     for i in range(20):         ijv_large_spec = invivo_spec[i*20:i*20+20]         ijv_small_spec = invivo_spec[i*20+400:i*20+20+400]         sim_ijv_large_spec = sim_spec[i*20:i*20+20]         sim_ijv_small_spec = sim_spec[i*20+400:i*20+20+400]                  ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue', label=r'$IJV_{large}$ sim')         ax[i//4][i%4].plot(wavelength, sim_ijv_small_spec, linestyle='--', color = 'orange', label=r'$IJV_{small}$ sim')         ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'blue', label=r'$IJV_{large}$ in vivo')         ax[i//4][i%4].plot(wavelength, ijv_small_spec, color = 'orange', label=r'$IJV_{small}$ in vivo')         ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")         ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")         ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')         # ax[i//4][i%4].legend()     fig.tight_layout()     plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                                 fancybox=True, shadow=True)     plt.savefig(os.path.join(\"pic\", 'ctchen', 'invivo', f\"invivo_{invivo_idx+1}.png\"), dpi=300, format='png', bbox_inches='tight')          # plt.show()     plt.close() <pre>now processing test_0...\nnow processing test_0...\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[16], line 9\n      1 for invivo_idx in range(result_diff_spec.shape[0]):\n      2     # rmspe = np.sqrt(np.mean(np.square((prediction_input[:, :800] - result_diff_spec[invivo_idx])/result_diff_spec[invivo_idx]), axis=1))\n      3     # closest_idx = np.argsort(rmspe)[1]\n   (...)\n      6     # sim_spec = prediction_input[closest_idx][:800]\n      7     # invivo_spec = result_diff_spec[invivo_idx]\n----&gt; 9     used_rmspe, closest_idx, now_ijv_SO2, sim_spec, invivo_spec = find_best_sol(invivo_diff_spec=result_diff_spec[invivo_idx])\n     10     fig, ax = plt.subplots(5,4,figsize=(16,12))\n     11     fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.2f}%, RMSPE : {used_rmspe:.2f}%', fontsize=20)\n\nCell In[14], line 10, in find_best_sol(invivo_diff_spec)\n      7 prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id\n      8 for i, s in enumerate(test_SO2):\n---&gt; 10     surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n     11                                                 f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))\n     12     surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n     13                                                 f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))\n     14     for wl_idx in range(len(wavelength)):\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:912, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\n    899 kwds_defaults = _refine_defaults_read(\n    900     dialect,\n    901     delimiter,\n   (...)\n    908     dtype_backend=dtype_backend,\n    909 )\n    910 kwds.update(kwds_defaults)\n--&gt; 912 return _read(filepath_or_buffer, kwds)\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:583, in _read(filepath_or_buffer, kwds)\n    580     return parser\n    582 with parser:\n--&gt; 583     return parser.read(nrows)\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1721, in TextFileReader.read(self, nrows)\n   1718     else:\n   1719         new_rows = len(index)\n-&gt; 1721     df = DataFrame(col_dict, columns=columns, index=index)\n   1723     self._currow += new_rows\n   1724 return df\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\core\\frame.py:708, in DataFrame.__init__(self, data, index, columns, dtype, copy)\n    702     mgr = self._init_mgr(\n    703         data, axes={\"index\": index, \"columns\": columns}, dtype=dtype, copy=copy\n    704     )\n    706 elif isinstance(data, dict):\n    707     # GH#38939 de facto copy defaults to False only in non-dict cases\n--&gt; 708     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n    709 elif isinstance(data, ma.MaskedArray):\n    710     from numpy.ma import mrecords\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:481, in dict_to_mgr(data, index, columns, dtype, typ, copy)\n    477     else:\n    478         # dtype check to exclude e.g. range objects, scalars\n    479         arrays = [x.copy() if hasattr(x, \"dtype\") else x for x in arrays]\n--&gt; 481 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\core\\internals\\construction.py:153, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\n    150 axes = [columns, index]\n    152 if typ == \"block\":\n--&gt; 153     return create_block_manager_from_column_arrays(\n    154         arrays, axes, consolidate=consolidate, refs=refs\n    155     )\n    156 elif typ == \"array\":\n    157     return ArrayManager(arrays, [index, columns])\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2137, in create_block_manager_from_column_arrays(arrays, axes, consolidate, refs)\n   2119 def create_block_manager_from_column_arrays(\n   2120     arrays: list[ArrayLike],\n   2121     axes: list[Index],\n   (...)\n   2133     # These last three are sufficient to allow us to safely pass\n   2134     #  verify_integrity=False below.\n   2136     try:\n-&gt; 2137         blocks = _form_blocks(arrays, consolidate, refs)\n   2138         mgr = BlockManager(blocks, axes, verify_integrity=False)\n   2139     except ValueError as e:\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2215, in _form_blocks(arrays, consolidate, refs)\n   2212 if issubclass(dtype.type, (str, bytes)):\n   2213     dtype = np.dtype(object)\n-&gt; 2215 values, placement = _stack_arrays(list(tup_block), dtype)\n   2216 if is_dtlike:\n   2217     values = ensure_wrapped_if_datetimelike(values)\n\nFile d:\\ijv_code_organized\\venv\\lib\\site-packages\\pandas\\core\\internals\\managers.py:2257, in _stack_arrays(tuples, dtype)\n   2255 stacked = np.empty(shape, dtype=dtype)\n   2256 for i, arr in enumerate(arrays):\n-&gt; 2257     stacked[i] = arr\n   2259 return stacked, placement\n\nKeyboardInterrupt: </pre> In\u00a0[5]: Copied! <pre>for id in range(1):\n    id = 150\n    print(f'now processing test_{id}...')\n    # for blc in bloodConc:\n    for blc in [138]:\n        prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id\n        for i, s in enumerate(test_SO2):\n            \n            surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n                                                        f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))\n            surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test', \n                                                        f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))\n            for wl_idx in range(len(wavelength)):\n                T2_large_SDS2 = surrogate_result_T2['largeIJV_SDS2'].to_numpy()\n                T2_large_SDS1 = surrogate_result_T2['largeIJV_SDS1'][wl_idx]\n                T1_large_SDS2 = surrogate_result_T1['largeIJV_SDS2'].to_numpy()\n                T1_large_SDS1 = surrogate_result_T1['largeIJV_SDS1'][wl_idx]\n\n                T2_small_SDS2 = surrogate_result_T2['smallIJV_SDS2'].to_numpy()\n                T2_small_SDS1 = surrogate_result_T2['smallIJV_SDS1'][wl_idx]\n                T1_small_SDS2 = surrogate_result_T1['smallIJV_SDS2'].to_numpy()\n                T1_small_SDS1 = surrogate_result_T1['smallIJV_SDS1'][wl_idx]\n\n                prediction_input[i][wl_idx*20 : wl_idx*20+20] = T2_large_SDS1 /T2_large_SDS2 - T1_large_SDS1 /T1_large_SDS2\n                prediction_input[i][400+wl_idx*20 : 400+wl_idx*20+20] = T2_small_SDS1 / T2_small_SDS2  - T1_small_SDS1 / T1_small_SDS2\n</pre> for id in range(1):     id = 150     print(f'now processing test_{id}...')     # for blc in bloodConc:     for blc in [138]:         prediction_input = np.empty((len(test_SO2),2*len(wavelength)*len(wavelength)+5)) # T1_large_SDS1/SDS2 T1_small_SDS1/SDS2 T2_large_SDS1/SDS2 T2_small_SDS1/SDS2 bloodConc ans id         for i, s in enumerate(test_SO2):                          surrogate_result_T1 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test',                                                          f'bloodConc_{blc}', 'SO2_0.7', f'{id}_test.csv'))             surrogate_result_T2 = pd.read_csv(os.path.join(\"dataset\", \"surrogate_result\", 'test',                                                          f'bloodConc_{blc}', f'SO2_{s}', f'{id}_test.csv'))             for wl_idx in range(len(wavelength)):                 T2_large_SDS2 = surrogate_result_T2['largeIJV_SDS2'].to_numpy()                 T2_large_SDS1 = surrogate_result_T2['largeIJV_SDS1'][wl_idx]                 T1_large_SDS2 = surrogate_result_T1['largeIJV_SDS2'].to_numpy()                 T1_large_SDS1 = surrogate_result_T1['largeIJV_SDS1'][wl_idx]                  T2_small_SDS2 = surrogate_result_T2['smallIJV_SDS2'].to_numpy()                 T2_small_SDS1 = surrogate_result_T2['smallIJV_SDS1'][wl_idx]                 T1_small_SDS2 = surrogate_result_T1['smallIJV_SDS2'].to_numpy()                 T1_small_SDS1 = surrogate_result_T1['smallIJV_SDS1'][wl_idx]                  prediction_input[i][wl_idx*20 : wl_idx*20+20] = T2_large_SDS1 /T2_large_SDS2 - T1_large_SDS1 /T1_large_SDS2                 prediction_input[i][400+wl_idx*20 : 400+wl_idx*20+20] = T2_small_SDS1 / T2_small_SDS2  - T1_small_SDS1 / T1_small_SDS2 <pre>now processing test_150...\n</pre> In\u00a0[17]: Copied! <pre>result_OD1_spec.shape\n</pre> result_OD1_spec.shape Out[17]: <pre>(84, 800)</pre> In\u00a0[18]: Copied! <pre>invivo_spec.mean()\n</pre> invivo_spec.mean() Out[18]: <pre>0.001201502604287068</pre> In\u00a0[19]: Copied! <pre>result_diff_spec = result_OD2_spec - result_OD1_spec\nresult_diff_spec.shape\n</pre> result_diff_spec = result_OD2_spec - result_OD1_spec result_diff_spec.shape Out[19]: <pre>(84, 800)</pre> In\u00a0[20]: Copied! <pre># sim_spec = prediction_input[33][:800]\ninvivo_spec = result_diff_spec[16]\nnow_ijv_SO2 = 0\nused_rmspe = 0\n# invivo_spec = (invivo_spec - invivo_spec.max())/ (invivo_spec.max()-invivo_spec.min())\nfig, ax = plt.subplots(5,4,figsize=(16,12))\nfig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.2f}%, RMSPE : {used_rmspe:.2f}%', fontsize=20)\nsim_ijv_large_spec = []\nsim_ijv_small_spec = []\nijv_large_spec = []\nijv_small_spec = []\n\nfor label_idx, sim_spec in enumerate(prediction_input[25:35]):\n    sim_spec = sim_spec[:800]\n    # sim_spec = (sim_spec - prediction_input.max())/ (prediction_input.max()-prediction_input.min())\n    for i in range(20):\n        ijv_large_spec = invivo_spec[i*20:i*20+20]\n        ijv_small_spec = invivo_spec[i*20+400:i*20+20+400]\n        sim_ijv_large_spec = sim_spec[i*20:i*20+20]\n        sim_ijv_small_spec = sim_spec[i*20+400:i*20+20+400]\n        \n        if label_idx == 0:\n            ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue', label=r'$IJV_{large}$ sim')\n            # ax[i//4][i%4].plot(wavelength, sim_ijv_small_spec, linestyle='--', color = 'orange', label=r'$IJV_{small}$ sim')\n            ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'red', label=r'$IJV_{large}$ in vivo')\n            # ax[i//4][i%4].plot(wavelength, ijv_small_spec, color = 'orange', label=r'$IJV_{small}$ in vivo')\n        else:\n            ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue')\n            ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'red')\n        ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")\n        ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")\n        ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')\n        # ax[i//4][i%4].legend()\nfig.tight_layout()\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                            fancybox=True, shadow=True)\nplt.show()\n</pre> # sim_spec = prediction_input[33][:800] invivo_spec = result_diff_spec[16] now_ijv_SO2 = 0 used_rmspe = 0 # invivo_spec = (invivo_spec - invivo_spec.max())/ (invivo_spec.max()-invivo_spec.min()) fig, ax = plt.subplots(5,4,figsize=(16,12)) fig.suptitle(r'$\\Delta$OD spectrum @ $\\Delta$SO2 :' + f'{now_ijv_SO2:.2f}%, RMSPE : {used_rmspe:.2f}%', fontsize=20) sim_ijv_large_spec = [] sim_ijv_small_spec = [] ijv_large_spec = [] ijv_small_spec = []  for label_idx, sim_spec in enumerate(prediction_input[25:35]):     sim_spec = sim_spec[:800]     # sim_spec = (sim_spec - prediction_input.max())/ (prediction_input.max()-prediction_input.min())     for i in range(20):         ijv_large_spec = invivo_spec[i*20:i*20+20]         ijv_small_spec = invivo_spec[i*20+400:i*20+20+400]         sim_ijv_large_spec = sim_spec[i*20:i*20+20]         sim_ijv_small_spec = sim_spec[i*20+400:i*20+20+400]                  if label_idx == 0:             ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue', label=r'$IJV_{large}$ sim')             # ax[i//4][i%4].plot(wavelength, sim_ijv_small_spec, linestyle='--', color = 'orange', label=r'$IJV_{small}$ sim')             ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'red', label=r'$IJV_{large}$ in vivo')             # ax[i//4][i%4].plot(wavelength, ijv_small_spec, color = 'orange', label=r'$IJV_{small}$ in vivo')         else:             ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue')             ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'red')         ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")         ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")         ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')         # ax[i//4][i%4].legend() fig.tight_layout() plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                             fancybox=True, shadow=True) plt.show() In\u00a0[21]: Copied! <pre>fig, ax = plt.subplots(5,4,figsize=(16,12))\nfig.suptitle(r'$\\Delta$OD spectrum in vivo', fontsize=20)\nsim_ijv_large_spec = []\nsim_ijv_small_spec = []\nijv_large_spec = []\nijv_small_spec = []\n# invivo_spec = result_diff_spec[15]*10\n\nbaseline_spec_set = result_OD1_spec[:5]\nHP_spec_set = result_OD1_spec[8:15]\nrecovery_spec_set = result_OD1_spec[-5:]\nfor i in range(20):\n    # ijv_large_spec = invivo_spec[i*20:i*20+20]\n    # ijv_small_spec = invivo_spec[i*20+400:i*20+20+400]\n    # sim_ijv_large_spec = sim_spec[i*20:i*20+20]\n    # sim_ijv_small_spec = sim_spec[i*20+400:i*20+20+400]\n    for b_idx, baseline_spec in enumerate(baseline_spec_set):\n        if (i==19) &amp; (b_idx==0):\n            ax[i//4][i%4].plot(wavelength, baseline_spec[i*20:i*20+20], linestyle='--', color = 'b', label=r'$IJV_{large}$ baseline sim')\n        else:\n            ax[i//4][i%4].plot(wavelength, baseline_spec[i*20:i*20+20], linestyle='--', color = 'b')\n    for HP_idx, HP_spec in enumerate(HP_spec_set):\n        if (i==19) &amp; (HP_idx==0):\n            ax[i//4][i%4].plot(wavelength, HP_spec[i*20:i*20+20], linestyle='--', color = 'r', label=r'$IJV_{large}$ HP sim')\n        else:\n            ax[i//4][i%4].plot(wavelength, HP_spec[i*20:i*20+20], linestyle='--', color = 'r')\n    for r_idx, recovery_spec in enumerate(recovery_spec_set):\n        if (i==19) &amp; (r_idx==0):\n            ax[i//4][i%4].plot(wavelength, recovery_spec[i*20:i*20+20], linestyle='--', color = 'g', label=r'$IJV_{large}$ recovery sim')\n        else:\n            ax[i//4][i%4].plot(wavelength, recovery_spec[i*20:i*20+20], linestyle='--', color = 'g')\n    \n    # ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue', label=r'$IJV_{large}$ sim')\n    # ax[i//4][i%4].plot(wavelength, sim_ijv_small_spec, linestyle='--', color = 'orange', label=r'$IJV_{small}$ sim')\n    # ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'blue', label=r'$IJV_{large}$ in vivo')\n    # ax[i//4][i%4].plot(wavelength, ijv_small_spec, color = 'orange', label=r'$IJV_{small}$ in vivo')\n    ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")\n    ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")\n    ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')\n    # ax[i//4][i%4].legend()\nfig.tight_layout()\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n                            fancybox=True, shadow=True)\nplt.show()\n</pre> fig, ax = plt.subplots(5,4,figsize=(16,12)) fig.suptitle(r'$\\Delta$OD spectrum in vivo', fontsize=20) sim_ijv_large_spec = [] sim_ijv_small_spec = [] ijv_large_spec = [] ijv_small_spec = [] # invivo_spec = result_diff_spec[15]*10  baseline_spec_set = result_OD1_spec[:5] HP_spec_set = result_OD1_spec[8:15] recovery_spec_set = result_OD1_spec[-5:] for i in range(20):     # ijv_large_spec = invivo_spec[i*20:i*20+20]     # ijv_small_spec = invivo_spec[i*20+400:i*20+20+400]     # sim_ijv_large_spec = sim_spec[i*20:i*20+20]     # sim_ijv_small_spec = sim_spec[i*20+400:i*20+20+400]     for b_idx, baseline_spec in enumerate(baseline_spec_set):         if (i==19) &amp; (b_idx==0):             ax[i//4][i%4].plot(wavelength, baseline_spec[i*20:i*20+20], linestyle='--', color = 'b', label=r'$IJV_{large}$ baseline sim')         else:             ax[i//4][i%4].plot(wavelength, baseline_spec[i*20:i*20+20], linestyle='--', color = 'b')     for HP_idx, HP_spec in enumerate(HP_spec_set):         if (i==19) &amp; (HP_idx==0):             ax[i//4][i%4].plot(wavelength, HP_spec[i*20:i*20+20], linestyle='--', color = 'r', label=r'$IJV_{large}$ HP sim')         else:             ax[i//4][i%4].plot(wavelength, HP_spec[i*20:i*20+20], linestyle='--', color = 'r')     for r_idx, recovery_spec in enumerate(recovery_spec_set):         if (i==19) &amp; (r_idx==0):             ax[i//4][i%4].plot(wavelength, recovery_spec[i*20:i*20+20], linestyle='--', color = 'g', label=r'$IJV_{large}$ recovery sim')         else:             ax[i//4][i%4].plot(wavelength, recovery_spec[i*20:i*20+20], linestyle='--', color = 'g')          # ax[i//4][i%4].plot(wavelength, sim_ijv_large_spec, linestyle='--', color = 'blue', label=r'$IJV_{large}$ sim')     # ax[i//4][i%4].plot(wavelength, sim_ijv_small_spec, linestyle='--', color = 'orange', label=r'$IJV_{small}$ sim')     # ax[i//4][i%4].plot(wavelength, ijv_large_spec, color = 'blue', label=r'$IJV_{large}$ in vivo')     # ax[i//4][i%4].plot(wavelength, ijv_small_spec, color = 'orange', label=r'$IJV_{small}$ in vivo')     ax[i//4][i%4].set_xlabel(\"wavelength(nm)\")     ax[i//4][i%4].set_ylabel(f\"$\\Delta$OD\")     ax[i//4][i%4].title.set_text(f'based on {wavelength[i]} nm')     # ax[i//4][i%4].legend() fig.tight_layout() plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),                             fancybox=True, shadow=True) plt.show()"},{"location":"prediction_model/plot_spectrum_noise/","title":"Plot Spectrum Noise","text":"In\u00a0[30]: Copied! <pre>import os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scienceplots\nimport matplotlib as mpl\nimport matplotlib\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\n# plt.style.use('science')\nplt.style.use(\"seaborn-darkgrid\")\nplt.rcParams.update({'font.size': 14})\n</pre> import os import pandas as pd import matplotlib.pyplot as plt import scienceplots import matplotlib as mpl import matplotlib # Default settings mpl.rcParams.update(mpl.rcParamsDefault) # plt.style.use('science') plt.style.use(\"seaborn-darkgrid\") plt.rcParams.update({'font.size': 14}) <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_14136\\779055010.py:10: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[31]: Copied! <pre>result_folder = 'prediction_model_formula8'\n</pre> result_folder = 'prediction_model_formula8' In\u00a0[32]: Copied! <pre>fix_noise_data = pd.read_csv(os.path.join('pic', result_folder, 'all_train_data_on_test_set','fix_noise_result.csv'))\nfix_noise = fix_noise_data['noise']\nfix_noise_RMSE = fix_noise_data['RMSE']\n\nrand_noise_data = pd.read_csv(os.path.join('pic', result_folder, 'all_train_data_on_test_set','random_noise_result.csv'))\nrand_noise = rand_noise_data['noise']\nrand_noise_RMSE = rand_noise_data['RMSE']\n</pre> fix_noise_data = pd.read_csv(os.path.join('pic', result_folder, 'all_train_data_on_test_set','fix_noise_result.csv')) fix_noise = fix_noise_data['noise'] fix_noise_RMSE = fix_noise_data['RMSE']  rand_noise_data = pd.read_csv(os.path.join('pic', result_folder, 'all_train_data_on_test_set','random_noise_result.csv')) rand_noise = rand_noise_data['noise'] rand_noise_RMSE = rand_noise_data['RMSE'] In\u00a0[37]: Copied! <pre>plt.figure(figsize=(8,6))\nplt.plot(fix_noise*100, fix_noise_RMSE, 'o-')\nplt.title('spectrum with fixed noise')\nplt.xlabel('fixed noise(%)')\nplt.ylabel('prediction performance, RMSE(%)')\nplt.savefig(os.path.join(\"pic\", result_folder, \"fixed_noise_result.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.figure(figsize=(8,6)) plt.plot(fix_noise*100, fix_noise_RMSE, 'o-') plt.title('spectrum with fixed noise') plt.xlabel('fixed noise(%)') plt.ylabel('prediction performance, RMSE(%)') plt.savefig(os.path.join(\"pic\", result_folder, \"fixed_noise_result.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[38]: Copied! <pre>plt.figure(figsize=(8,6))\nplt.plot(rand_noise*100, rand_noise_RMSE, 'o-')\nplt.title('spectrum with random noise')\nplt.xlabel('random noise(%)')\nplt.ylabel('prediction performance, RMSE(%)')\nplt.savefig(os.path.join(\"pic\", result_folder, \"random_noise_result.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.figure(figsize=(8,6)) plt.plot(rand_noise*100, rand_noise_RMSE, 'o-') plt.title('spectrum with random noise') plt.xlabel('random noise(%)') plt.ylabel('prediction performance, RMSE(%)') plt.savefig(os.path.join(\"pic\", result_folder, \"random_noise_result.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()"},{"location":"surrogate_model/plot_result/","title":"Plot Result","text":"In\u00a0[12]: Copied! <pre>import torch.nn as nn\nimport torch\nimport matplotlib.pyplot as plt\nimport pickle\nimport seaborn as sns \nimport pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport json\nfrom Preprocessing import dataload\nfrom surrogate_model import ANN\nimport matplotlib as mpl\nfrom tqdm import tqdm\n# Default settings\nmpl.rcParams.update(mpl.rcParamsDefault)\nplt.style.use(\"seaborn-darkgrid\")\n</pre> import torch.nn as nn import torch import matplotlib.pyplot as plt import pickle import seaborn as sns  import pandas as pd import numpy as np import sys import os import json from Preprocessing import dataload from surrogate_model import ANN import matplotlib as mpl from tqdm import tqdm # Default settings mpl.rcParams.update(mpl.rcParamsDefault) plt.style.use(\"seaborn-darkgrid\") <pre>C:\\Users\\dicky1031\\AppData\\Local\\Temp\\ipykernel_4676\\1858896284.py:18: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-&lt;style&gt;'. Alternatively, directly use the seaborn API instead.\n  plt.style.use(\"seaborn-darkgrid\")\n</pre> In\u00a0[13]: Copied! <pre>subject = 'ctchen'\nsave_folder = \"ijv_small\"\nos.makedirs(os.path.join(\"pic\", subject, save_folder), exist_ok=True)\nsave_path = os.path.join(\"pic\", subject, save_folder)\nresult_folder = f\"{subject}_small\"\n</pre> subject = 'ctchen' save_folder = \"ijv_small\" os.makedirs(os.path.join(\"pic\", subject, save_folder), exist_ok=True) save_path = os.path.join(\"pic\", subject, save_folder) result_folder = f\"{subject}_small\" <p>${\\bf Plot\\;RMSPE\\;of\\;Testset}$</p> In\u00a0[14]: Copied! <pre># plot result\nwith open(os.path.join(\"model_save\", result_folder, \"trlog.json\"), 'r') as f:\n    trlog = json.load(f)\n\nmin_loss = min(trlog['test_loss'])\nep = trlog['epoch']\nbest_model = trlog['best_model']\nmodel = ANN().cuda()\nmodel.load_state_dict(torch.load(best_model))\ntest_loader = torch.load(os.path.join(\"model_save\",result_folder,\"test_loader.pth\"))\n\nmodel.eval()\nerror = 0\ncount = 0\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    # output = output.view(-1)\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target).detach().cpu().numpy()\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    if count == 0:\n        plt.plot(x[:,0],y[:,0], 'r.', markersize=5, label='predict')\n        plt.plot(x[:,0],x[:,0],'b', label='optimal')\n        \n        plt.plot(x[:,1],y[:,1], 'r.', markersize=5)\n        plt.plot(x[:,1],x[:,1],'b')\n    else:\n        plt.plot(x,y, 'r.', markersize=5)\n        plt.plot(x,x,'b')\n    count += 1\n    \nerror = error/len(test_loader)\nplt.title(f\"RMSPE={100*error:.2f}%\")\nplt.xlabel(\"truth reflectance\")\nplt.ylabel(\"predict reflectance\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(save_path,\"RMSPE.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> # plot result with open(os.path.join(\"model_save\", result_folder, \"trlog.json\"), 'r') as f:     trlog = json.load(f)  min_loss = min(trlog['test_loss']) ep = trlog['epoch'] best_model = trlog['best_model'] model = ANN().cuda() model.load_state_dict(torch.load(best_model)) test_loader = torch.load(os.path.join(\"model_save\",result_folder,\"test_loader.pth\"))  model.eval() error = 0 count = 0 for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     # output = output.view(-1)     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target).detach().cpu().numpy()     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     if count == 0:         plt.plot(x[:,0],y[:,0], 'r.', markersize=5, label='predict')         plt.plot(x[:,0],x[:,0],'b', label='optimal')                  plt.plot(x[:,1],y[:,1], 'r.', markersize=5)         plt.plot(x[:,1],x[:,1],'b')     else:         plt.plot(x,y, 'r.', markersize=5)         plt.plot(x,x,'b')     count += 1      error = error/len(test_loader) plt.title(f\"RMSPE={100*error:.2f}%\") plt.xlabel(\"truth reflectance\") plt.ylabel(\"predict reflectance\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(save_path,\"RMSPE.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() <p>${\\bf Plot\\;Loss}$</p> In\u00a0[15]: Copied! <pre>tr_loss = trlog['train_loss']\nts_loss = trlog['test_loss']\nepoch = range(trlog['epoch'])\nplt.plot(epoch[10:-1],tr_loss[10:-1],'blue')\nplt.plot(epoch[10:-1],ts_loss[10:-1],'r')\nplt.legend([\"train loss\", \"test loss\"])\nplt.xlabel(\"epoch\")\nplt.ylabel(\"loss\")\nplt.savefig(os.path.join(save_path,\"loss.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> tr_loss = trlog['train_loss'] ts_loss = trlog['test_loss'] epoch = range(trlog['epoch']) plt.plot(epoch[10:-1],tr_loss[10:-1],'blue') plt.plot(epoch[10:-1],ts_loss[10:-1],'r') plt.legend([\"train loss\", \"test loss\"]) plt.xlabel(\"epoch\") plt.ylabel(\"loss\") plt.savefig(os.path.join(save_path,\"loss.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() <p>${\\bf Plot\\;RMSPE\\;Each\\;SDS}$</p> In\u00a0[16]: Copied! <pre>error1 = 0\nerror2 = 0\ncount = 0\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    # get SDS1\n    y1 = torch.exp(-output[:,0]).detach().cpu().numpy()\n    x1 = torch.exp(-target[:,0]).detach().cpu().numpy() \n    # get SDS2\n    y2 = torch.exp(-output[:,1]).detach().cpu().numpy()\n    x2 = torch.exp(-target[:,1]).detach().cpu().numpy() \n    \n    # get error \n    error1 += torch.sqrt(torch.square((torch.tensor(y1)-torch.tensor(x1))/torch.tensor(x1)).mean()).item()\n    error2 += torch.sqrt(torch.square((torch.tensor(y2)-torch.tensor(x2))/torch.tensor(x2)).mean()).item()\n    \n    if count == 0:\n        plt.plot(x1,y1, 'r.', markersize=5, label='SDS10mm, predict')\n        plt.plot(x2,y2, 'b.', markersize=5, label='SDS20mm, predict')\n        plt.plot(x1,x1, 'black', label='optimal')\n    else:\n        plt.plot(x1,y1, 'r.', markersize=5, )\n        plt.plot(x2,y2, 'b.', markersize=5, )\n        plt.plot(x1,x1, 'black')\n        plt.plot(x2,x2, 'black')\n    count += 1\n    \nerror1 = error1/len(test_loader)\nerror2 = error2/len(test_loader)\n\nplt.title(f\"SDS1 RMSPE:{100*error1:.2f}%, SDS2 RMSPE:{100*error2:.2f}%\")\nplt.xlabel(\"truth reflectance\")\nplt.ylabel(\"predict reflectance\")\n# plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n#           fancybox=True, shadow=True)\nplt.legend(fancybox=True, shadow=True)\nplt.savefig(os.path.join(save_path,\"RMSPE_SDS_10and20.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> error1 = 0 error2 = 0 count = 0 for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     # get SDS1     y1 = torch.exp(-output[:,0]).detach().cpu().numpy()     x1 = torch.exp(-target[:,0]).detach().cpu().numpy()      # get SDS2     y2 = torch.exp(-output[:,1]).detach().cpu().numpy()     x2 = torch.exp(-target[:,1]).detach().cpu().numpy()           # get error      error1 += torch.sqrt(torch.square((torch.tensor(y1)-torch.tensor(x1))/torch.tensor(x1)).mean()).item()     error2 += torch.sqrt(torch.square((torch.tensor(y2)-torch.tensor(x2))/torch.tensor(x2)).mean()).item()          if count == 0:         plt.plot(x1,y1, 'r.', markersize=5, label='SDS10mm, predict')         plt.plot(x2,y2, 'b.', markersize=5, label='SDS20mm, predict')         plt.plot(x1,x1, 'black', label='optimal')     else:         plt.plot(x1,y1, 'r.', markersize=5, )         plt.plot(x2,y2, 'b.', markersize=5, )         plt.plot(x1,x1, 'black')         plt.plot(x2,x2, 'black')     count += 1      error1 = error1/len(test_loader) error2 = error2/len(test_loader)  plt.title(f\"SDS1 RMSPE:{100*error1:.2f}%, SDS2 RMSPE:{100*error2:.2f}%\") plt.xlabel(\"truth reflectance\") plt.ylabel(\"predict reflectance\") # plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), #           fancybox=True, shadow=True) plt.legend(fancybox=True, shadow=True) plt.savefig(os.path.join(save_path,\"RMSPE_SDS_10and20.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() In\u00a0[17]: Copied! <pre>error = 0\ncount = 0\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    # output = output.view(-1)\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target).detach().cpu().numpy()\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    if count == 0:\n        plt.plot(x,y, 'r.', markersize=5, label='predict')\n        plt.plot(x,x,'b', label='optimal')\n    else:\n        plt.plot(x,y, 'r.', markersize=5)\n        plt.plot(x,x,'b')\n    count += 1\nerror = error/len(test_loader)\nplt.title(f\"RMSPE:{100*error:.2f}%\")\nplt.xlabel(\"truth reflectance\")\nplt.ylabel(\"predict reflectance\")\nplt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n          fancybox=True, shadow=True)\nplt.savefig(os.path.join(save_path,\"RMSPE_all.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\n# plot individual\nerror = 0\ncount = 0\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    output = output[:,0].view(-1) # get SDS1_pred\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target[:,0]).detach().cpu().numpy() # get SDS1_truth\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    if count == 0:\n        plt.plot(x,y, 'r.', markersize=5, label='predict')\n        plt.plot(x,x,'b', label='optimal')\n    else:\n        plt.plot(x,y, 'r.', markersize=5)\n        plt.plot(x,x,'b')\n    count += 1\nerror = error/len(test_loader)\nplt.title(f\"SDS10mm RMSPE:{100*error:.2f}%\")\nplt.xlabel(\"truth reflectance\")\nplt.ylabel(\"predict reflectance\")\n# plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n#           fancybox=True, shadow=True)\nplt.legend(fancybox=True, shadow=True)\nplt.savefig(os.path.join(save_path,\"RMSPE_SDS_10.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nerror = 0\ncount = 0\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    output = output[:,-1].view(-1) # get SDS_last_pred\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target[:,-1]).detach().cpu().numpy() # get SDS_last_truth\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    if count == 0:\n        plt.plot(x,y, 'r.', markersize=5, label='predict')\n        plt.plot(x,x,'b', label='optimal')\n    else:\n        plt.plot(x,y, 'r.', markersize=5)\n        plt.plot(x,x,'b')\n    count += 1\nerror = error/len(test_loader)\nplt.title(f\"SDS20mm RMSPE:{100*error:.2f}%\")\nplt.xlabel(\"truth reflectance\")\nplt.ylabel(\"predict reflectance\")\n# plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),\n#           fancybox=True, shadow=True)\nplt.legend(fancybox=True, shadow=True)\nplt.savefig(os.path.join(save_path,\"RMSPE_SDS_20.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> error = 0 count = 0 for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     # output = output.view(-1)     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target).detach().cpu().numpy()     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     if count == 0:         plt.plot(x,y, 'r.', markersize=5, label='predict')         plt.plot(x,x,'b', label='optimal')     else:         plt.plot(x,y, 'r.', markersize=5)         plt.plot(x,x,'b')     count += 1 error = error/len(test_loader) plt.title(f\"RMSPE:{100*error:.2f}%\") plt.xlabel(\"truth reflectance\") plt.ylabel(\"predict reflectance\") plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5),           fancybox=True, shadow=True) plt.savefig(os.path.join(save_path,\"RMSPE_all.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()  # plot individual error = 0 count = 0 for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     output = output[:,0].view(-1) # get SDS1_pred     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target[:,0]).detach().cpu().numpy() # get SDS1_truth     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     if count == 0:         plt.plot(x,y, 'r.', markersize=5, label='predict')         plt.plot(x,x,'b', label='optimal')     else:         plt.plot(x,y, 'r.', markersize=5)         plt.plot(x,x,'b')     count += 1 error = error/len(test_loader) plt.title(f\"SDS10mm RMSPE:{100*error:.2f}%\") plt.xlabel(\"truth reflectance\") plt.ylabel(\"predict reflectance\") # plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), #           fancybox=True, shadow=True) plt.legend(fancybox=True, shadow=True) plt.savefig(os.path.join(save_path,\"RMSPE_SDS_10.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()  error = 0 count = 0 for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     output = output[:,-1].view(-1) # get SDS_last_pred     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target[:,-1]).detach().cpu().numpy() # get SDS_last_truth     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     if count == 0:         plt.plot(x,y, 'r.', markersize=5, label='predict')         plt.plot(x,x,'b', label='optimal')     else:         plt.plot(x,y, 'r.', markersize=5)         plt.plot(x,x,'b')     count += 1 error = error/len(test_loader) plt.title(f\"SDS20mm RMSPE:{100*error:.2f}%\") plt.xlabel(\"truth reflectance\") plt.ylabel(\"predict reflectance\") # plt.legend(loc='center left', bbox_to_anchor=(1.05, 0.5), #           fancybox=True, shadow=True) plt.legend(fancybox=True, shadow=True) plt.savefig(os.path.join(save_path,\"RMSPE_SDS_20.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() <p>${\\bf Plot\\;Hist}$</p> In\u00a0[18]: Copied! <pre>plt.rcParams.update({'font.size': 16})\n\nmodel.eval()\nerror = 0\nerror_set = {\"error\":[]}\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target).detach().cpu().numpy()\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    e = np.abs(100*((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean(-1).numpy())\n    for i in e:\n        error_set['error'].append(i)\nerror = error/len(test_loader)        \nerror_set = pd.DataFrame(error_set)\nplt.figure(figsize=(12,6))\nnum, bin, patch = plt.hist(data=error_set, x='error', bins=300)\nall_std = np.std(error_set['error'])\nall_mean = np.mean(error_set['error'])\nplt.vlines(all_mean, 0, max(num), color='b')\nplt.vlines(all_mean+2*all_std, 0, max(num), color='r')\nplt.vlines(all_mean-2*all_std, 0, max(num), color='r')\nplt.text(all_mean, max(num),f\"{all_mean:.2f}%\",ha='center',va='bottom')\nplt.text(all_mean+2*all_std, max(num),f\"{all_mean+2*all_std:.2f}%\",ha='center',va='bottom')\nplt.text(all_mean-2*all_std, max(num),f\"{all_mean-2*all_std:.2f}%\",ha='center',va='bottom')\nplt.title(f\"testing error histogram  \\n relative error mean: {all_mean:.2f}% std: {all_std:.2f}% RMSPE: {100*error:.2f}%\")\nplt.xlabel(\"RMSPE\")\nplt.ylabel(\"count\")\nplt.savefig(os.path.join(save_path,\"error_hist_all.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\nerror_all = error\n\nerror = 0\nerror_set = {\"error\":[]}\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    output = output[:,0].view(-1)\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target[:,0]).detach().cpu().numpy()\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    e = np.abs(100*((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).numpy())\n    for i in e:\n        error_set['error'].append(i)\nerror = error/len(test_loader)        \nerror_set = pd.DataFrame(error_set)\nplt.figure(figsize=(12,6))\nnum, bin, patch = plt.hist(data=error_set, x='error', bins=300)\nstd = np.std(error_set['error'])\nmean = np.mean(error_set['error'])\nplt.vlines(mean, 0, max(num), color='b')\nplt.vlines(mean+2*std, 0, max(num), color='r')\nplt.vlines(mean-2*std, 0, max(num), color='r')\nplt.text(mean, max(num),f\"{mean:.2f}%\",ha='center',va='bottom')\nplt.text(mean+2*std, max(num),f\"{mean+2*std:.2f}%\",ha='center',va='bottom')\nplt.text(mean-2*std, max(num),f\"{mean-2*std:.2f}%\",ha='center',va='bottom')\nplt.title(f\"SDS10mm testing error histogram  \\n relative error mean: {mean:.2f}% std: {std:.2f}% RMSPE: {100*error:.2f}%\")\nplt.xlabel(\"RMSPE\")\nplt.ylabel(\"count\")\nplt.savefig(os.path.join(save_path,\"error_hist_SDS_10.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n\nerror = 0\nerror_set = {\"error\":[]}\nfor batch_idx, (data,target) in enumerate(test_loader):\n    data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()\n    output = model(data)\n    output = output[:,-1].view(-1)\n    y = torch.exp(-output).detach().cpu().numpy()\n    x = torch.exp(-target[:,-1]).detach().cpu().numpy()\n    error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()\n    e = np.abs(100*((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).numpy())\n    for i in e:\n        error_set['error'].append(i)\nerror = error/len(test_loader)        \nerror_set = pd.DataFrame(error_set)\nplt.figure(figsize=(12,6))\nnum, bin, patch = plt.hist(data=error_set, x='error', bins=300)\nstd = np.std(error_set['error'])\nmean = np.mean(error_set['error'])\nplt.vlines(mean, 0, max(num), color='b')\nplt.vlines(mean+2*std, 0, max(num), color='r')\nplt.vlines(mean-2*std, 0, max(num), color='r')\nplt.text(mean, max(num),f\"{mean:.2f}%\",ha='center',va='bottom')\nplt.text(mean+2*std, max(num),f\"{mean+2*std:.2f}%\",ha='center',va='bottom')\nplt.text(mean-2*std, max(num),f\"{mean-2*std:.2f}%\",ha='center',va='bottom')\nplt.title(f\"SDS20mm testing error histogram  \\n relative error mean: {mean:.2f}% std: {std:.2f}% RMSPE: {100*error:.2f}%\")\nplt.xlabel(\"RMSPE\")\nplt.ylabel(\"count\")\nplt.savefig(os.path.join(save_path,\"error_hist_SDS_20.png\"), dpi=300, format='png', bbox_inches='tight')\nplt.show()\n</pre> plt.rcParams.update({'font.size': 16})  model.eval() error = 0 error_set = {\"error\":[]} for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target).detach().cpu().numpy()     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     e = np.abs(100*((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean(-1).numpy())     for i in e:         error_set['error'].append(i) error = error/len(test_loader)         error_set = pd.DataFrame(error_set) plt.figure(figsize=(12,6)) num, bin, patch = plt.hist(data=error_set, x='error', bins=300) all_std = np.std(error_set['error']) all_mean = np.mean(error_set['error']) plt.vlines(all_mean, 0, max(num), color='b') plt.vlines(all_mean+2*all_std, 0, max(num), color='r') plt.vlines(all_mean-2*all_std, 0, max(num), color='r') plt.text(all_mean, max(num),f\"{all_mean:.2f}%\",ha='center',va='bottom') plt.text(all_mean+2*all_std, max(num),f\"{all_mean+2*all_std:.2f}%\",ha='center',va='bottom') plt.text(all_mean-2*all_std, max(num),f\"{all_mean-2*all_std:.2f}%\",ha='center',va='bottom') plt.title(f\"testing error histogram  \\n relative error mean: {all_mean:.2f}% std: {all_std:.2f}% RMSPE: {100*error:.2f}%\") plt.xlabel(\"RMSPE\") plt.ylabel(\"count\") plt.savefig(os.path.join(save_path,\"error_hist_all.png\"), dpi=300, format='png', bbox_inches='tight') plt.show() error_all = error  error = 0 error_set = {\"error\":[]} for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     output = output[:,0].view(-1)     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target[:,0]).detach().cpu().numpy()     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     e = np.abs(100*((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).numpy())     for i in e:         error_set['error'].append(i) error = error/len(test_loader)         error_set = pd.DataFrame(error_set) plt.figure(figsize=(12,6)) num, bin, patch = plt.hist(data=error_set, x='error', bins=300) std = np.std(error_set['error']) mean = np.mean(error_set['error']) plt.vlines(mean, 0, max(num), color='b') plt.vlines(mean+2*std, 0, max(num), color='r') plt.vlines(mean-2*std, 0, max(num), color='r') plt.text(mean, max(num),f\"{mean:.2f}%\",ha='center',va='bottom') plt.text(mean+2*std, max(num),f\"{mean+2*std:.2f}%\",ha='center',va='bottom') plt.text(mean-2*std, max(num),f\"{mean-2*std:.2f}%\",ha='center',va='bottom') plt.title(f\"SDS10mm testing error histogram  \\n relative error mean: {mean:.2f}% std: {std:.2f}% RMSPE: {100*error:.2f}%\") plt.xlabel(\"RMSPE\") plt.ylabel(\"count\") plt.savefig(os.path.join(save_path,\"error_hist_SDS_10.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()  error = 0 error_set = {\"error\":[]} for batch_idx, (data,target) in enumerate(test_loader):     data,target = data.to(torch.float32).cuda(), target.to(torch.float32).cuda()     output = model(data)     output = output[:,-1].view(-1)     y = torch.exp(-output).detach().cpu().numpy()     x = torch.exp(-target[:,-1]).detach().cpu().numpy()     error += torch.sqrt(torch.square((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).mean()).item()     e = np.abs(100*((torch.tensor(y)-torch.tensor(x))/torch.tensor(x)).numpy())     for i in e:         error_set['error'].append(i) error = error/len(test_loader)         error_set = pd.DataFrame(error_set) plt.figure(figsize=(12,6)) num, bin, patch = plt.hist(data=error_set, x='error', bins=300) std = np.std(error_set['error']) mean = np.mean(error_set['error']) plt.vlines(mean, 0, max(num), color='b') plt.vlines(mean+2*std, 0, max(num), color='r') plt.vlines(mean-2*std, 0, max(num), color='r') plt.text(mean, max(num),f\"{mean:.2f}%\",ha='center',va='bottom') plt.text(mean+2*std, max(num),f\"{mean+2*std:.2f}%\",ha='center',va='bottom') plt.text(mean-2*std, max(num),f\"{mean-2*std:.2f}%\",ha='center',va='bottom') plt.title(f\"SDS20mm testing error histogram  \\n relative error mean: {mean:.2f}% std: {std:.2f}% RMSPE: {100*error:.2f}%\") plt.xlabel(\"RMSPE\") plt.ylabel(\"count\") plt.savefig(os.path.join(save_path,\"error_hist_SDS_20.png\"), dpi=300, format='png', bbox_inches='tight') plt.show()"},{"location":"surrogate_model/plot_surrogate_model_structure/","title":"Plot Surrogate Model Structure","text":"In\u00a0[1]: Copied! <pre>import torch\nimport numpy as np\nfrom surrogate_model import ANN\nimport random\nimport os\nos.makedirs(os.path.join('pic'), exist_ok=True)\n</pre> import torch import numpy as np from surrogate_model import ANN import random import os os.makedirs(os.path.join('pic'), exist_ok=True) In\u00a0[2]: Copied! <pre>model = ANN().cuda()\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Model visualization\ninput_names = ['optical parameter']\noutput_names = ['reflectance']\ntensor_input = np.array([random.random() for i in range(10)])\ntensor_input = torch.tensor(tensor_input)\ntensor_input = tensor_input.to(torch.float32).to(device)\ntorch.onnx.export(model, tensor_input, os.path.join('pic', 'surrogate_model_structure.onnx'), input_names=input_names, output_names=output_names)\n</pre> model = ANN().cuda() device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Model visualization input_names = ['optical parameter'] output_names = ['reflectance'] tensor_input = np.array([random.random() for i in range(10)]) tensor_input = torch.tensor(tensor_input) tensor_input = tensor_input.to(torch.float32).to(device) torch.onnx.export(model, tensor_input, os.path.join('pic', 'surrogate_model_structure.onnx'), input_names=input_names, output_names=output_names) <pre>============= Diagnostic Run torch.onnx.export version 2.0.0+cu117 =============\nverbose: False, log level: Level.ERROR\n======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n\n</pre>"}]}